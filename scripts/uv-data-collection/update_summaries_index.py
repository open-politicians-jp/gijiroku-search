#!/usr/bin/env python3
"""
è¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ—¢å­˜ã®è¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ summaries_index.json ã‚’ç”Ÿæˆãƒ»æ›´æ–°ã—ã¾ã™ã€‚
ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ã®å‹•çš„ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
"""

import json
import os
import logging
from datetime import datetime
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def update_summaries_index():
    """è¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°"""
    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
        project_root = Path(__file__).parent.parent.parent
        summaries_dir = project_root / "frontend" / "public" / "data" / "summaries"
        
        logger.info(f"ğŸ“ è¦ç´„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {summaries_dir}")
        
        if not summaries_dir.exists():
            logger.warning(f"è¦ç´„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {summaries_dir}")
            summaries_dir.mkdir(parents=True, exist_ok=True)
            logger.info("è¦ç´„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ")
        
        # summariesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        summary_files = []
        for file_path in summaries_dir.glob("summary_*.json"):
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«è‡ªä½“ã¯é™¤å¤–
            if file_path.name != "summaries_index.json":
                summary_files.append(file_path.name)
        
        # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        summary_files.sort(key=lambda x: extract_date_from_filename(x), reverse=True)
        
        logger.info(f"ğŸ” ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(summary_files)}")
        for file in summary_files:
            logger.info(f"  - {file}")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        index_data = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_files": len(summary_files),
                "description": "Summary files index for dynamic loading",
                "last_updated": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            "files": summary_files
        }
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        index_path = summaries_dir / "summaries_index.json"
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… è¦ç´„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°å®Œäº†!")
        logger.info(f"  - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(summary_files)}")
        logger.info(f"  - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {index_path}")
        logger.info(f"  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {index_path.stat().st_size} bytes")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def extract_date_from_filename(filename: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºï¼ˆã‚½ãƒ¼ãƒˆç”¨ï¼‰"""
    try:
        # summary_20250603_è¡†è­°_è­°é™¢é‹å–¶å§”å“¡ä¼š.json ã‹ã‚‰ 20250603 ã‚’æŠ½å‡º
        parts = filename.split('_')
        if len(parts) >= 2:
            return parts[1]  # 20250603
        return "00000000"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    except:
        return "00000000"

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ è¦ç´„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°é–‹å§‹")
    
    success = update_summaries_index()
    
    if success:
        logger.info("âœ¨ è¦ç´„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°å‡¦ç†å®Œäº†")
    else:
        logger.error("âŒ è¦ç´„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°å‡¦ç†å¤±æ•—")
        exit(1)

if __name__ == "__main__":
    main()