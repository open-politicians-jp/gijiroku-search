#!/usr/bin/env python3
"""
é€²è¡Œçš„ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Issue #26å¯¾å¿œ)

GitHub Actionsç”¨ã«è¨­è¨ˆã•ã‚ŒãŸé€²è¡Œçš„ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ï¼š
- æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æœ€å¤æ—¥ä»˜ã‚’åŸºæº–ã¨ã—ãŸé€²è¡Œçš„ãªéå»ãƒ‡ãƒ¼ã‚¿åé›†
- ãƒ•ã‚¡ã‚¤ãƒ«åè§£æã«ã‚ˆã‚Šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æœŸé–“ã‚’æŠŠæ¡
- ä¸è¶³æœŸé–“ã‚’è‡ªå‹•è¨ˆç®—ã—ã¦éå»ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
- IPå½è£…ã¨ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
- ãƒ•ã‚¡ã‚¤ãƒ«åå½¢å¼: speeches_YYYYMM01_HHMMSS.json (ãƒ‡ãƒ¼ã‚¿æœŸé–“åŸºæº–)

ç’°å¢ƒå¤‰æ•°:
- MONTHS_BACK: åé›†æœˆæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3)
- USE_PROGRESSIVE: é€²è¡Œçš„åé›†ã‚’ä½¿ç”¨ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: true)  
- FORCE_UPDATE: å¼·åˆ¶æ›´æ–° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: false)
- PAST_DATA_MODE: éå»ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: false)
"""

import os
import json
import time
import random
import requests
import calendar
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DailyKokkaiAPIClient:
    """æ¯æ—¥å®Ÿè¡Œç”¨ã®å›½ä¼šAPIåé›†ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ (Issue #26å¯¾å¿œ)"""
    
    def __init__(self):
        self.base_url = "https://kokkai.ndl.go.jp/api/speech"
        self.session = requests.Session()
        self.ua = UserAgent()
        self.request_count = 0
        self.update_headers()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.raw_data_dir = self.project_root / "data" / "raw" / "speeches"
        self.frontend_data_dir = self.project_root / "frontend" / "public" / "data" / "speeches"
        self.raw_data_dir.mkdir(parents=True, exist_ok=True)
        self.frontend_data_dir.mkdir(parents=True, exist_ok=True)
        
    def update_headers(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°"""
        referrers = [
            'https://www.google.com/',
            'https://www.yahoo.co.jp/',
            'https://www.bing.com/',
            'https://kokkai.ndl.go.jp/',
            'https://www.ndl.go.jp/'
        ]
        
        headers = {
            'User-Agent': self.ua.random,
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': random.choice(referrers),
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        self.session.headers.update(headers)
        
    def rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’èª¿æ•´"""
        self.request_count += 1
        
        # 10ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«ãƒ˜ãƒƒãƒ€ãƒ¼æ›´æ–°
        if self.request_count % 10 == 0:
            self.update_headers()
            logger.info(f"ãƒ˜ãƒƒãƒ€ãƒ¼æ›´æ–° ({self.request_count}ãƒªã‚¯ã‚¨ã‚¹ãƒˆ)")
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™: 1-3ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ å¾…æ©Ÿ
        wait_time = random.uniform(1.0, 3.0)
        time.sleep(wait_time)
        
    def get_last_day_of_month(self, year: int, month: int) -> int:
        """æŒ‡å®šå¹´æœˆã®æœ€çµ‚æ—¥ã‚’å–å¾—ï¼ˆã†ã‚‹ã†å¹´å¯¾å¿œï¼‰"""
        return calendar.monthrange(year, month)[1]
        
    def get_monthly_data(self, year: int, month: int) -> List[Dict[str, Any]]:
        """æŒ‡å®šæœˆã®è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        logger.info(f"ğŸ“… {year}å¹´{month}æœˆã®ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        
        all_speeches = []
        start_record = 1
        records_per_request = 100
        
        # æŒ‡å®šæœˆã®æœ€å¾Œã®æ—¥ã‚’å–å¾—
        last_day = self.get_last_day_of_month(year, month)
        logger.info(f"ğŸ“… {year}å¹´{month}æœˆ: 1æ—¥ã€œ{last_day}æ—¥ (è¨ˆ{last_day}æ—¥)")
        
        while True:
            self.rate_limit()
            
            params = {
                'startRecord': start_record,
                'maximumRecords': records_per_request,
                'recordPacking': 'json',
                'from': f"{year}-{month:02d}-01",
                'until': f"{year}-{month:02d}-{last_day:02d}"
            }
            
            try:
                logger.info(f"ğŸ” ãƒ¬ã‚³ãƒ¼ãƒ‰ {start_record}ã€œ{start_record + records_per_request - 1} ã‚’å–å¾—ä¸­...")
                response = self.session.get(self.base_url, params=params, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                
                if 'speechRecord' not in data:
                    logger.warning(f"âš ï¸ speechRecord not found in response")
                    break
                    
                speeches = data['speechRecord']
                
                if not speeches:
                    logger.info(f"âœ… {year}å¹´{month}æœˆã®ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
                    break
                    
                # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
                normalized_speeches = []
                for speech in speeches:
                    normalized_speech = self.normalize_speech_data(speech)
                    if normalized_speech:
                        normalized_speeches.append(normalized_speech)
                
                all_speeches.extend(normalized_speeches)
                logger.info(f"ğŸ“Š {len(normalized_speeches)}ä»¶è¿½åŠ  (ç´¯è¨ˆ: {len(all_speeches)}ä»¶)")
                
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸
                start_record += records_per_request
                
                # å®‰å…¨åˆ¶é™: æœˆé–“5000ä»¶ã¾ã§
                if len(all_speeches) >= 5000:
                    logger.warning(f"âš ï¸ æœˆé–“åˆ¶é™(5000ä»¶)ã«é”ã—ãŸãŸã‚çµ‚äº†")
                    break
                    
            except Exception as e:
                logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
                break
                
        logger.info(f"ğŸ‰ {year}å¹´{month}æœˆ: åˆè¨ˆ {len(all_speeches)}ä»¶ åé›†å®Œäº†")
        return all_speeches
        
    def normalize_speech_data(self, speech: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–"""
        try:
            # åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æŠ½å‡º
            normalized = {
                'id': speech.get('speechID', ''),
                'date': speech.get('date', ''),
                'session': int(speech.get('session', 0)),
                'house': speech.get('nameOfHouse', ''),
                'committee': speech.get('nameOfMeeting', ''),
                'meeting_info': self.extract_meeting_details(speech),
                'speaker': speech.get('speaker', ''),
                'party': None,
                'party_normalized': None,
                'party_aliases': [],
                'position': speech.get('speakerPosition', ''),
                'text': speech.get('speech', ''),
                'url': speech.get('speechURL', ''),
                'created_at': datetime.now().isoformat()
            }
            
            # æ”¿å…šæƒ…å ±ã®å‡¦ç†
            speaker_group = speech.get('speakerGroup', '')
            if speaker_group:
                normalized['party'] = speaker_group
                normalized['party_normalized'] = self.normalize_party_name(speaker_group)
                normalized['party_aliases'] = self.get_party_aliases(normalized['party_normalized'])
            
            return normalized
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return None
            
    def normalize_party_name(self, party_name: str) -> str:
        """æ”¿å…šåã‚’æ­£è¦åŒ–"""
        party_mapping = {
            'è‡ªç”±æ°‘ä¸»å…š': 'è‡ªç”±æ°‘ä¸»å…š',
            'è‡ªæ°‘å…š': 'è‡ªç”±æ°‘ä¸»å…š',
            'ç«‹æ†²æ°‘ä¸»å…š': 'ç«‹æ†²æ°‘ä¸»å…š',
            'ç«‹æ°‘': 'ç«‹æ†²æ°‘ä¸»å…š',
            'æ—¥æœ¬ç¶­æ–°ã®ä¼š': 'æ—¥æœ¬ç¶­æ–°ã®ä¼š',
            'ç¶­æ–°': 'æ—¥æœ¬ç¶­æ–°ã®ä¼š',
            'å…¬æ˜å…š': 'å…¬æ˜å…š',
            'å…¬æ˜': 'å…¬æ˜å…š',
            'æ—¥æœ¬å…±ç”£å…š': 'æ—¥æœ¬å…±ç”£å…š',
            'å…±ç”£å…š': 'æ—¥æœ¬å…±ç”£å…š',
            'å…±ç”£': 'æ—¥æœ¬å…±ç”£å…š',
            'å›½æ°‘æ°‘ä¸»å…š': 'å›½æ°‘æ°‘ä¸»å…š',
            'å›½æ°‘': 'å›½æ°‘æ°‘ä¸»å…š',
            'ã‚Œã„ã‚æ–°é¸çµ„': 'ã‚Œã„ã‚æ–°é¸çµ„',
            'ã‚Œã„ã‚': 'ã‚Œã„ã‚æ–°é¸çµ„'
        }
        
        for key, normalized in party_mapping.items():
            if key in party_name:
                return normalized
        
        return party_name
        
    def get_party_aliases(self, party_name: str) -> List[str]:
        """æ”¿å…šã®ç•¥ç§°ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        aliases_mapping = {
            'è‡ªç”±æ°‘ä¸»å…š': ['è‡ªæ°‘å…š', 'è‡ªæ°‘', 'LDP'],
            'ç«‹æ†²æ°‘ä¸»å…š': ['ç«‹æ°‘', 'ç«‹æ†²'],
            'æ—¥æœ¬ç¶­æ–°ã®ä¼š': ['ç¶­æ–°', 'ç¶­æ–°ã®ä¼š', 'å¤§é˜ªç¶­æ–°'],
            'å…¬æ˜å…š': ['å…¬æ˜'],
            'æ—¥æœ¬å…±ç”£å…š': ['å…±ç”£å…š', 'å…±ç”£', 'JCP'],
            'å›½æ°‘æ°‘ä¸»å…š': ['å›½æ°‘'],
            'ã‚Œã„ã‚æ–°é¸çµ„': ['ã‚Œã„ã‚']
        }
        
        return aliases_mapping.get(party_name, [])
        
    def extract_meeting_details(self, speech: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼šè­°ã®è©³ç´°æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            meeting_details = {
                'session_name': speech.get('nameOfSession', ''),
                'meeting_name': speech.get('nameOfMeeting', ''),
                'house': speech.get('nameOfHouse', ''),
                'meeting_number': speech.get('meetingNumber', ''),
                'date': speech.get('date', ''),
                'issue': speech.get('issue', ''),
                'pdf_url': speech.get('pdfURL', ''),
                'speech_order': speech.get('speechOrder', ''),
                'meeting_type': self.classify_meeting_type(speech.get('nameOfMeeting', ''))
            }
            
            return meeting_details
            
        except Exception as e:
            logger.error(f"âŒ ä¼šè­°è©³ç´°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {}
            
    def classify_meeting_type(self, meeting_name: str) -> str:
        """ä¼šè­°åã‹ã‚‰ä¼šè­°ç¨®åˆ¥ã‚’åˆ†é¡"""
        if not meeting_name:
            return 'ä¸æ˜'
            
        if 'æœ¬ä¼šè­°' in meeting_name:
            return 'æœ¬ä¼šè­°'
        elif 'äºˆç®—å§”å“¡ä¼š' in meeting_name:
            return 'äºˆç®—å§”å“¡ä¼š'
        elif 'ç‰¹åˆ¥å§”å“¡ä¼š' in meeting_name:
            return 'ç‰¹åˆ¥å§”å“¡ä¼š'
        elif 'å¸¸ä»»å§”å“¡ä¼š' in meeting_name or 'å§”å“¡ä¼š' in meeting_name:
            return 'å¸¸ä»»å§”å“¡ä¼š'
        elif 'å¯©æŸ»ä¼š' in meeting_name:
            return 'å¯©æŸ»ä¼š'
        elif 'èª¿æŸ»ä¼š' in meeting_name:
            return 'èª¿æŸ»ä¼š'
        elif 'åˆ†ç§‘ä¼š' in meeting_name:
            return 'åˆ†ç§‘ä¼š'
        else:
            return 'ãã®ä»–'
        
    def generate_filename(self, year: int, month: int, day_range: str) -> str:
        """çµ±ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åå½¢å¼ã§ç”Ÿæˆ: speeches_YYYYMM01_HHMMSS.jsonï¼ˆãƒ‡ãƒ¼ã‚¿æœŸé–“åŸºæº–ï¼‰"""
        timestamp = datetime.now().strftime("%H%M%S")
        return f"speeches_{year}{month:02d}01_{timestamp}.json"
        
    def save_monthly_data(self, speeches: List[Dict[str, Any]], year: int, month: int):
        """æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã«ã‚‚åŒæ™‚ä¿å­˜ï¼‰"""
        if not speeches:
            logger.warning(f"âš ï¸ {year}å¹´{month}æœˆ: ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            return
            
        filename = self.generate_filename(year, month, "")
        raw_filepath = self.raw_data_dir / filename
        frontend_filepath = self.frontend_data_dir / filename
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã§ä¿å­˜
        data = {
            "metadata": {
                "data_type": "speeches",
                "year": year,
                "month": month,
                "total_count": len(speeches),
                "generated_at": datetime.now().isoformat(),
                "source": "https://kokkai.ndl.go.jp/api.html",
                "collection_method": "progressive_past_data_collection",
                "filename_format": "speeches_YYYYMM01_HHMMSS.json",
                "period": f"{year}-{month:02d}",
                "is_past_data": os.getenv('PAST_DATA_MODE', 'false').lower() == 'true'
            },
            "data": speeches
        }
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜
        with open(raw_filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        with open(frontend_filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        file_size = raw_filepath.stat().st_size / (1024 * 1024)
        logger.info(f"ğŸ’¾ ä¿å­˜å®Œäº†: {filename} ({file_size:.1f} MB)")
        logger.info(f"  - ç”Ÿãƒ‡ãƒ¼ã‚¿: {raw_filepath}")
        logger.info(f"  - ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: {frontend_filepath}")

def analyze_existing_data_periods(client):
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ«åè§£æã«ã‚ˆã‚Šåé›†æ¸ˆã¿æœŸé–“ã‚’æŠŠæ¡ (Issue #26)"""
    
    # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    existing_files = list(client.frontend_data_dir.glob("speeches_*.json"))
    covered_periods = set()
    oldest_period = None
    newest_period = None
    
    logger.info(f"ğŸ” æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ« {len(existing_files)} ä»¶ã‹ã‚‰åé›†æ¸ˆã¿æœŸé–“ã‚’è§£æä¸­...")
    
    for file_path in existing_files:
        try:
            filename = file_path.name
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            if filename.startswith('speeches_') and filename.endswith('.json'):
                # speeches_YYYYMM01_HHMMSS.json ãƒ‘ã‚¿ãƒ¼ãƒ³
                import re
                match = re.match(r'speeches_(\d{4})(\d{2})01_\d{6}\.json', filename)
                if match:
                    year, month = int(match.group(1)), int(match.group(2))
                    period = (year, month)
                    covered_periods.add(period)
                    
                    # æœ€å¤ãƒ»æœ€æ–°æœŸé–“ã®æ›´æ–°
                    if oldest_period is None or period < oldest_period:
                        oldest_period = period
                    if newest_period is None or period > newest_period:
                        newest_period = period
                        
                # æ—§å½¢å¼ã‚‚å¯¾å¿œï¼ˆspeeches_YYYYMMDD_HHMMSS.jsonç­‰ï¼‰
                elif re.match(r'speeches_(\d{4})(\d{2})\d{2}_\d{6}\.json', filename):
                    match = re.match(r'speeches_(\d{4})(\d{2})\d{2}_\d{6}\.json', filename)
                    if match:
                        year, month = int(match.group(1)), int(match.group(2))
                        period = (year, month)
                        covered_periods.add(period)
                        
                        if oldest_period is None or period < oldest_period:
                            oldest_period = period
                        if newest_period is None or period > newest_period:
                            newest_period = period
                            
        except Exception as e:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«åè§£æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            continue
    
    return covered_periods, oldest_period, newest_period

def get_progressive_collection_months(client, months_to_collect=3):
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«åè§£æã‚’åŸºã«é€²è¡Œçš„ãªåé›†å¯¾è±¡æœˆã‚’æ±ºå®š (Issue #26å¯¾å¿œ)"""
    
    covered_periods, oldest_period, newest_period = analyze_existing_data_periods(client)
    
    if not covered_periods:
        logger.info("ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãªã—: ç¾åœ¨æ—¥ä»˜ã‹ã‚‰éå»ãƒ‡ãƒ¼ã‚¿ã‚’åé›†")
        current_date = datetime.now()
        target_months = []
        for i in range(months_to_collect):
            target_date = current_date - relativedelta(months=i)
            target_months.append((target_date.year, target_date.month))
        return target_months
    
    logger.info(f"ğŸ“Š åé›†æ¸ˆã¿æœŸé–“: {len(covered_periods)}ãƒ¶æœˆåˆ†")
    logger.info(f"ğŸ“Š æœ€å¤æœŸé–“: {oldest_period[0]}å¹´{oldest_period[1]}æœˆ")
    logger.info(f"ğŸ“Š æœ€æ–°æœŸé–“: {newest_period[0]}å¹´{newest_period[1]}æœˆ")
    
    # æœ€å¤æœŸé–“ã‚ˆã‚Šéå»ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†å¯¾è±¡ã¨ã™ã‚‹
    target_months = []
    base_year, base_month = oldest_period
    base_date = datetime(base_year, base_month, 1)
    
    # éå»ã«å‘ã‹ã£ã¦æœªåé›†ã®æœˆã‚’ç‰¹å®š
    for i in range(1, months_to_collect + 1):
        target_date = base_date - relativedelta(months=i)
        target_period = (target_date.year, target_date.month)
        
        # æ—¢ã«åé›†æ¸ˆã¿ã®æœŸé–“ã¯ã‚¹ã‚­ãƒƒãƒ—
        if target_period not in covered_periods:
            target_months.append(target_period)
    
    if target_months:
        logger.info(f"ğŸ“… éå»ãƒ‡ãƒ¼ã‚¿åé›†: æœ€å¤æœŸé–“ {base_year}å¹´{base_month}æœˆ ã‚ˆã‚Šéå» {len(target_months)}ãƒ¶æœˆåˆ†")
        logger.info(f"ğŸ“… åé›†å¯¾è±¡: {', '.join([f'{y}å¹´{m}æœˆ' for y, m in target_months])}")
    else:
        logger.info(f"âœ… éå» {months_to_collect}ãƒ¶æœˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«åé›†æ¸ˆã¿")
    
    return target_months

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("ğŸš€ æ¯æ—¥ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šå–å¾—
    months_back = float(os.getenv('MONTHS_BACK', '3'))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’3ãƒ¶æœˆã«å¤‰æ›´
    force_update = os.getenv('FORCE_UPDATE', 'false').lower() == 'true'
    use_progressive = os.getenv('USE_PROGRESSIVE', 'true').lower() == 'true'
    
    logger.info(f"ğŸ“‹ è¨­å®š: {months_back}ãƒ¶æœˆåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†")
    logger.info(f"ğŸ“‹ å¼·åˆ¶æ›´æ–°: {force_update}")
    logger.info(f"ğŸ“‹ é€²è¡Œçš„åé›†: {use_progressive}")
    
    client = DailyKokkaiAPIClient()
    
    if use_progressive:
        # æ–°ã—ã„é€²è¡Œçš„åé›†ãƒ­ã‚¸ãƒƒã‚¯
        target_months = get_progressive_collection_months(client, int(months_back))
    else:
        # å¾“æ¥ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰
        current_date = datetime.now()
        target_months = []
        
        if months_back < 1:
            target_months.append((current_date.year, current_date.month))
            logger.info(f"ğŸ“… å°æ•°ç‚¹æœˆæ•°({months_back})ã®ãŸã‚ç¾åœ¨æœˆã®ã¿åé›†")
        else:
            months_to_collect = int(months_back) + (1 if months_back % 1 > 0 else 0)
            logger.info(f"ğŸ“… å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯: {months_to_collect}ãƒ¶æœˆåˆ†ã‚’åé›†å¯¾è±¡")
            
            for i in range(months_to_collect):
                target_date = current_date - relativedelta(months=i)
                target_months.append((target_date.year, target_date.month))
    
    # åé›†å¯¾è±¡æœˆã‚’è¡¨ç¤º
    logger.info(f"ğŸ“… åé›†å¯¾è±¡æœˆ: {', '.join([f'{y}å¹´{m}æœˆ' for y, m in target_months])}")
    
    # å„æœˆã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    for year, month in target_months:
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
        existing_files = list(client.raw_data_dir.glob(f"speeches_{year}{month:02d}*.json"))
        
        if existing_files and not force_update:
            logger.info(f"â­ï¸ {year}å¹´{month}æœˆ: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Šã€ã‚¹ã‚­ãƒƒãƒ—")
            continue
            
        # ãƒ‡ãƒ¼ã‚¿åé›†
        speeches = client.get_monthly_data(year, month)
        
        if speeches:
            client.save_monthly_data(speeches, year, month)
        else:
            logger.warning(f"âš ï¸ {year}å¹´{month}æœˆ: ãƒ‡ãƒ¼ã‚¿ãªã—")
            
    logger.info("âœ¨ æ¯æ—¥ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†!")

if __name__ == "__main__":
    main()