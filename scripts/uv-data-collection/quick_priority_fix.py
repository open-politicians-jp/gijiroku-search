#!/usr/bin/env python3
"""
å„ªå…ˆåº¦ã®é«˜ã„éƒ½é“åºœçœŒã®è¿…é€Ÿä¿®æ­£
"""

import json
import logging
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QuickPriorityFixer:
    def __init__(self):
        self.session = requests.Session()
        ua = UserAgent()
        desktop_ua = ua.random
        
        self.session.headers.update({
            'User-Agent': desktop_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # å•é¡Œã®ã‚ã‚‹éƒ½é“åºœçœŒï¼ˆç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã™ãã‚‹ï¼‰
        self.priority_prefectures = [
            (1, "åŒ—æµ·é“"),    # å®Ÿéš›11å â†’ ç¾åœ¨0å
            (3, "å²©æ‰‹çœŒ"),    # æ¨å®š6-8å â†’ ç¾åœ¨0å
            (14, "ç¥å¥ˆå·çœŒ"), # å®Ÿéš›15å â†’ ç¾åœ¨3å
            (33, "å²¡å±±çœŒ"),   # æ¨å®š6-8å â†’ ç¾åœ¨0å
            (45, "å®®å´çœŒ"),   # æ¨å®š4-6å â†’ ç¾åœ¨0å
            (46, "é¹¿å…å³¶çœŒ"), # æ¨å®š5-7å â†’ ç¾åœ¨0å
            
            # 1åã—ã‹ãªã„éƒ½é“åºœçœŒï¼ˆå°‘ãªã™ãã‚‹ï¼‰
            (2, "é’æ£®çœŒ"),    # ç¾åœ¨1å â†’ æ¨å®š4-6å
            (4, "å®®åŸçœŒ"),    # ç¾åœ¨1å â†’ æ¨å®š6-8å
            (5, "ç§‹ç”°çœŒ"),    # ç¾åœ¨1å â†’ æ¨å®š3-5å
            (17, "çŸ³å·çœŒ"),   # ç¾åœ¨1å â†’ æ¨å®š3-5å
            (18, "ç¦äº•çœŒ"),   # ç¾åœ¨1å â†’ æ¨å®š3-4å
            (19, "å±±æ¢¨çœŒ"),   # ç¾åœ¨1å â†’ æ¨å®š3-4å
            (20, "é•·é‡çœŒ"),   # ç¾åœ¨1å â†’ æ¨å®š6-8å
            (35, "å±±å£çœŒ"),   # ç¾åœ¨1å â†’ æ¨å®š4-6å
            (38, "æ„›åª›çœŒ"),   # ç¾åœ¨1å â†’ æ¨å®š4-6å
            (47, "æ²–ç¸„çœŒ"),   # ç¾åœ¨1å â†’ æ¨å®š4-6å
        ]
        
        # æ”¿å…šãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
        self.parties = [
            "è‡ªç”±æ°‘ä¸»å…š", "ç«‹æ†²æ°‘ä¸»å…š", "æ—¥æœ¬ç¶­æ–°ã®ä¼š", "å…¬æ˜å…š", "æ—¥æœ¬å…±ç”£å…š",
            "å›½æ°‘æ°‘ä¸»å…š", "ã‚Œã„ã‚æ–°é¸çµ„", "å‚æ”¿å…š", "ç¤¾ä¼šæ°‘ä¸»å…š", "NHKå…š",
            "æ—¥æœ¬ä¿å®ˆå…š", "æ—¥æœ¬æ”¹é©å…š", "ç„¡æ‰€å±", "ç„¡æ‰€å±é€£åˆ", "æ—¥æœ¬èª çœŸä¼š",
            "æ—¥æœ¬ã®å®¶åº­ã‚’å®ˆã‚‹ä¼š", "å†ç”Ÿã®é“", "å·®åˆ¥æ’²æ»…å…š", "æ ¸èåˆå…š", "ãƒãƒ¼ãƒ ã¿ã‚‰ã„",
            "å¤šå¤«å¤šå¦»å…š", "å›½æ”¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ä¼š", "æ–°å…šã‚„ã¾ã¨", "æœªåˆ†é¡"
        ]
    
    def extract_all_candidates_robust(self, pref_code: int, pref_name: str):
        """å …ç‰¢ãªå€™è£œè€…æŠ½å‡º"""
        url = f"https://sangiin.go2senkyo.com/2025/prefecture/{pref_code}"
        logger.info(f"ğŸ” {pref_name} (ã‚³ãƒ¼ãƒ‰: {pref_code}) å …ç‰¢æŠ½å‡ºé–‹å§‹")
        
        try:
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                logger.error(f"âŒ {pref_name} ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ã¾ãšå®Ÿéš›ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯æ•°ã‚’ç¢ºèª
            profile_links = soup.find_all('a', href=re.compile(r'/seijika/\d+'))
            unique_ids = set()
            for link in profile_links:
                match = re.search(r'/seijika/(\d+)', link.get('href', ''))
                if match:
                    unique_ids.add(match.group(1))
            
            logger.info(f"ğŸ“Š {pref_name} ç™ºè¦‹ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«: {len(unique_ids)}å€‹")
            
            candidates = []
            processed_ids = set()
            
            # å„ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã«å¯¾ã—ã¦å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º
            for i, candidate_id in enumerate(unique_ids):
                try:
                    # ã“ã®IDã«å¯¾å¿œã™ã‚‹ãƒªãƒ³ã‚¯ã‚’å†æ¤œç´¢
                    target_link = soup.find('a', href=re.compile(f'/seijika/{candidate_id}'))
                    if not target_link:
                        continue
                    
                    candidate = self.extract_candidate_from_context(target_link, pref_name, candidate_id, url)
                    if candidate and candidate_id not in processed_ids:
                        candidates.append(candidate)
                        processed_ids.add(candidate_id)
                        logger.info(f"  âœ… {i+1}: {candidate['name']} ({candidate['party']})")
                
                except Exception as e:
                    logger.debug(f"å€™è£œè€… {candidate_id} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            logger.info(f"ğŸ¯ {pref_name} æŠ½å‡ºå®Œäº†: {len(candidates)}å")
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def extract_candidate_from_context(self, link, prefecture: str, candidate_id: str, page_url: str):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": f"go2s_{candidate_id}",
                "name": "",
                "prefecture": prefecture,
                "constituency": prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', ''),
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "profile_url": f"https://go2senkyo.com{link.get('href')}",
                "source_page": page_url,
                "source": "robust_extraction",
                "collected_at": datetime.now().isoformat()
            }
            
            # 1. åå‰æŠ½å‡ºï¼ˆè¤‡æ•°æ–¹æ³•ï¼‰
            name = self.extract_name_robust(link)
            if name:
                candidate["name"] = name
            else:
                # åå‰ãŒå–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                return None
            
            # 2. æ”¿å…šæŠ½å‡º
            party = self.extract_party_robust(link)
            if party:
                candidate["party"] = party
                candidate["party_normalized"] = party
            
            return candidate
            
        except Exception as e:
            logger.debug(f"å€™è£œè€…æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_name_robust(self, link):
        """å …ç‰¢ãªåå‰æŠ½å‡º"""
        name = ""
        
        try:
            # æ–¹æ³•1: ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥ç¢ºèª
            link_text = link.get_text(strip=True)
            if link_text and link_text not in ['è©³ç´°ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'è©³ç´°']:
                if re.match(r'[ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,}', link_text):
                    return link_text
            
            # æ–¹æ³•2: è¦ªè¦ç´ éšå±¤æ¤œç´¢
            current = link.parent
            search_levels = 0
            
            while current and search_levels < 8:  # ã‚ˆã‚Šæ·±ãæ¤œç´¢
                # ã‚¯ãƒ©ã‚¹åãƒ™ãƒ¼ã‚¹æ¤œç´¢
                name_candidates = current.find_all(class_=re.compile(r'name|title|candidate|person'))
                for elem in name_candidates:
                    text = elem.get_text(strip=True)
                    if text and len(text) <= 20:  # é©åº¦ãªé•·ã•
                        name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,8})', text)
                        if name_match:
                            potential_name = name_match.group(1)
                            if potential_name not in ['è©³ç´°', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'é¸æŒ™', 'å€™è£œè€…', 'æ”¿æ²»å®¶']:
                                return potential_name
                
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹æ¤œç´¢
                current_text = current.get_text()
                text_lines = [line.strip() for line in current_text.split('\n') if line.strip()]
                
                for line in text_lines:
                    if len(line) <= 15:  # çŸ­ã„è¡Œã«æ³¨ç›®
                        name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,8})', line)
                        if name_match:
                            potential_name = name_match.group(1)
                            if (potential_name not in ['è©³ç´°', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'é¸æŒ™', 'å€™è£œè€…', 'æ”¿æ²»å®¶', 'è‡ªæ°‘å…š', 'æ°‘ä¸»å…š'] and
                                len(potential_name) >= 2):
                                return potential_name
                
                current = current.parent
                search_levels += 1
            
            # æ–¹æ³•3: ã‚ˆã‚Šåºƒç¯„å›²ã®å…„å¼Ÿè¦ç´ æ¤œç´¢
            if link.parent:
                for sibling in link.parent.find_all_next(limit=10):
                    sibling_text = sibling.get_text(strip=True)
                    if sibling_text and len(sibling_text) <= 10:
                        name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,8})', sibling_text)
                        if name_match:
                            potential_name = name_match.group(1)
                            if potential_name not in ['è©³ç´°', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«']:
                                return potential_name
        
        except Exception as e:
            logger.debug(f"åå‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return name
    
    def extract_party_robust(self, link):
        """å …ç‰¢ãªæ”¿å…šæŠ½å‡º"""
        try:
            # åºƒç¯„å›²ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
            current = link.parent
            search_levels = 0
            
            while current and search_levels < 10:
                current_text = current.get_text()
                
                for party in self.parties:
                    if party in current_text:
                        return party
                
                current = current.parent
                search_levels += 1
            
        except Exception as e:
            logger.debug(f"æ”¿å…šæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return "æœªåˆ†é¡"

def quick_priority_fix():
    """å„ªå…ˆåº¦ä¿®æ­£ã®å®Ÿè¡Œ"""
    logger.info("ğŸš€ å„ªå…ˆåº¦ã®é«˜ã„éƒ½é“åºœçœŒã®è¿…é€Ÿä¿®æ­£é–‹å§‹...")
    
    fixer = QuickPriorityFixer()
    
    # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    current_candidates = data.get('data', [])
    logger.info(f"ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿: {len(current_candidates)}å")
    
    # å•é¡Œã®ã‚ã‚‹éƒ½é“åºœçœŒã‚’ä¿®æ­£
    fixed_candidates = []
    problem_prefectures = set()
    
    for pref_code, pref_name in fixer.priority_prefectures:
        problem_prefectures.add(pref_name)
    
    # å•é¡Œã®ãªã„éƒ½é“åºœçœŒã®ãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒ
    for candidate in current_candidates:
        if candidate['prefecture'] not in problem_prefectures:
            fixed_candidates.append(candidate)
    
    logger.info(f"ğŸ“Š ä¿æŒãƒ‡ãƒ¼ã‚¿: {len(fixed_candidates)}å")
    
    # å•é¡Œã®ã‚ã‚‹éƒ½é“åºœçœŒã‚’å†åé›†
    for pref_code, pref_name in fixer.priority_prefectures:
        logger.info(f"\n=== {pref_name} ä¿®æ­£å‡¦ç† ===")
        
        try:
            new_candidates = fixer.extract_all_candidates_robust(pref_code, pref_name)
            if new_candidates:
                fixed_candidates.extend(new_candidates)
                logger.info(f"âœ… {pref_name} ä¿®æ­£å®Œäº†: {len(new_candidates)}åè¿½åŠ ")
            else:
                logger.warning(f"âš ï¸ {pref_name} ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(1.5)
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} ä¿®æ­£ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    logger.info(f"\nğŸ“Š ä¿®æ­£å¾Œç·æ•°: {len(fixed_candidates)}å")
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    save_fixed_data(fixed_candidates, data_dir)

def save_fixed_data(candidates, data_dir):
    """ä¿®æ­£ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
    logger.info("ğŸ’¾ ä¿®æ­£ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜...")
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    
    for candidate in candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    data = {
        "metadata": {
            "data_type": "go2senkyo_priority_fixed_sangiin_2025",
            "collection_method": "priority_robust_extraction",
            "total_candidates": len(candidates),
            "generated_at": datetime.now().isoformat(),
            "source_site": "sangiin.go2senkyo.com",
            "coverage": {
                "constituency_types": 1,
                "parties": len(party_stats),
                "prefectures": len(prefecture_stats)
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": prefecture_stats,
            "by_constituency_type": {"single_member": len(candidates)}
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    priority_file = data_dir / f"go2senkyo_priority_fixed_{timestamp}.json"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(priority_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ä¿å­˜å®Œäº†: {priority_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    logger.info("\nğŸ“Š ä¿®æ­£å¾Œçµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(candidates)}å")
    logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
    logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_stats)}éƒ½é“åºœçœŒ")
    
    logger.info("\nğŸ—¾ éƒ½é“åºœçœŒåˆ¥çµ±è¨ˆï¼ˆå•é¡ŒãŒã‚ã£ãŸéƒ½é“åºœçœŒï¼‰:")
    problem_prefs = ["åŒ—æµ·é“", "å²©æ‰‹çœŒ", "ç¥å¥ˆå·çœŒ", "å²¡å±±çœŒ", "å®®å´çœŒ", "é¹¿å…å³¶çœŒ", 
                    "é’æ£®çœŒ", "å®®åŸçœŒ", "ç§‹ç”°çœŒ", "çŸ³å·çœŒ", "ç¦äº•çœŒ", "å±±æ¢¨çœŒ", 
                    "é•·é‡çœŒ", "å±±å£çœŒ", "æ„›åª›çœŒ", "æ²–ç¸„çœŒ"]
    
    for pref in problem_prefs:
        count = prefecture_stats.get(pref, 0)
        logger.info(f"  {pref}: {count}å")

if __name__ == "__main__":
    quick_priority_fix()