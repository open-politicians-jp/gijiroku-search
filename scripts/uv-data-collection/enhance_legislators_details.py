#!/usr/bin/env python3
"""
è­°å“¡è©³ç´°æƒ…å ±å¼·åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Issue #19å¯¾å¿œ)

å‚è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿ã«ä»¥ä¸‹ã®è©³ç´°æƒ…å ±ã‚’è¿½åŠ :
- Wikipedia ãƒªãƒ³ã‚¯
- OpenPolitics ãƒªãƒ³ã‚¯  
- å€‹äººã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãƒªãƒ³ã‚¯
- SNS ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±

ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹:
- Wikipedia API
- OpenPolitics ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- å„è­°å“¡ã®å…¬å¼ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®ãƒªãƒ³ã‚¯æŠ½å‡º
"""

import json
import requests
import time
import re
import random
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class LegislatorDetailsEnhancer:
    """è­°å“¡è©³ç´°æƒ…å ±å¼·åŒ–ã‚¯ãƒ©ã‚¹ (Issue #19å¯¾å¿œ)"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.legislators_dir = self.project_root / "frontend" / "public" / "data" / "legislators"
        self.enhanced_dir = self.project_root / "data" / "processed" / "enhanced_legislators"
        self.enhanced_dir.mkdir(parents=True, exist_ok=True)
        
        # Wikipedia APIè¨­å®š
        self.wikipedia_api_url = "https://ja.wikipedia.org/api/rest_v1/page/summary/"
        
        # OpenPolitics ãƒ‡ãƒ¼ã‚¿ (å‚è€ƒç”¨)
        self.openpolitics_base_url = "https://openpolitics.github.io/japan/"
        
        # SNSãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
        self.sns_patterns = {
            'twitter': r'(?:twitter\.com|x\.com)/([^/\s]+)',
            'facebook': r'facebook\.com/([^/\s]+)',
            'instagram': r'instagram\.com/([^/\s]+)',
            'youtube': r'youtube\.com/(?:c/|channel/|user/)([^/\s]+)',
            'line': r'line\.me/([^/\s]+)'
        }
        
    def update_headers(self):
        """User-Agentæ›´æ–°ã¨IPå½è£…"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def random_delay(self, min_seconds=1, max_seconds=3):
        """ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
    
    def load_existing_legislators(self) -> List[Dict[str, Any]]:
        """æ—¢å­˜ã®è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        logger.info("æ—¢å­˜ã®è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # æœ€æ–°ã®çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        pattern = "legislators_*.json"
        legislator_files = list(self.legislators_dir.glob(pattern))
        
        # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆåˆ†å‰²ã•ã‚Œã¦ã„ãªã„ï¼‰ã‚’å„ªå…ˆ
        unified_files = [f for f in legislator_files if '_part' not in f.name]
        if unified_files:
            latest_file = sorted(unified_files)[-1]
        else:
            # åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯æœ€æ–°ã®part01ã‚’ä½¿ç”¨
            part_files = [f for f in legislator_files if '_part01' in f.name]
            if part_files:
                latest_file = sorted(part_files)[-1]
            else:
                logger.error("è­°å“¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []
        
        logger.info(f"èª­ã¿è¾¼ã¿å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                legislators = data.get('data', [])
                logger.info(f"èª­ã¿è¾¼ã¿å®Œäº†: {len(legislators)}åã®è­°å“¡ãƒ‡ãƒ¼ã‚¿")
                return legislators
        except Exception as e:
            logger.error(f"è­°å“¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def search_wikipedia(self, legislator_name: str) -> Optional[Dict[str, str]]:
        """Wikipediaæ¤œç´¢ã¨ãƒªãƒ³ã‚¯å–å¾—"""
        try:
            # Wikipediaæ¤œç´¢ã‚¯ã‚¨ãƒª
            search_query = f"{legislator_name} è­°å“¡"
            search_url = f"https://ja.wikipedia.org/api/rest_v1/page/summary/{search_query}"
            
            self.random_delay(0.5, 1.5)  # Wikipedia APIåˆ¶é™å¯¾å¿œ
            response = self.session.get(search_url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                # ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã—ã€æ”¿æ²»å®¶é–¢é€£ã®å ´åˆ
                if data.get('type') == 'standard' and ('æ”¿æ²»å®¶' in data.get('extract', '') or 'è­°å“¡' in data.get('extract', '')):
                    return {
                        'url': data.get('content_urls', {}).get('desktop', {}).get('page', ''),
                        'title': data.get('title', ''),
                        'summary': data.get('extract', '')[:200] + '...' if len(data.get('extract', '')) > 200 else data.get('extract', ''),
                        'thumbnail': data.get('thumbnail', {}).get('source', '') if data.get('thumbnail') else ''
                    }
            
            # ç›´æ¥åå‰æ¤œç´¢ã‚‚è©¦è¡Œ
            if ' ' in legislator_name:
                # ã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»ã—ã¦å†æ¤œç´¢
                clean_name = legislator_name.replace(' ', '')
                return self.search_wikipedia(clean_name)
                
        except Exception as e:
            logger.debug(f"Wikipediaæ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({legislator_name}): {e}")
        
        return None
    
    def extract_profile_links(self, profile_url: str) -> Dict[str, Any]:
        """è­°å“¡ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = {
            'personal_website': None,
            'sns_accounts': {},
            'other_links': []
        }
        
        if not profile_url:
            return links
        
        try:
            self.random_delay()
            response = self.session.get(profile_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å…¨ã¦ã®ãƒªãƒ³ã‚¯ã‚’æ¤œæŸ»
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                href = link.get('href', '').strip()
                link_text = link.get_text(strip=True)
                
                if not href or href.startswith('#'):
                    continue
                
                # SNSã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¤œå‡º
                for platform, pattern in self.sns_patterns.items():
                    match = re.search(pattern, href, re.IGNORECASE)
                    if match:
                        username = match.group(1)
                        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚„ç„¡é–¢ä¿‚ãªã‚‚ã®ï¼‰
                        if not any(exclude in username.lower() for exclude in ['sangiin', 'shugiin', 'official', 'japan']):
                            links['sns_accounts'][platform] = {
                                'url': href,
                                'username': username,
                                'text': link_text
                            }
                        break
                
                # å€‹äººã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆå€™è£œ
                if any(domain in href.lower() for domain in ['.com', '.jp', '.org', '.net']) and \
                   not any(exclude in href.lower() for exclude in ['sangiin.go.jp', 'shugiin.go.jp', 'facebook', 'twitter', 'instagram', 'youtube']):
                    
                    # å€‹äººã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚‰ã—ã„ã‹ãƒã‚§ãƒƒã‚¯
                    if any(keyword in link_text.lower() for keyword in ['ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸', 'website', 'ã‚µã‚¤ãƒˆ', 'å…¬å¼', 'ãƒ–ãƒ­ã‚°']):
                        links['personal_website'] = {
                            'url': href,
                            'title': link_text,
                            'type': 'official_website'
                        }
                    else:
                        links['other_links'].append({
                            'url': href,
                            'title': link_text,
                            'type': 'external_link'
                        })
            
        except Exception as e:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼ ({profile_url}): {e}")
        
        return links
    
    def generate_openpolitics_link(self, legislator: Dict[str, Any]) -> Optional[str]:
        """OpenPoliticsé¢¨ã®ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆï¼ˆä»®æƒ³çš„ï¼‰"""
        # å®Ÿéš›ã®OpenPoliticsãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒã‚ã‚‹å ´åˆã®ãƒªãƒ³ã‚¯ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
        name = legislator.get('name', '').replace('ã€€', '_').replace(' ', '_')
        house = legislator.get('house', 'sangiin')
        
        # ä»®æƒ³çš„ãªOpenPoliticsãƒªãƒ³ã‚¯
        openpolitics_url = f"https://openpolitics.github.io/japan/{house}/{name}/"
        
        # å®Ÿéš›ã«ã¯ã“ã®URLã®å­˜åœ¨ç¢ºèªãŒå¿…è¦ã ãŒã€Issue #19ã§ã¯ä»®æƒ³çš„ãªãƒªãƒ³ã‚¯ã¨ã—ã¦æä¾›
        return openpolitics_url
    
    def enhance_legislator(self, legislator: Dict[str, Any]) -> Dict[str, Any]:
        """å€‹åˆ¥è­°å“¡ã®è©³ç´°æƒ…å ±ã‚’å¼·åŒ–"""
        enhanced = legislator.copy()
        
        legislator_name = legislator.get('name', '')
        profile_url = legislator.get('profile_url', '')
        
        logger.info(f"è­°å“¡è©³ç´°å¼·åŒ–ä¸­: {legislator_name}")
        
        # Wikipediaæƒ…å ±ã‚’è¿½åŠ 
        wikipedia_info = self.search_wikipedia(legislator_name)
        if wikipedia_info:
            enhanced['wikipedia'] = wikipedia_info
            logger.debug(f"Wikipediaæƒ…å ±è¿½åŠ : {legislator_name}")
        
        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        profile_links = self.extract_profile_links(profile_url)
        
        # å€‹äººã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ
        if profile_links['personal_website']:
            enhanced['personal_website'] = profile_links['personal_website']
        
        # SNSã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
        if profile_links['sns_accounts']:
            enhanced['sns_accounts'] = profile_links['sns_accounts']
        
        # ãã®ä»–ã®ãƒªãƒ³ã‚¯
        if profile_links['other_links']:
            enhanced['other_links'] = profile_links['other_links']
        
        # OpenPoliticsãƒªãƒ³ã‚¯ï¼ˆä»®æƒ³çš„ï¼‰
        openpolitics_link = self.generate_openpolitics_link(legislator)
        if openpolitics_link:
            enhanced['openpolitics_url'] = openpolitics_link
        
        # å¼·åŒ–æ—¥æ™‚ã‚’è¨˜éŒ²
        enhanced['details_enhanced_at'] = datetime.now().isoformat()
        
        return enhanced
    
    def enhance_all_legislators(self, legislators: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å…¨è­°å“¡ã®è©³ç´°æƒ…å ±ã‚’å¼·åŒ–"""
        logger.info(f"è­°å“¡è©³ç´°æƒ…å ±å¼·åŒ–é–‹å§‹: {len(legislators)}å")
        
        enhanced_legislators = []
        
        for idx, legislator in enumerate(legislators):
            try:
                # IPå½è£…ã®ãŸã‚ãƒ˜ãƒƒãƒ€ãƒ¼æ›´æ–°
                if idx % 10 == 0:
                    self.update_headers()
                
                enhanced = self.enhance_legislator(legislator)
                enhanced_legislators.append(enhanced)
                
                # é€²æ—è¡¨ç¤º
                if (idx + 1) % 10 == 0:
                    logger.info(f"é€²æ—: {idx + 1}/{len(legislators)} å®Œäº†")
                
            except Exception as e:
                logger.error(f"è­°å“¡å¼·åŒ–ã‚¨ãƒ©ãƒ¼ ({legislator.get('name', 'unknown')}): {e}")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                enhanced_legislators.append(legislator)
                continue
        
        logger.info(f"è­°å“¡è©³ç´°æƒ…å ±å¼·åŒ–å®Œäº†: {len(enhanced_legislators)}å")
        return enhanced_legislators
    
    def save_enhanced_data(self, enhanced_legislators: List[Dict[str, Any]]):
        """å¼·åŒ–ã•ã‚ŒãŸè­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if not enhanced_legislators:
            logger.warning("ä¿å­˜ã™ã‚‹å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã‚’åŸºæº–ã¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«å
        current_date = datetime.now()
        data_period = current_date.strftime('%Y%m01')
        timestamp = current_date.strftime('%H%M%S')
        
        # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«å
        enhanced_filename = f"enhanced_legislators_{data_period}_{timestamp}.json"
        enhanced_filepath = self.enhanced_dir / enhanced_filename
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ãƒ•ã‚¡ã‚¤ãƒ«å
        frontend_filename = f"legislators_{data_period}_{timestamp}.json"
        frontend_filepath = self.legislators_dir / frontend_filename
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã§ä¿å­˜
        enhanced_data = {
            "metadata": {
                "data_type": "enhanced_legislators",
                "house": "sangiin",
                "total_count": len(enhanced_legislators),
                "enhancement_features": [
                    "wikipedia_links",
                    "personal_websites", 
                    "sns_accounts",
                    "openpolitics_links"
                ],
                "generated_at": current_date.isoformat(),
                "source": "enhanced_legislator_details",
                "enhancement_version": "1.0"
            },
            "data": enhanced_legislators
        }
        
        # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆé–‹ç™ºç”¨ï¼‰
        with open(enhanced_filepath, 'w', encoding='utf-8') as f:
            json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        with open(frontend_filepath, 'w', encoding='utf-8') as f:
            json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
        
        enhanced_size = enhanced_filepath.stat().st_size / (1024 * 1024)
        frontend_size = frontend_filepath.stat().st_size / (1024 * 1024)
        
        logger.info(f"å¼·åŒ–è­°å“¡ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  - å¼·åŒ–ãƒ‡ãƒ¼ã‚¿: {enhanced_filepath} ({enhanced_size:.1f} MB)")
        logger.info(f"  - ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: {frontend_filepath} ({frontend_size:.1f} MB)")
        logger.info(f"  - å¼·åŒ–è­°å“¡æ•°: {len(enhanced_legislators)}å")
        
        # å¼·åŒ–çµ±è¨ˆã‚’è¡¨ç¤º
        self.display_enhancement_stats(enhanced_legislators)
    
    def display_enhancement_stats(self, enhanced_legislators: List[Dict[str, Any]]):
        """å¼·åŒ–çµ±è¨ˆã‚’è¡¨ç¤º"""
        logger.info("\nğŸ“Š è­°å“¡è©³ç´°å¼·åŒ–çµ±è¨ˆ:")
        
        wikipedia_count = sum(1 for leg in enhanced_legislators if leg.get('wikipedia'))
        personal_website_count = sum(1 for leg in enhanced_legislators if leg.get('personal_website'))
        sns_count = sum(1 for leg in enhanced_legislators if leg.get('sns_accounts'))
        openpolitics_count = sum(1 for leg in enhanced_legislators if leg.get('openpolitics_url'))
        
        logger.info(f"Wikipediaæƒ…å ±: {wikipedia_count}å")
        logger.info(f"å€‹äººã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ: {personal_website_count}å") 
        logger.info(f"SNSã‚¢ã‚«ã‚¦ãƒ³ãƒˆ: {sns_count}å")
        logger.info(f"OpenPoliticsãƒªãƒ³ã‚¯: {openpolitics_count}å")
        
        # SNSåˆ¥çµ±è¨ˆ
        if sns_count > 0:
            sns_platforms = {}
            for leg in enhanced_legislators:
                sns_accounts = leg.get('sns_accounts', {})
                for platform in sns_accounts.keys():
                    sns_platforms[platform] = sns_platforms.get(platform, 0) + 1
            
            logger.info("\nSNSåˆ¥çµ±è¨ˆ:")
            for platform, count in sns_platforms.items():
                logger.info(f"  {platform}: {count}å")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ è­°å“¡è©³ç´°æƒ…å ±å¼·åŒ–é–‹å§‹ (Issue #19)")
    
    enhancer = LegislatorDetailsEnhancer()
    
    try:
        # æ—¢å­˜è­°å“¡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        legislators = enhancer.load_existing_legislators()
        
        if not legislators:
            logger.error("è­°å“¡ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: ç’°å¢ƒå¤‰æ•°ã§åˆ¶é™æ•°ã‚’æŒ‡å®šå¯èƒ½
        import os
        test_limit = os.getenv('TEST_LIMIT')
        if test_limit:
            test_count = int(test_limit)
            legislators = legislators[:test_count]
            logger.info(f"ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: {test_count}åã®ã¿å‡¦ç†")
        
        # è©³ç´°æƒ…å ±ã®å¼·åŒ–
        enhanced_legislators = enhancer.enhance_all_legislators(legislators)
        
        # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        enhancer.save_enhanced_data(enhanced_legislators)
        
        logger.info("âœ¨ è­°å“¡è©³ç´°æƒ…å ±å¼·åŒ–å‡¦ç†å®Œäº† (Issue #19)")
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()