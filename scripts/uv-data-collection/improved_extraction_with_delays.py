#!/usr/bin/env python3
"""
é…å»¶ã‚’è€ƒæ…®ã—ãŸæ”¹è‰¯ç‰ˆãƒ‡ãƒ¼ã‚¿æŠ½å‡º
"""

import json
import logging
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ImprovedExtractionWithDelays:
    def __init__(self):
        self.session = requests.Session()
        ua = UserAgent()
        desktop_ua = ua.random
        
        self.session.headers.update({
            'User-Agent': desktop_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # ãƒ†ã‚¹ãƒˆç”¨å„ªå…ˆéƒ½é“åºœçœŒ
        self.test_prefectures = [
            (14, "ç¥å¥ˆå·çœŒ"),  # ç‰§å±±å•é¡Œã®ç¢ºèª
            (1, "åŒ—æµ·é“"),    # å¤§ããªçœŒã§ã®ãƒ†ã‚¹ãƒˆ
            (13, "æ±äº¬éƒ½"),   # æœ€å¤§æ•°ã§ã®ãƒ†ã‚¹ãƒˆ
        ]
        
        # æ”¿å…šãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
        self.parties = [
            "è‡ªç”±æ°‘ä¸»å…š", "ç«‹æ†²æ°‘ä¸»å…š", "æ—¥æœ¬ç¶­æ–°ã®ä¼š", "å…¬æ˜å…š", "æ—¥æœ¬å…±ç”£å…š",
            "å›½æ°‘æ°‘ä¸»å…š", "ã‚Œã„ã‚æ–°é¸çµ„", "å‚æ”¿å…š", "ç¤¾ä¼šæ°‘ä¸»å…š", "NHKå…š",
            "æ—¥æœ¬ä¿å®ˆå…š", "æ—¥æœ¬æ”¹é©å…š", "ç„¡æ‰€å±", "ç„¡æ‰€å±é€£åˆ", "æ—¥æœ¬èª çœŸä¼š",
            "æ—¥æœ¬ã®å®¶åº­ã‚’å®ˆã‚‹ä¼š", "å†ç”Ÿã®é“", "å·®åˆ¥æ’²æ»…å…š", "æ ¸èåˆå…š", "ãƒãƒ¼ãƒ ã¿ã‚‰ã„",
            "å¤šå¤«å¤šå¦»å…š", "å›½æ”¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ä¼š", "æ–°å…šã‚„ã¾ã¨", "æœªåˆ†é¡"
        ]
    
    def extract_with_proper_delays(self, pref_code: int, pref_name: str):
        """é©åˆ‡ãªé…å»¶ã‚’è€ƒæ…®ã—ãŸæŠ½å‡º"""
        url = f"https://sangiin.go2senkyo.com/2025/prefecture/{pref_code}"
        logger.info(f"ğŸ” {pref_name} (ã‚³ãƒ¼ãƒ‰: {pref_code}) é…å»¶è€ƒæ…®æŠ½å‡ºé–‹å§‹")
        
        try:
            # åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                logger.error(f"âŒ {pref_name} ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            logger.info(f"ğŸ“Š {pref_name} åˆå›HTMLå–å¾—: {len(response.text):,} æ–‡å­—")
            
            # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            logger.info(f"â³ {pref_name} å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„èª­ã¿è¾¼ã¿å¾…æ©Ÿä¸­...")
            time.sleep(3)  # 3ç§’å¾…æ©Ÿ
            
            # å†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ•ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
            response2 = self.session.get(url, timeout=30)
            logger.info(f"ğŸ“Š {pref_name} å†å–å¾—HTML: {len(response2.text):,} æ–‡å­—")
            
            soup = BeautifulSoup(response2.text, 'html.parser')
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯æ¤œç´¢
            profile_links = soup.find_all('a', href=re.compile(r'/seijika/\d+'))
            unique_ids = set()
            for link in profile_links:
                match = re.search(r'/seijika/(\d+)', link.get('href', ''))
                if match:
                    unique_ids.add(match.group(1))
            
            logger.info(f"ğŸ“Š {pref_name} ç™ºè¦‹ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«: {len(unique_ids)}å€‹")
            
            candidates = []
            processed_ids = set()
            
            # å„å€™è£œè€…ã®è©³ç´°æƒ…å ±ã‚’é †æ¬¡å–å¾—
            for i, candidate_id in enumerate(unique_ids):
                try:
                    # å€™è£œè€…è©³ç´°å–å¾—ï¼ˆã•ã‚‰ã«é…å»¶ï¼‰
                    candidate = self.extract_candidate_detailed(soup, candidate_id, pref_name, url, i)
                    if candidate and candidate_id not in processed_ids:
                        candidates.append(candidate)
                        processed_ids.add(candidate_id)
                        logger.info(f"  âœ… {i+1}: {candidate['name']} ({candidate['party']}) - ID: {candidate_id}")
                    
                    # å€‹åˆ¥å€™è£œè€…é–“ã®é…å»¶
                    time.sleep(0.5)
                
                except Exception as e:
                    logger.debug(f"å€™è£œè€… {candidate_id} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            logger.info(f"ğŸ¯ {pref_name} é…å»¶è€ƒæ…®æŠ½å‡ºå®Œäº†: {len(candidates)}å")
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def extract_candidate_detailed(self, soup: BeautifulSoup, candidate_id: str, prefecture: str, page_url: str, index: int):
        """è©³ç´°ãªå€™è£œè€…æƒ…å ±æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": f"go2s_{candidate_id}",
                "name": "",
                "prefecture": prefecture,
                "constituency": prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', ''),
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "profile_url": f"https://go2senkyo.com/seijika/{candidate_id}",
                "source_page": page_url,
                "source": "delayed_extraction",
                "collected_at": datetime.now().isoformat()
            }
            
            # è©²å½“ã™ã‚‹ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            target_links = soup.find_all('a', href=re.compile(f'/seijika/{candidate_id}'))
            
            if not target_links:
                return None
            
            # æœ€ã‚‚æƒ…å ±ãŒè±Šå¯Œãªãƒªãƒ³ã‚¯ã‚’é¸æŠ
            best_link = None
            max_context_length = 0
            
            for link in target_links:
                context_text = ""
                if link.parent:
                    context_text = link.parent.get_text()
                if len(context_text) > max_context_length:
                    max_context_length = len(context_text)
                    best_link = link
            
            if not best_link:
                best_link = target_links[0]
            
            # åå‰æŠ½å‡ºï¼ˆè¤‡æ•°æ–¹æ³•ã‚’çµ„ã¿åˆã‚ã›ï¼‰
            name = self.extract_name_comprehensive(best_link)
            if name:
                candidate["name"] = name
            else:
                # åå‰ãŒå–å¾—ã§ããªã„å ´åˆã¯è©³ç´°ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
                name = self.fetch_name_from_profile_page(candidate_id)
                if name:
                    candidate["name"] = name
                else:
                    return None
            
            # æ”¿å…šæŠ½å‡º
            party = self.extract_party_comprehensive(best_link)
            if party:
                candidate["party"] = party
                candidate["party_normalized"] = party
            
            return candidate
            
        except Exception as e:
            logger.debug(f"è©³ç´°å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_name_comprehensive(self, link):
        """åŒ…æ‹¬çš„ãªåå‰æŠ½å‡º"""
        name = ""
        
        try:
            # æ–¹æ³•1: ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ
            link_text = link.get_text(strip=True)
            if link_text and link_text not in ['è©³ç´°ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'è©³ç´°']:
                if re.match(r'[ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,}', link_text):
                    return link_text
            
            # æ–¹æ³•2: ç‰¹å®šã‚¯ãƒ©ã‚¹åã§ã®æ¤œç´¢
            current = link.parent
            search_depth = 0
            
            while current and search_depth < 10:
                # å€™è£œè€…åã‚’å«ã¿ãã†ãªã‚¯ãƒ©ã‚¹
                name_elements = current.find_all(class_=re.compile(r'name|title|candidate|person|profile'))
                for elem in name_elements:
                    text = elem.get_text(strip=True)
                    if text and len(text) <= 20:
                        # æ—¥æœ¬äººåãƒ‘ã‚¿ãƒ¼ãƒ³
                        name_patterns = [
                            r'([ä¸€-é¾¯]{2,4}[ã²ã‚‰ãŒãª]{0,4})',  # æ¼¢å­—ï¼‹ã²ã‚‰ãŒãª
                            r'([ä¸€-é¾¯]{2,6})',                 # æ¼¢å­—ã®ã¿
                            r'([ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,8})',  # æ··åˆ
                        ]
                        
                        for pattern in name_patterns:
                            matches = re.findall(pattern, text)
                            for match in matches:
                                if match not in ['è©³ç´°', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'é¸æŒ™', 'å€™è£œ', 'æ”¿å…š', 'è­°å“¡']:
                                    return match
                
                current = current.parent
                search_depth += 1
            
            # æ–¹æ³•3: ã‚ˆã‚Šåºƒç¯„å›²ã®ãƒ†ã‚­ã‚¹ãƒˆè§£æ
            if link.parent:
                parent_text = link.parent.get_text()
                text_lines = [line.strip() for line in parent_text.split('\n') if line.strip()]
                
                for line in text_lines:
                    if len(line) <= 15 and 'è©³ç´°' not in line and 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«' not in line:
                        name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,8})', line)
                        if name_match:
                            potential_name = name_match.group(1)
                            if len(potential_name) >= 2:
                                return potential_name
        
        except Exception as e:
            logger.debug(f"åŒ…æ‹¬çš„åå‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return name
    
    def fetch_name_from_profile_page(self, candidate_id: str):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰åå‰ã‚’ç›´æ¥å–å¾—"""
        try:
            profile_url = f"https://go2senkyo.com/seijika/{candidate_id}"
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰åå‰å–å¾—: {profile_url}")
            
            response = self.session.get(profile_url, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã®åå‰è¦ç´ ã‚’æ¤œç´¢
                name_selectors = [
                    'h1',
                    '.candidate-name',
                    '.profile-name',
                    '.person-name',
                    'title'
                ]
                
                for selector in name_selectors:
                    elements = soup.select(selector)
                    for elem in elements:
                        text = elem.get_text(strip=True)
                        if text and len(text) <= 20:
                            name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,8})', text)
                            if name_match:
                                return name_match.group(1)
            
            # å°‘ã—å¾…æ©Ÿã—ã¦ã‹ã‚‰æ¬¡ã®å‡¦ç†ã¸
            time.sleep(1)
        
        except Exception as e:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸åå‰å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return ""
    
    def extract_party_comprehensive(self, link):
        """åŒ…æ‹¬çš„ãªæ”¿å…šæŠ½å‡º"""
        try:
            current = link.parent
            search_depth = 0
            
            while current and search_depth < 12:
                current_text = current.get_text()
                
                for party in self.parties:
                    if party in current_text:
                        return party
                
                current = current.parent
                search_depth += 1
            
        except Exception as e:
            logger.debug(f"åŒ…æ‹¬çš„æ”¿å…šæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return "æœªåˆ†é¡"

def test_improved_extraction():
    """æ”¹è‰¯ç‰ˆæŠ½å‡ºã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("ğŸš€ é…å»¶è€ƒæ…®æ”¹è‰¯ç‰ˆæŠ½å‡ºã®ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    extractor = ImprovedExtractionWithDelays()
    
    # ãƒ†ã‚¹ãƒˆå¯¾è±¡éƒ½é“åºœçœŒ
    test_results = []
    
    for pref_code, pref_name in extractor.test_prefectures:
        logger.info(f"\n=== {pref_name} ãƒ†ã‚¹ãƒˆ ===")
        
        try:
            candidates = extractor.extract_with_proper_delays(pref_code, pref_name)
            test_results.extend(candidates)
            
            logger.info(f"âœ… {pref_name} ãƒ†ã‚¹ãƒˆå®Œäº†: {len(candidates)}å")
            
            # è©³ç´°è¡¨ç¤º
            logger.info(f"ğŸ“‹ {pref_name} å€™è£œè€…ä¸€è¦§:")
            for i, candidate in enumerate(candidates):
                logger.info(f"  {i+1}. {candidate['name']} ({candidate['party']}) - ID: {candidate['candidate_id']}")
            
            # ãƒ†ã‚¹ãƒˆé–“ã®å¾…æ©Ÿ
            time.sleep(2)
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    logger.info(f"\nğŸ¯ ãƒ†ã‚¹ãƒˆå®Œäº†: ç·è¨ˆ {len(test_results)}å")
    
    # çµæœä¿å­˜
    save_test_results(test_results)

def save_test_results(candidates):
    """ãƒ†ã‚¹ãƒˆçµæœã®ä¿å­˜"""
    logger.info("ğŸ’¾ ãƒ†ã‚¹ãƒˆçµæœã®ä¿å­˜...")
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    
    for candidate in candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    data = {
        "metadata": {
            "data_type": "go2senkyo_delayed_test_sangiin_2025",
            "collection_method": "delayed_comprehensive_extraction",
            "total_candidates": len(candidates),
            "generated_at": datetime.now().isoformat(),
            "source_site": "sangiin.go2senkyo.com",
            "test_prefectures": ["ç¥å¥ˆå·çœŒ", "åŒ—æµ·é“", "æ±äº¬éƒ½"]
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": prefecture_stats,
            "by_constituency_type": {"single_member": len(candidates)}
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    test_file = data_dir / f"go2senkyo_delayed_test_{timestamp}.json"
    
    with open(test_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ãƒ†ã‚¹ãƒˆçµæœä¿å­˜: {test_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    logger.info("\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœçµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(candidates)}å")
    
    for pref, count in prefecture_stats.items():
        logger.info(f"  {pref}: {count}å")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    name_counts = {}
    for candidate in candidates:
        name = candidate['name']
        prefecture = candidate['prefecture']
        key = f"{name} ({prefecture})"
        name_counts[key] = name_counts.get(key, 0) + 1
    
    duplicates = {k: v for k, v in name_counts.items() if v > 1}
    if duplicates:
        logger.info("\nâš ï¸ é‡è¤‡å€™è£œè€…:")
        for name_pref, count in duplicates.items():
            logger.info(f"  {name_pref}: {count}å›")

if __name__ == "__main__":
    test_improved_extraction()