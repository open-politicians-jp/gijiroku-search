#!/usr/bin/env python3
"""
å…¨éƒ½é“åºœçœŒã®å®Œå…¨ä¿®æ­£ç‰ˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
æ—¢å­˜ã®ä¸å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿®æ­£ç‰ˆæ‰‹æ³•ã§è£œå®Œ
"""

import json
import logging
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CompleteFixedDataGenerator:
    def __init__(self):
        self.session = requests.Session()
        ua = UserAgent()
        desktop_ua = ua.random
        
        self.session.headers.update({
            'User-Agent': desktop_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # æ”¿å…šãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
        self.parties = [
            "è‡ªç”±æ°‘ä¸»å…š", "ç«‹æ†²æ°‘ä¸»å…š", "æ—¥æœ¬ç¶­æ–°ã®ä¼š", "å…¬æ˜å…š", "æ—¥æœ¬å…±ç”£å…š",
            "å›½æ°‘æ°‘ä¸»å…š", "ã‚Œã„ã‚æ–°é¸çµ„", "å‚æ”¿å…š", "ç¤¾ä¼šæ°‘ä¸»å…š", "NHKå…š",
            "æ—¥æœ¬ä¿å®ˆå…š", "æ—¥æœ¬æ”¹é©å…š", "ç„¡æ‰€å±", "ç„¡æ‰€å±é€£åˆ", "æ—¥æœ¬èª çœŸä¼š",
            "æ—¥æœ¬ã®å®¶åº­ã‚’å®ˆã‚‹ä¼š", "å†ç”Ÿã®é“", "å·®åˆ¥æ’²æ»…å…š", "æ ¸èåˆå…š", "ãƒãƒ¼ãƒ ã¿ã‚‰ã„",
            "å¤šå¤«å¤šå¦»å…š", "å›½æ”¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ä¼š", "æ–°å…šã‚„ã¾ã¨", "æœªåˆ†é¡"
        ]

    def enhance_existing_candidate(self, candidate):
        """æ—¢å­˜å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’ä¿®æ­£ç‰ˆæ‰‹æ³•ã§å¼·åŒ–"""
        candidate_id = candidate.get('candidate_id', '').replace('go2s_', '')
        if not candidate_id:
            return candidate
        
        try:
            logger.debug(f"å€™è£œè€…å¼·åŒ–ä¸­: {candidate.get('name', 'unknown')} (ID: {candidate_id})")
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰æ­£ç¢ºãªæƒ…å ±ã‚’å–å¾—
            enhanced_info = self.get_accurate_candidate_info(candidate_id)
            if enhanced_info:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å¼·åŒ–
                candidate.update({
                    'name': enhanced_info.get('name', candidate.get('name', '')),
                    'name_kana': enhanced_info.get('name_kana', ''),
                    'party': enhanced_info.get('party', candidate.get('party', 'æœªåˆ†é¡')),
                    'party_normalized': enhanced_info.get('party', candidate.get('party', 'æœªåˆ†é¡')),
                    'profile_url': f"https://go2senkyo.com/seijika/{candidate_id}",
                    'source': 'enhanced_extraction',
                    'enhanced_at': datetime.now().isoformat()
                })
                
                return candidate
            
            # å¼·åŒ–ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            return candidate
            
        except Exception as e:
            logger.debug(f"å€™è£œè€…å¼·åŒ–ã‚¨ãƒ©ãƒ¼ {candidate_id}: {e}")
            return candidate

    def get_accurate_candidate_info(self, candidate_id: str):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰æ­£ç¢ºãªå€™è£œè€…æƒ…å ±ã‚’å–å¾—"""
        profile_url = f"https://go2senkyo.com/seijika/{candidate_id}"
        
        try:
            response = self.session.get(profile_url, timeout=15)
            if response.status_code != 200:
                logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {candidate_id}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # åå‰ã¨èª­ã¿ã®æŠ½å‡º
            name_info = self.extract_name_and_reading_from_profile(soup)
            if not name_info or not name_info.get('name'):
                return None
            
            # æ”¿å…šæƒ…å ±ã®æŠ½å‡º
            party = self.extract_party_from_profile(soup)
            
            return {
                'name': name_info['name'],
                'name_kana': name_info.get('reading', ''),
                'party': party
            }
            
        except Exception as e:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ {candidate_id}: {e}")
            return None

    def extract_name_and_reading_from_profile(self, soup: BeautifulSoup):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰åå‰ã¨èª­ã¿ã‚’æŠ½å‡º"""
        try:
            # æ–¹æ³•1: Go2senkyoç‰¹æœ‰ã®æ§‹é€ ã‚’ä½¿ç”¨
            name_elem = soup.find('h1', class_='p_seijika_profle_ttl')
            if name_elem:
                name_text = name_elem.get_text(strip=True)
                
                # èª­ã¿å–å¾—
                reading_elem = soup.find('p', class_='p_seijika_profle_subttl')
                reading_text = ""
                if reading_elem:
                    reading_full = reading_elem.get_text(strip=True)
                    reading_match = re.search(r'^([ã‚¡-ãƒ¶ãƒ¼\s]+)', reading_full)
                    if reading_match:
                        reading_text = reading_match.group(1).strip()
                
                return {"name": name_text, "reading": reading_text}
            
            # æ–¹æ³•2: titleã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
            title_elem = soup.find('title')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                
                # "ç‰§å±±ã²ã‚ãˆï¼ˆãƒã‚­ãƒ¤ãƒãƒ’ãƒ­ã‚¨ï¼‰ï½œæ”¿æ²»å®¶æƒ…å ±ï½œé¸æŒ™ãƒ‰ãƒƒãƒˆã‚³ãƒ " å½¢å¼
                title_match = re.search(r'^([ä¸€-é¾¯ã²ã‚‰ãŒãª\s]+)ï¼ˆ([ã‚¡-ãƒ¶ãƒ¼\s]+)ï¼‰', title_text)
                if title_match:
                    name = title_match.group(1).strip()
                    reading = title_match.group(2).strip()
                    return {"name": name, "reading": reading}
                
                # "å€™è£œè€…å | ã‚µã‚¤ãƒˆå" å½¢å¼
                if '|' in title_text:
                    name_part = title_text.split('|')[0].strip()
                    name_clean = re.sub(r'ï¼ˆ.*?ï¼‰', '', name_part).strip()
                    name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãª\s]+)', name_clean)
                    if name_match:
                        name = name_match.group(1).strip()
                        return {"name": name, "reading": ""}
            
        except Exception as e:
            logger.debug(f"åå‰ãƒ»èª­ã¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {"name": "", "reading": ""}

    def extract_party_from_profile(self, soup: BeautifulSoup):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰æ”¿å…šæƒ…å ±ã‚’æŠ½å‡º"""
        try:
            page_text = soup.get_text()
            
            for party in self.parties:
                if party in page_text:
                    return party
            
        except Exception as e:
            logger.debug(f"æ”¿å…šæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return "æœªåˆ†é¡"

def generate_complete_fixed_data():
    """å®Œå…¨ä¿®æ­£ç‰ˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
    logger.info("ğŸš€ å…¨éƒ½é“åºœçœŒå®Œå…¨ä¿®æ­£ç‰ˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹...")
    
    generator = CompleteFixedDataGenerator()
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_candidates = data.get('data', [])
    logger.info(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {len(original_candidates)}å")
    
    # é‡è¤‡é™¤å»
    unique_candidates = deduplicate_candidates(original_candidates)
    logger.info(f"ğŸ“Š é‡è¤‡é™¤å»å¾Œ: {len(unique_candidates)}å")
    
    # å„å€™è£œè€…ã‚’å¼·åŒ–
    enhanced_candidates = []
    
    for i, candidate in enumerate(unique_candidates):
        try:
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 20 == 0 or (i + 1) <= 5:
                logger.info(f"ğŸ“ é€²æ—: {i+1}/{len(unique_candidates)} - {candidate.get('name', 'unknown')}")
            
            # å€™è£œè€…å¼·åŒ–
            enhanced = generator.enhance_existing_candidate(candidate)
            enhanced_candidates.append(enhanced)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(0.8)
            
        except Exception as e:
            logger.error(f"âŒ å€™è£œè€… {i+1} å¼·åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            enhanced_candidates.append(candidate)
            continue
    
    logger.info(f"ğŸ¯ å¼·åŒ–å®Œäº†: {len(enhanced_candidates)}å")
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    save_complete_fixed_data(enhanced_candidates, data_dir)

def deduplicate_candidates(candidates):
    """å€™è£œè€…é‡è¤‡é™¤å»"""
    seen_ids = set()
    unique_candidates = []
    
    for candidate in candidates:
        candidate_id = candidate.get('candidate_id', '')
        if candidate_id and candidate_id not in seen_ids:
            seen_ids.add(candidate_id)
            unique_candidates.append(candidate)
    
    return unique_candidates

def save_complete_fixed_data(candidates, data_dir):
    """å®Œå…¨ä¿®æ­£ç‰ˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
    logger.info("ğŸ’¾ å®Œå…¨ä¿®æ­£ç‰ˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜...")
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    enhanced_count = 0
    
    for candidate in candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
        
        if candidate.get('name_kana'):
            enhanced_count += 1
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    data = {
        "metadata": {
            "data_type": "go2senkyo_complete_fixed_sangiin_2025",
            "collection_method": "existing_data_profile_enhancement",
            "total_candidates": len(candidates),
            "enhanced_candidates": enhanced_count,
            "generated_at": datetime.now().isoformat(),
            "source_site": "go2senkyo.com (profile pages)",
            "coverage": {
                "constituency_types": 1,
                "parties": len(party_stats),
                "prefectures": len(prefecture_stats)
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": prefecture_stats,
            "by_constituency_type": {"single_member": len(candidates)}
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    complete_file = data_dir / f"go2senkyo_complete_fixed_{timestamp}.json"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(complete_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ä¿å­˜å®Œäº†: {complete_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    logger.info("\nğŸ“Š å®Œå…¨ä¿®æ­£ç‰ˆçµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(candidates)}å")
    logger.info(f"  å¼·åŒ–æˆåŠŸ: {enhanced_count}å ({enhanced_count/len(candidates)*100:.1f}%)")
    logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
    logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_stats)}éƒ½é“åºœçœŒ")

if __name__ == "__main__":
    generate_complete_fixed_data()