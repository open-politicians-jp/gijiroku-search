#!/usr/bin/env python3
"""
å…¨éƒ½é“åºœçœŒã®åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åé›†ä¿®æ­£
"""

import json
import logging
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComprehensiveDataFixer:
    def __init__(self):
        self.session = requests.Session()
        ua = UserAgent()
        desktop_ua = ua.random
        
        self.session.headers.update({
            'User-Agent': desktop_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # éƒ½é“åºœçœŒãƒãƒƒãƒ”ãƒ³ã‚°
        self.prefectures = {
            1: "åŒ—æµ·é“", 2: "é’æ£®çœŒ", 3: "å²©æ‰‹çœŒ", 4: "å®®åŸçœŒ", 5: "ç§‹ç”°çœŒ", 6: "å±±å½¢çœŒ", 7: "ç¦å³¶çœŒ",
            8: "èŒ¨åŸçœŒ", 9: "æ ƒæœ¨çœŒ", 10: "ç¾¤é¦¬çœŒ", 11: "åŸ¼ç‰çœŒ", 12: "åƒè‘‰çœŒ", 13: "æ±äº¬éƒ½", 14: "ç¥å¥ˆå·çœŒ",
            15: "æ–°æ½ŸçœŒ", 16: "å¯Œå±±çœŒ", 17: "çŸ³å·çœŒ", 18: "ç¦äº•çœŒ", 19: "å±±æ¢¨çœŒ", 20: "é•·é‡çœŒ", 21: "å²é˜œçœŒ",
            22: "é™å²¡çœŒ", 23: "æ„›çŸ¥çœŒ", 24: "ä¸‰é‡çœŒ", 25: "æ»‹è³€çœŒ", 26: "äº¬éƒ½åºœ", 27: "å¤§é˜ªåºœ", 28: "å…µåº«çœŒ",
            29: "å¥ˆè‰¯çœŒ", 30: "å’Œæ­Œå±±çœŒ", 31: "é³¥å–çœŒ", 32: "å³¶æ ¹çœŒ", 33: "å²¡å±±çœŒ", 34: "åºƒå³¶çœŒ", 35: "å±±å£çœŒ",
            36: "å¾³å³¶çœŒ", 37: "é¦™å·çœŒ", 38: "æ„›åª›çœŒ", 39: "é«˜çŸ¥çœŒ", 40: "ç¦å²¡çœŒ", 41: "ä½è³€çœŒ", 42: "é•·å´çœŒ",
            43: "ç†Šæœ¬çœŒ", 44: "å¤§åˆ†çœŒ", 45: "å®®å´çœŒ", 46: "é¹¿å…å³¶çœŒ", 47: "æ²–ç¸„çœŒ"
        }
        
        # æ”¿å…šãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
        self.parties = [
            "è‡ªç”±æ°‘ä¸»å…š", "ç«‹æ†²æ°‘ä¸»å…š", "æ—¥æœ¬ç¶­æ–°ã®ä¼š", "å…¬æ˜å…š", "æ—¥æœ¬å…±ç”£å…š",
            "å›½æ°‘æ°‘ä¸»å…š", "ã‚Œã„ã‚æ–°é¸çµ„", "å‚æ”¿å…š", "ç¤¾ä¼šæ°‘ä¸»å…š", "NHKå…š",
            "æ—¥æœ¬ä¿å®ˆå…š", "æ—¥æœ¬æ”¹é©å…š", "ç„¡æ‰€å±", "ç„¡æ‰€å±é€£åˆ", "æ—¥æœ¬èª çœŸä¼š",
            "æ—¥æœ¬ã®å®¶åº­ã‚’å®ˆã‚‹ä¼š", "å†ç”Ÿã®é“", "å·®åˆ¥æ’²æ»…å…š", "æ ¸èåˆå…š", "ãƒãƒ¼ãƒ ã¿ã‚‰ã„",
            "å¤šå¤«å¤šå¦»å…š", "å›½æ”¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ä¼š", "æ–°å…šã‚„ã¾ã¨", "æœªåˆ†é¡"
        ]
    
    def get_prefecture_candidates_enhanced(self, pref_code: int):
        """å¼·åŒ–ã•ã‚ŒãŸéƒ½é“åºœçœŒå€™è£œè€…å–å¾—"""
        pref_name = self.prefectures.get(pref_code, f"æœªçŸ¥_{pref_code}")
        url = f"https://sangiin.go2senkyo.com/2025/prefecture/{pref_code}"
        
        logger.info(f"ğŸ” {pref_name} (ã‚³ãƒ¼ãƒ‰: {pref_code}) ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        
        try:
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                logger.error(f"âŒ {pref_name} ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            logger.debug(f"ğŸ“Š {pref_name} HTMLå–å¾—: {len(response.text):,} æ–‡å­—")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è¤‡æ•°ã®æŠ½å‡ºæ–¹æ³•ã‚’è©¦è¡Œ
            candidates = []
            
            # æ–¹æ³•1: å€™è£œè€…ãƒ–ãƒ­ãƒƒã‚¯ç›´æ¥æ¤œç´¢
            candidates.extend(self.extract_from_candidate_blocks(soup, pref_name, url))
            
            # æ–¹æ³•2: ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‹ã‚‰æŠ½å‡º
            if not candidates:
                candidates.extend(self.extract_from_profile_links(soup, pref_name, url))
            
            # æ–¹æ³•3: ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            if not candidates:
                candidates.extend(self.extract_from_text_patterns(soup, pref_name, url))
            
            # é‡è¤‡é™¤å»
            unique_candidates = self.deduplicate_candidates(candidates)
            
            logger.info(f"âœ… {pref_name} å–å¾—å®Œäº†: {len(unique_candidates)}å")
            
            return unique_candidates
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def extract_from_candidate_blocks(self, soup: BeautifulSoup, prefecture: str, page_url: str):
        """å€™è£œè€…ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰æŠ½å‡º"""
        candidates = []
        
        try:
            # è¤‡æ•°ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œ
            block_selectors = [
                'div.p_senkyoku_list_block',
                'div[class*="candidate"]',
                'div[class*="list_block"]',
                'div[class*="person"]'
            ]
            
            for selector in block_selectors:
                blocks = soup.select(selector)
                if blocks:
                    logger.debug(f"{prefecture}: {selector}ã§{len(blocks)}å€‹ã®ãƒ–ãƒ­ãƒƒã‚¯ç™ºè¦‹")
                    
                    for i, block in enumerate(blocks):
                        candidate = self.extract_candidate_from_block(block, prefecture, page_url, i)
                        if candidate:
                            candidates.append(candidate)
                    break
            
        except Exception as e:
            logger.debug(f"{prefecture} ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates
    
    def extract_from_profile_links(self, soup: BeautifulSoup, prefecture: str, page_url: str):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‹ã‚‰æŠ½å‡º"""
        candidates = []
        
        try:
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯æ¤œç´¢
            profile_links = soup.find_all('a', href=re.compile(r'/seijika/\d+'))
            logger.debug(f"{prefecture}: {len(profile_links)}å€‹ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ç™ºè¦‹")
            
            seen_ids = set()
            
            for i, link in enumerate(profile_links):
                try:
                    href = link.get('href', '')
                    match = re.search(r'/seijika/(\d+)', href)
                    if match:
                        candidate_id = match.group(1)
                        if candidate_id not in seen_ids:
                            seen_ids.add(candidate_id)
                            candidate = self.extract_candidate_from_link(link, prefecture, page_url, candidate_id)
                            if candidate:
                                candidates.append(candidate)
                except Exception as e:
                    logger.debug(f"ãƒªãƒ³ã‚¯ {i} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            logger.debug(f"{prefecture} ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates
    
    def extract_from_text_patterns(self, soup: BeautifulSoup, prefecture: str, page_url: str):
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æŠ½å‡º"""
        candidates = []
        
        try:
            # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å€™è£œè€…åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
            page_text = soup.get_text()
            
            # æ—¥æœ¬äººåãƒ‘ã‚¿ãƒ¼ãƒ³
            name_patterns = re.findall(r'([ä¸€-é¾¯]{1,2}[\sã€€]*[ä¸€-é¾¯ã²ã‚‰ãŒãª]{1,6})', page_text)
            
            for name in name_patterns:
                if len(name.strip()) >= 2:
                    candidate = {
                        "candidate_id": f"text_pattern_{len(candidates)}",
                        "name": name.strip(),
                        "prefecture": prefecture,
                        "constituency": prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', ''),
                        "constituency_type": "single_member",
                        "party": "æœªåˆ†é¡",
                        "party_normalized": "æœªåˆ†é¡",
                        "profile_url": "",
                        "source_page": page_url,
                        "source": "text_pattern_extraction",
                        "collected_at": datetime.now().isoformat()
                    }
                    candidates.append(candidate)
            
        except Exception as e:
            logger.debug(f"{prefecture} ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates
    
    def extract_candidate_from_block(self, block, prefecture: str, page_url: str, index: int):
        """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": f"block_{index}",
                "name": "",
                "prefecture": prefecture,
                "constituency": prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', ''),
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "profile_url": "",
                "source_page": page_url,
                "source": "block_extraction",
                "collected_at": datetime.now().isoformat()
            }
            
            # åå‰æŠ½å‡º
            name = self.extract_name_from_element(block)
            if name:
                candidate["name"] = name
            
            # æ”¿å…šæŠ½å‡º
            party = self.extract_party_from_element(block)
            if party:
                candidate["party"] = party
                candidate["party_normalized"] = party
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLæŠ½å‡º
            profile_link = block.find('a', href=re.compile(r'/seijika/\d+'))
            if profile_link:
                href = profile_link.get('href')
                if href:
                    candidate["profile_url"] = f"https://go2senkyo.com{href}"
                    match = re.search(r'/seijika/(\d+)', href)
                    if match:
                        candidate["candidate_id"] = f"go2s_{match.group(1)}"
            
            return candidate if candidate["name"] else None
            
        except Exception as e:
            logger.debug(f"ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_candidate_from_link(self, link, prefecture: str, page_url: str, candidate_id: str):
        """ãƒªãƒ³ã‚¯ã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": f"go2s_{candidate_id}",
                "name": "",
                "prefecture": prefecture,
                "constituency": prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', ''),
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "profile_url": f"https://go2senkyo.com{link.get('href')}",
                "source_page": page_url,
                "source": "link_extraction",
                "collected_at": datetime.now().isoformat()
            }
            
            # åå‰æŠ½å‡º
            name = self.extract_name_from_element(link.parent if link.parent else link)
            if name:
                candidate["name"] = name
            
            # æ”¿å…šæŠ½å‡º
            party = self.extract_party_from_element(link.parent if link.parent else link)
            if party:
                candidate["party"] = party
                candidate["party_normalized"] = party
            
            return candidate if candidate["name"] else None
            
        except Exception as e:
            logger.debug(f"ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_name_from_element(self, element):
        """è¦ç´ ã‹ã‚‰åå‰æŠ½å‡º"""
        try:
            text = element.get_text() if element else ""
            
            # æ—¥æœ¬äººåãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ
            name_patterns = [
                r'([ä¸€-é¾¯]{1,2}[\sã€€]*[ä¸€-é¾¯ã²ã‚‰ãŒãª]{1,6})',
                r'([ä¸€-é¾¯ã²ã‚‰ãŒãª]{2,8})',
            ]
            
            for pattern in name_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    name = match.strip().replace('\u3000', ' ')
                    # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰
                    if name not in ['è©³ç´°', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'é¸æŒ™', 'å€™è£œ', 'æ”¿å…š', 'ç„¡æ‰€å±'] and len(name) >= 2:
                        return name
            
        except Exception as e:
            logger.debug(f"åå‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return ""
    
    def extract_party_from_element(self, element):
        """è¦ç´ ã‹ã‚‰æ”¿å…šæŠ½å‡º"""
        try:
            text = element.get_text() if element else ""
            
            for party in self.parties:
                if party in text:
                    return party
            
        except Exception as e:
            logger.debug(f"æ”¿å…šæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return "æœªåˆ†é¡"
    
    def deduplicate_candidates(self, candidates):
        """å€™è£œè€…é‡è¤‡é™¤å»"""
        seen = set()
        unique = []
        
        for candidate in candidates:
            key = (candidate["name"], candidate["prefecture"])
            if key not in seen and candidate["name"]:
                seen.add(key)
                unique.append(candidate)
        
        return unique

def comprehensive_fix():
    """å…¨éƒ½é“åºœçœŒã®åŒ…æ‹¬çš„ä¿®æ­£"""
    logger.info("ğŸ”§ å…¨éƒ½é“åºœçœŒãƒ‡ãƒ¼ã‚¿ã®åŒ…æ‹¬çš„ä¿®æ­£é–‹å§‹...")
    
    fixer = ComprehensiveDataFixer()
    all_candidates = []
    
    # å…¨éƒ½é“åºœçœŒã‚’å‡¦ç†
    for pref_code in range(1, 48):  # 1-47
        try:
            pref_name = fixer.prefectures[pref_code]
            logger.info(f"\n=== {pref_code}/47: {pref_name} ===")
            
            candidates = fixer.get_prefecture_candidates_enhanced(pref_code)
            all_candidates.extend(candidates)
            
            logger.info(f"ğŸ“Š é€²æ—: {pref_code}/47 å®Œäº† - ç·å€™è£œè€…: {len(all_candidates)}å")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            time.sleep(1)
            
        except Exception as e:
            logger.error(f"âŒ {pref_code} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    logger.info(f"\nğŸ¯ å…¨åé›†å®Œäº†: {len(all_candidates)}å")
    
    # ãƒ‡ãƒ¼ã‚¿æ•´ç†ãƒ»ä¿å­˜
    save_comprehensive_data(all_candidates)

def save_comprehensive_data(candidates):
    """åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
    logger.info("ğŸ’¾ åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜é–‹å§‹...")
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    
    for candidate in candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    data = {
        "metadata": {
            "data_type": "go2senkyo_comprehensive_fixed_sangiin_2025",
            "collection_method": "comprehensive_enhanced_extraction",
            "total_candidates": len(candidates),
            "generated_at": datetime.now().isoformat(),
            "source_site": "sangiin.go2senkyo.com",
            "coverage": {
                "constituency_types": 1,
                "parties": len(party_stats),
                "prefectures": len(prefecture_stats)
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": prefecture_stats,
            "by_constituency_type": {"single_member": len(candidates)}
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    comprehensive_file = data_dir / f"go2senkyo_comprehensive_{timestamp}.json"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(comprehensive_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ä¿å­˜å®Œäº†: {comprehensive_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    logger.info("\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(candidates)}å")
    logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
    logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_stats)}éƒ½é“åºœçœŒ")
    
    logger.info("\nğŸ›ï¸ éƒ½é“åºœçœŒåˆ¥çµ±è¨ˆï¼ˆä¸Šä½15ï¼‰:")
    top_prefectures = sorted(prefecture_stats.items(), key=lambda x: x[1], reverse=True)[:15]
    for pref, count in top_prefectures:
        logger.info(f"  {pref}: {count}å")

if __name__ == "__main__":
    comprehensive_fix()