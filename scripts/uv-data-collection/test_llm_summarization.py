#!/usr/bin/env python3
"""
è»½é‡LLMè¦ç´„ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

GitHub Actionsç’°å¢ƒã§ã®è»½é‡ãƒ¢ãƒ‡ãƒ«å‹•ä½œç¢ºèªã¨è¦ç´„å“è³ªè©•ä¾¡
- Phi-3.5 Mini, Qwen2.5, Llama3.2ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
- ä¸€ã¤ã®ä¼šè­°ãƒ‡ãƒ¼ã‚¿ã§ã®è©³ç´°æ¤œè¨¼
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ»å‡¦ç†æ™‚é–“ã®è¨ˆæ¸¬
"""

import json
import time
import psutil
import requests
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class LLMSummarizationTester:
    """è»½é‡LLMè¦ç´„ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.speeches_dir = self.project_root / "frontend" / "public" / "data" / "speeches"
        self.test_results_dir = self.project_root / "data" / "test_results"
        self.test_results_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡å„ªå…ˆï¼‰
        self.test_models = [
            {
                'name': 'phi3.5',
                'size': '3.8B',
                'description': 'Microsoft Phi-3.5 Mini (æœ€è»½é‡)',
                'ollama_model': 'phi3.5:latest'
            },
            {
                'name': 'qwen2.5:3b',
                'size': '3B',
                'description': 'Qwen 2.5 3B (æ—¥æœ¬èªå¼·åŒ–)',
                'ollama_model': 'qwen2.5:3b'
            },
            {
                'name': 'llama3.2:3b',
                'size': '3B', 
                'description': 'Llama 3.2 3B (ãƒãƒ©ãƒ³ã‚¹å‹)',
                'ollama_model': 'llama3.2:3b'
            }
        ]
        
        # Ollamaè¨­å®š
        self.ollama_url = "http://localhost:11434/api/generate"
        self.ollama_available = self.check_ollama_availability()
        
    def check_ollama_availability(self) -> bool:
        """Ollamaæ¥ç¶šç¢ºèª"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                available_models = [model['name'] for model in response.json().get('models', [])]
                logger.info(f"âœ… Ollamaåˆ©ç”¨å¯èƒ½ã€ãƒ¢ãƒ‡ãƒ«æ•°: {len(available_models)}")
                logger.info(f"åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {', '.join(available_models[:5])}")
                return True
            else:
                logger.warning("âš ï¸ Ollamaæ¥ç¶šå¤±æ•—")
                return False
        except Exception as e:
            logger.warning(f"âš ï¸ Ollamaæœªèµ·å‹•: {e}")
            return False
    
    def get_sample_meeting_data(self) -> Optional[Dict[str, Any]]:
        """ã‚µãƒ³ãƒ—ãƒ«ä¼šè­°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        logger.info("ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ä¼šè­°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        # æœ€æ–°ã®ã‚¹ãƒ”ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        speech_files = list(self.speeches_dir.glob("speeches_*.json"))
        if not speech_files:
            logger.error("âŒ è­°äº‹éŒ²ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        latest_file = sorted(speech_files)[-1]
        logger.info(f"ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                speeches = data if isinstance(data, list) else data.get('data', [])
                
                # æœ€åˆã®ä¼šè­°ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                if speeches:
                    first_speech = speeches[0]
                    meeting_key = f"{first_speech.get('date', 'unknown')}_{first_speech.get('house', 'unknown')}_{first_speech.get('committee', 'æœ¬ä¼šè­°')}"
                    
                    # åŒã˜ä¼šè­°ã®å…¨ç™ºè¨€ã‚’åé›†
                    meeting_speeches = []
                    for speech in speeches:
                        speech_key = f"{speech.get('date', 'unknown')}_{speech.get('house', 'unknown')}_{speech.get('committee', 'æœ¬ä¼šè­°')}"
                        if speech_key == meeting_key:
                            meeting_speeches.append(speech)
                        if len(meeting_speeches) >= 20:  # ãƒ†ã‚¹ãƒˆç”¨ã«åˆ¶é™
                            break
                    
                    meeting_info = {
                        'key': meeting_key,
                        'date': first_speech.get('date', 'unknown'),
                        'house': first_speech.get('house', 'unknown'),
                        'committee': first_speech.get('committee', 'æœ¬ä¼šè­°'),
                        'speeches': meeting_speeches,
                        'speech_count': len(meeting_speeches),
                        'speakers': list(set(s.get('speaker', 'ä¸æ˜') for s in meeting_speeches)),
                        'parties': list(set(s.get('party', 'ä¸æ˜') for s in meeting_speeches if s.get('party')))
                    }
                    
                    logger.info(f"âœ… ã‚µãƒ³ãƒ—ãƒ«ä¼šè­°: {meeting_key}")
                    logger.info(f"ğŸ“Š ç™ºè¨€æ•°: {len(meeting_speeches)}ã€ç™ºè¨€è€…: {len(meeting_info['speakers'])}å")
                    return meeting_info
                else:
                    logger.error("âŒ è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                    return None
                    
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def prepare_meeting_text(self, meeting_info: Dict[str, Any]) -> str:
        """ä¼šè­°ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ç”¨ã«æº–å‚™"""
        speeches = meeting_info['speeches']
        text_parts = []
        
        for speech in speeches[:15]:  # ãƒ†ã‚¹ãƒˆç”¨ã«ç™ºè¨€æ•°åˆ¶é™
            speaker = speech.get('speaker', 'ä¸æ˜')
            party = speech.get('party', 'ä¸æ˜')
            text = speech.get('text', '')
            
            if len(text.strip()) > 20:  # çŸ­ã™ãã‚‹ç™ºè¨€ã¯é™¤å¤–
                text_parts.append(f"ã€{speaker}({party})ã€‘{text[:800]}")
        
        full_text = '\n\n'.join(text_parts)
        
        # é•·ã™ãã‚‹å ´åˆã¯åˆ¶é™ï¼ˆè»½é‡ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰
        if len(full_text) > 6000:
            full_text = full_text[:6000] + '...'
        
        return full_text
    
    def create_japanese_prompt(self, meeting_info: Dict[str, Any], meeting_text: str) -> str:
        """æ—¥æœ¬èªç‰¹åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
        prompt = f"""ä»¥ä¸‹ã®å›½ä¼š{meeting_info['committee']}ä¼šè­°ï¼ˆ{meeting_info['date']}ï¼‰ã®è­°äº‹éŒ²ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ä¼šè­°æƒ…å ±:
- æ—¥ä»˜: {meeting_info['date']}
- å§”å“¡ä¼š: {meeting_info['committee']}  
- é™¢: {meeting_info['house']}
- ç™ºè¨€è€…æ•°: {len(meeting_info['speakers'])}å
- å‚åŠ æ”¿å…š: {', '.join(meeting_info['parties'][:5])}

è­°äº‹éŒ²å†…å®¹:
{meeting_text}

ä»¥ä¸‹ã®å½¢å¼ã§è¦ç´„ã—ã¦ãã ã•ã„:

ã€ä¼šè­°æ¦‚è¦ã€‘
(ã“ã®ä¼šè­°ã§ä½•ãŒè­°è«–ã•ã‚ŒãŸã‹ã‚’2-3è¡Œã§èª¬æ˜)

ã€ä¸»è¦è­°è«–ãƒã‚¤ãƒ³ãƒˆã€‘
1. (ç¬¬1ã®é‡è¦ãªè­°è«–å†…å®¹)
2. (ç¬¬2ã®é‡è¦ãªè­°è«–å†…å®¹) 
3. (ç¬¬3ã®é‡è¦ãªè­°è«–å†…å®¹)

ã€çµè«–ãƒ»åˆæ„äº‹é …ã€‘
(ä¼šè­°ã®çµè«–ã‚„æ±ºå®šäº‹é …ã€ãªã‘ã‚Œã°ä¸»è¦ãªåˆæ„ç‚¹)

ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚"""
        
        return prompt
    
    def test_model_summary(self, model_info: Dict[str, str], meeting_info: Dict[str, Any], meeting_text: str) -> Dict[str, Any]:
        """å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã§ã®è¦ç´„ãƒ†ã‚¹ãƒˆ"""
        model_name = model_info['name']
        logger.info(f"ğŸ§ª {model_name} è¦ç´„ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        start_time = time.time()
        
        try:
            prompt = self.create_japanese_prompt(meeting_info, meeting_text)
            
            # Ollama APIå‘¼ã³å‡ºã—
            payload = {
                "model": model_info['ollama_model'],
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,
                    "max_tokens": 1000,
                    "num_predict": 1000
                }
            }
            
            response = requests.post(self.ollama_url, json=payload, timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            response.raise_for_status()
            
            result = response.json()
            summary_text = result.get('response', '')
            
            end_time = time.time()
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            
            # è¦ç´„å“è³ªè©•ä¾¡
            quality_score = self.evaluate_summary_quality(summary_text, meeting_info)
            
            test_result = {
                'model': model_name,
                'model_info': model_info,
                'success': True,
                'summary': summary_text,
                'processing_time': round(end_time - start_time, 2),
                'memory_usage': round(memory_after - memory_before, 2),
                'memory_peak': round(memory_after, 2),
                'quality_score': quality_score,
                'summary_length': len(summary_text),
                'tested_at': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… {model_name} è¦ç´„å®Œäº†:")
            logger.info(f"   â±ï¸ å‡¦ç†æ™‚é–“: {test_result['processing_time']}ç§’")
            logger.info(f"   ğŸ§  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {test_result['memory_usage']}MB")
            logger.info(f"   ğŸ“Š å“è³ªã‚¹ã‚³ã‚¢: {quality_score}/10")
            
            return test_result
            
        except Exception as e:
            end_time = time.time()
            memory_after = process.memory_info().rss / 1024 / 1024
            
            logger.error(f"âŒ {model_name} è¦ç´„å¤±æ•—: {str(e)}")
            
            return {
                'model': model_name,
                'model_info': model_info,
                'success': False,
                'error': str(e),
                'processing_time': round(end_time - start_time, 2),
                'memory_usage': round(memory_after - memory_before, 2),
                'tested_at': datetime.now().isoformat()
            }
    
    def evaluate_summary_quality(self, summary: str, meeting_info: Dict[str, Any]) -> int:
        """è¦ç´„å“è³ªè©•ä¾¡ï¼ˆ1-10ç‚¹ï¼‰"""
        score = 0
        
        # åŸºæœ¬æ§‹é€ ãƒã‚§ãƒƒã‚¯
        if 'ã€ä¼šè­°æ¦‚è¦ã€‘' in summary:
            score += 2
        if 'ã€ä¸»è¦è­°è«–ãƒã‚¤ãƒ³ãƒˆã€‘' in summary:
            score += 2
        if 'ã€çµè«–' in summary or 'ã€åˆæ„äº‹é …ã€‘' in summary:
            score += 2
        
        # å†…å®¹ã®å……å®Ÿåº¦
        if len(summary) > 200:
            score += 1
        if len(summary) > 400:
            score += 1
        
        # æ—¥æœ¬èªã®è‡ªç„¶ã•ï¼ˆç°¡æ˜“ãƒã‚§ãƒƒã‚¯ï¼‰
        if 'ã€‚' in summary and 'ã€' in summary:
            score += 1
        
        # ä¼šè­°æƒ…å ±ã®å«æœ‰
        if meeting_info['committee'] in summary:
            score += 1
        
        return min(score, 10)
    
    def run_comparison_test(self) -> Dict[str, Any]:
        """å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸš€ è»½é‡LLMè¦ç´„æ¯”è¼ƒãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        if not self.ollama_available:
            logger.error("âŒ Ollamaæœªèµ·å‹•ã®ãŸã‚ãƒ†ã‚¹ãƒˆä¸­æ­¢")
            return {'error': 'Ollama not available'}
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æº–å‚™
        meeting_info = self.get_sample_meeting_data()
        if not meeting_info:
            return {'error': 'No meeting data available'}
        
        meeting_text = self.prepare_meeting_text(meeting_info)
        
        logger.info(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆå¯¾è±¡ä¼šè­°: {meeting_info['key']}")
        logger.info(f"ğŸ“„ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(meeting_text)}æ–‡å­—")
        
        # å„ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_results = []
        for model_info in self.test_models:
            result = self.test_model_summary(model_info, meeting_info, meeting_text)
            test_results.append(result)
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            time.sleep(2)
        
        # çµæœã¾ã¨ã‚
        comparison_result = {
            'test_overview': {
                'meeting_key': meeting_info['key'],
                'input_length': len(meeting_text),
                'models_tested': len(test_results),
                'successful_tests': sum(1 for r in test_results if r.get('success', False)),
                'tested_at': datetime.now().isoformat()
            },
            'meeting_info': meeting_info,
            'model_results': test_results,
            'recommendations': self.generate_recommendations(test_results)
        }
        
        # çµæœä¿å­˜
        self.save_test_results(comparison_result)
        
        return comparison_result
    
    def generate_recommendations(self, test_results: List[Dict[str, Any]]) -> Dict[str, str]:
        """ãƒ†ã‚¹ãƒˆçµæœåŸºæº–ã®æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        successful_results = [r for r in test_results if r.get('success', False)]
        
        if not successful_results:
            return {'status': 'No successful tests', 'recommendation': 'Check Ollama setup and model availability'}
        
        # æ€§èƒ½è©•ä¾¡
        best_speed = min(successful_results, key=lambda x: x.get('processing_time', float('inf')))
        best_memory = min(successful_results, key=lambda x: x.get('memory_usage', float('inf')))
        best_quality = max(successful_results, key=lambda x: x.get('quality_score', 0))
        
        recommendations = {
            'fastest_model': f"{best_speed['model']} ({best_speed['processing_time']}ç§’)",
            'most_memory_efficient': f"{best_memory['model']} ({best_memory['memory_usage']}MB)",
            'highest_quality': f"{best_quality['model']} (å“è³ª: {best_quality['quality_score']}/10)",
            'overall_recommendation': self.select_best_overall_model(successful_results)
        }
        
        return recommendations
    
    def select_best_overall_model(self, results: List[Dict[str, Any]]) -> str:
        """ç·åˆè©•ä¾¡ã§æœ€é©ãƒ¢ãƒ‡ãƒ«é¸å®š"""
        if not results:
            return "No successful models"
        
        # é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå“è³ª40%ã€é€Ÿåº¦30%ã€ãƒ¡ãƒ¢ãƒª30%ï¼‰
        scored_results = []
        max_quality = max(r.get('quality_score', 0) for r in results)
        min_time = min(r.get('processing_time', float('inf')) for r in results)
        min_memory = min(r.get('memory_usage', float('inf')) for r in results)
        
        for result in results:
            quality_score = (result.get('quality_score', 0) / max_quality) * 0.4 if max_quality > 0 else 0
            speed_score = (min_time / result.get('processing_time', float('inf'))) * 0.3
            memory_score = (min_memory / result.get('memory_usage', float('inf'))) * 0.3
            
            total_score = quality_score + speed_score + memory_score
            scored_results.append((result['model'], total_score))
        
        best_model = max(scored_results, key=lambda x: x[1])
        return f"{best_model[0]} (ç·åˆã‚¹ã‚³ã‚¢: {best_model[1]:.3f})"
    
    def save_test_results(self, results: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆçµæœä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"llm_comparison_test_{timestamp}.json"
        filepath = self.test_results_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ ãƒ†ã‚¹ãƒˆçµæœä¿å­˜: {filepath}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self.display_test_summary(results)
    
    def display_test_summary(self, results: Dict[str, Any]):
        """ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š è»½é‡LLMè¦ç´„ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        logger.info("="*60)
        
        overview = results['test_overview']
        logger.info(f"ğŸ¯ ãƒ†ã‚¹ãƒˆå¯¾è±¡: {overview['meeting_key']}")
        logger.info(f"ğŸ“ å…¥åŠ›é•·: {overview['input_length']}æ–‡å­—")
        logger.info(f"âœ… æˆåŠŸ: {overview['successful_tests']}/{overview['models_tested']}ãƒ¢ãƒ‡ãƒ«")
        
        logger.info("\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«åˆ¥çµæœ:")
        for result in results['model_results']:
            if result.get('success'):
                logger.info(f"  {result['model']}: å“è³ª{result['quality_score']}/10, {result['processing_time']}ç§’, {result['memory_usage']}MB")
            else:
                logger.info(f"  {result['model']}: âŒ {result.get('error', 'Unknown error')}")
        
        logger.info("\nğŸ† æ¨å¥¨äº‹é …:")
        recommendations = results['recommendations']
        for key, value in recommendations.items():
            logger.info(f"  {key}: {value}")
        
        logger.info("="*60)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸ§ª è»½é‡LLMè¦ç´„å“è³ªãƒ»æ€§èƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    tester = LLMSummarizationTester()
    
    try:
        results = tester.run_comparison_test()
        
        if 'error' in results:
            logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {results['error']}")
            return
        
        logger.info("âœ¨ è»½é‡LLMè¦ç´„ãƒ†ã‚¹ãƒˆå®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()