#!/usr/bin/env python3
"""
æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿èª¿æŸ»
"""

import requests
import logging
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def investigate_proportional():
    """æ¯”ä¾‹ä»£è¡¨ãƒšãƒ¼ã‚¸ã‚’èª¿æŸ»"""
    logger.info("ðŸ” æ¯”ä¾‹ä»£è¡¨ãƒšãƒ¼ã‚¸èª¿æŸ»é–‹å§‹...")
    
    ua = UserAgent()
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
    })
    
    base_url = "https://sangiin.go2senkyo.com"
    
    # èª¿æŸ»ã™ã‚‹URLå€™è£œ
    candidate_urls = [
        f"{base_url}/2025",
        f"{base_url}/2025/seitou",
        f"{base_url}/2025/hirei",
        f"{base_url}/2025/proportional",
        f"{base_url}/2025/party",
        f"{base_url}/2025/list"
    ]
    
    for url in candidate_urls:
        try:
            logger.info(f"ðŸ“ èª¿æŸ»ä¸­: {url}")
            response = session.get(url, timeout=30)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                title = soup.find('title')
                
                logger.info(f"âœ… ã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ: {url}")
                logger.info(f"  ã‚¿ã‚¤ãƒˆãƒ«: {title.get_text() if title else 'N/A'}")
                
                # æ¯”ä¾‹ä»£è¡¨é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’æŽ¢ã™
                proportional_links = []
                all_links = soup.find_all('a', href=True)
                
                for link in all_links:
                    href = link.get('href', '').lower()
                    text = link.get_text(strip=True).lower()
                    
                    if any(keyword in href or keyword in text for keyword in 
                          ['hirei', 'proportional', 'seitou', 'party', 'list', 'æ¯”ä¾‹', 'æ”¿å…š']):
                        proportional_links.append({
                            'text': link.get_text(strip=True),
                            'href': link.get('href')
                        })
                
                if proportional_links:
                    logger.info(f"  æ¯”ä¾‹ä»£è¡¨é–¢é€£ãƒªãƒ³ã‚¯: {len(proportional_links)}å€‹")
                    for i, link in enumerate(proportional_links[:5]):
                        logger.info(f"    {i+1}: {link['text']} -> {link['href']}")
                
                # ãƒšãƒ¼ã‚¸æ§‹é€ ã®åˆ†æž
                candidate_blocks = soup.find_all(['div', 'li', 'article'], class_=True)
                relevant_blocks = []
                
                for block in candidate_blocks[:20]:  # æœ€åˆã®20å€‹ã®ã¿ãƒã‚§ãƒƒã‚¯
                    class_names = ' '.join(block.get('class', []))
                    if any(keyword in class_names.lower() for keyword in 
                          ['candidate', 'person', 'member', 'seitou', 'party']):
                        relevant_blocks.append({
                            'tag': block.name,
                            'class': class_names,
                            'text': block.get_text(strip=True)[:100]
                        })
                
                if relevant_blocks:
                    logger.info(f"  é–¢é€£ãƒ–ãƒ­ãƒƒã‚¯: {len(relevant_blocks)}å€‹")
                    for i, block in enumerate(relevant_blocks[:3]):
                        logger.info(f"    {i+1}: {block['tag']}.{block['class']} - {block['text']}...")
                
                logger.info("")  # åŒºåˆ‡ã‚Š
                
            else:
                logger.debug(f"âŒ ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {url} (HTTP {response.status_code})")
                
        except Exception as e:
            logger.debug(f"âŒ ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            continue
    
    # æ”¿å…šä¸€è¦§ãƒšãƒ¼ã‚¸ã®è©³ç´°èª¿æŸ»
    logger.info("ðŸ” æ”¿å…šãƒšãƒ¼ã‚¸è©³ç´°èª¿æŸ»...")
    seitou_url = f"{base_url}/2025/seitou"
    
    try:
        response = session.get(seitou_url, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ”¿å…šãƒªãƒ³ã‚¯ã‚’æŽ¢ã™
            party_links = soup.find_all('a', href=True)
            party_candidates = []
            
            for link in party_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # æ”¿å…šIDãŒå«ã¾ã‚Œã¦ã„ã‚‹ãƒªãƒ³ã‚¯ã‚’æŽ¢ã™
                if '/seitou/' in href and text:
                    party_candidates.append({
                        'party_name': text,
                        'url': href if href.startswith('http') else f"{base_url}{href}"
                    })
            
            logger.info(f"ðŸ“‹ ç™ºè¦‹ã•ã‚ŒãŸæ”¿å…š: {len(party_candidates)}å€‹")
            for i, party in enumerate(party_candidates[:10]):
                logger.info(f"  {i+1}: {party['party_name']} -> {party['url']}")
        
    except Exception as e:
        logger.error(f"æ”¿å…šãƒšãƒ¼ã‚¸èª¿æŸ»ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    investigate_proportional()