#!/usr/bin/env python3
"""
ä¸»è¦éƒ½é“åºœçœŒé«˜é€Ÿåé›†ï¼ˆåŸºæœ¬æƒ…å ±ã®ã¿ï¼‰
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from collect_go2senkyo_optimized import Go2senkyoOptimizedCollector

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def quick_collect():
    """é«˜é€Ÿåé›†ï¼ˆè©³ç´°ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãªã—ï¼‰"""
    logger.info("ğŸš€ ä¸»è¦éƒ½é“åºœçœŒé«˜é€Ÿåé›†é–‹å§‹...")
    
    # ä¸»è¦éƒ½é“åºœçœŒ
    prefectures = {
        "æ±äº¬éƒ½": 13, "å¤§é˜ªåºœ": 27, "ç¥å¥ˆå·çœŒ": 14, "æ„›çŸ¥çœŒ": 23,
        "åŸ¼ç‰çœŒ": 11, "åƒè‘‰çœŒ": 12, "å…µåº«çœŒ": 28, "ç¦å²¡çœŒ": 40,
        "åŒ—æµ·é“": 1, "é™å²¡çœŒ": 22, "åºƒå³¶çœŒ": 34, "äº¬éƒ½åºœ": 26
    }
    
    collector = Go2senkyoOptimizedCollector()
    all_candidates = []
    
    try:
        for prefecture, code in prefectures.items():
            try:
                logger.info(f"ğŸ“ {prefecture} åé›†ä¸­...")
                
                # åŸºæœ¬æƒ…å ±ã®ã¿åé›†ï¼ˆè©³ç´°ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                url = f"{collector.base_url}/2025/prefecture/{code}"
                collector.random_delay(1, 2)
                
                response = collector.session.get(url, timeout=30)
                if response.status_code != 200:
                    logger.warning(f"{prefecture}: HTTP {response.status_code}")
                    continue
                
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # åŸºæœ¬å€™è£œè€…æƒ…å ±ã®ã¿æŠ½å‡º
                candidate_blocks = soup.find_all('div', class_='p_senkyoku_list_block')
                logger.info(f"{prefecture}: {len(candidate_blocks)}å€‹ãƒ–ãƒ­ãƒƒã‚¯ç™ºè¦‹")
                
                prefecture_candidates = []
                for i, block in enumerate(candidate_blocks):
                    try:
                        candidate_info = collector.extract_candidate_from_block(block, prefecture, url, i)
                        if candidate_info:
                            # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«åé›†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦åŸºæœ¬æƒ…å ±ã®ã¿
                            prefecture_candidates.append(candidate_info)
                    except Exception as e:
                        logger.debug(f"ãƒ–ãƒ­ãƒƒã‚¯{i}ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                all_candidates.extend(prefecture_candidates)
                logger.info(f"âœ… {prefecture}: {len(prefecture_candidates)}å")
                
                # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
                if prefecture_candidates:
                    sample = prefecture_candidates[0]
                    logger.info(f"  ã‚µãƒ³ãƒ—ãƒ«: {sample.get('name')} ({sample.get('party')})")
                
            except Exception as e:
                logger.error(f"âŒ {prefecture}ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        logger.info(f"ğŸ¯ åé›†å®Œäº†: {len(all_candidates)}å")
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        if all_candidates:
            # æ‰‹å‹•ã§ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            output_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
            main_file = output_dir / f"go2senkyo_quick_{timestamp}.json"
            latest_file = output_dir / "go2senkyo_optimized_latest.json"
            
            # çµ±è¨ˆè¨ˆç®—
            party_stats = {}
            pref_stats = {}
            
            for candidate in all_candidates:
                party = candidate.get('party', 'ç„¡æ‰€å±')
                prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
                
                party_stats[party] = party_stats.get(party, 0) + 1
                pref_stats[prefecture] = pref_stats.get(prefecture, 0) + 1
            
            save_data = {
                "metadata": {
                    "data_type": "go2senkyo_quick_sangiin_2025",
                    "collection_method": "quick_basic_scraping",
                    "total_candidates": len(all_candidates),
                    "generated_at": datetime.now().isoformat(),
                    "source_site": "sangiin.go2senkyo.com",
                    "collection_stats": {
                        "total_candidates": len(all_candidates),
                        "detailed_profiles": 0,
                        "with_photos": 0,
                        "with_policies": 0,
                        "errors": 0
                    },
                    "quality_metrics": {
                        "detail_coverage": "0%",
                        "photo_coverage": "0%", 
                        "policy_coverage": "0%"
                    }
                },
                "statistics": {
                    "by_party": party_stats,
                    "by_prefecture": pref_stats
                },
                "data": all_candidates
            }
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            with open(main_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {main_file}")
            logger.info(f"ğŸ“Š çµ±è¨ˆ:")
            logger.info(f"  ç·å€™è£œè€…æ•°: {len(all_candidates)}å")
            logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(pref_stats)}")
            logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}")
            
            # ä¸Šä½æ”¿å…šè¡¨ç¤º
            top_parties = sorted(party_stats.items(), key=lambda x: x[1], reverse=True)[:5]
            logger.info("  ä¸»è¦æ”¿å…š:")
            for party, count in top_parties:
                logger.info(f"    {party}: {count}å")
        
        return all_candidates
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        raise

if __name__ == "__main__":
    quick_collect()