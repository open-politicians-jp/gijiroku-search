#!/usr/bin/env python3
"""
å‚è­°é™¢è­°å“¡é¸æŒ™2025å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Issue #83å¯¾å¿œ)

2025å¹´å‚è­°é™¢è­°å“¡é¸æŒ™ã®å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’åŒ…æ‹¬çš„ã«åé›†ã—ã€ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ»æ”¿ç­–æƒ…å ±ã‚’æŠ½å‡º
å¯¾å¿œãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹:
- ç·å‹™çœé¸æŒ™ç®¡ç†å§”å“¡ä¼š
- éƒ½é“åºœçœŒé¸æŒ™ç®¡ç†å§”å“¡ä¼š
- å„æ”¿å…šå…¬å¼ã‚µã‚¤ãƒˆ
- Go2senkyoï¼ˆå¯èƒ½ãªå ´åˆï¼‰
- å€™è£œè€…å€‹äººã‚µã‚¤ãƒˆãƒ»SNS

æ©Ÿèƒ½:
- å€™è£œè€…åŸºæœ¬æƒ…å ±åé›†
- æ”¿å…šåˆ¥ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæŠ½å‡º
- æ”¿ç­–ç«‹å ´ãƒ»ä¸»å¼µã®æ§‹é€ åŒ–
- æ—¢å­˜è­°å“¡ãƒ‡ãƒ¼ã‚¿ã¨ã®ç´ä»˜ã‘
- frontend/public/data/sangiin_candidates/ ã«ä¿å­˜
"""

import json
import requests
import time
import re
import random
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin, urlparse
import csv

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Sangiin2025CandidatesCollector:
    """å‚é™¢é¸2025å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹ (Issue #83å¯¾å¿œ)"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.candidates_dir = self.project_root / "frontend" / "public" / "data" / "sangiin_candidates"
        self.raw_data_dir = self.project_root / "data" / "processed" / "sangiin_2025_candidates"
        self.raw_data_dir.mkdir(parents=True, exist_ok=True)
        self.candidates_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹URLè¨­å®š
        self.data_sources = {
            "soumu": "https://www.soumu.go.jp/senkyo/",
            "go2senkyo": "https://sangiin.go2senkyo.com/2025",
            "parties": {
                "è‡ªç”±æ°‘ä¸»å…š": {
                    "url": "https://www.jimin.jp/",
                    "candidates_path": "/senkyo/sangiin2025/",
                    "manifesto_path": "/policy/manifesto/"
                },
                "ç«‹æ†²æ°‘ä¸»å…š": {
                    "url": "https://cdp-japan.jp/",
                    "candidates_path": "/senkyo/2025-sangiin/",
                    "manifesto_path": "/manifesto/"
                },
                "æ—¥æœ¬ç¶­æ–°ã®ä¼š": {
                    "url": "https://o-ishin.jp/",
                    "candidates_path": "/sangiin2025/",
                    "manifesto_path": "/policy/"
                },
                "å…¬æ˜å…š": {
                    "url": "https://www.komei.or.jp/",
                    "candidates_path": "/senkyo/sangiin/2025/",
                    "manifesto_path": "/policy/"
                },
                "æ—¥æœ¬å…±ç”£å…š": {
                    "url": "https://www.jcp.or.jp/",
                    "candidates_path": "/sangiin-senkyo/2025/",
                    "manifesto_path": "/manifesto/"
                },
                "å›½æ°‘æ°‘ä¸»å…š": {
                    "url": "https://new-kokumin.jp/",
                    "candidates_path": "/sangiin2025/",
                    "manifesto_path": "/policy/"
                },
                "ã‚Œã„ã‚æ–°é¸çµ„": {
                    "url": "https://reiwa-shinsengumi.com/",
                    "candidates_path": "/sangiin2025/",
                    "manifesto_path": "/policy/"
                }
            }
        }
        
        # æ”¿å…šåæ­£è¦åŒ–ãƒãƒƒãƒ”ãƒ³ã‚°
        self.party_mapping = {
            'è‡ªç”±æ°‘ä¸»å…š': 'è‡ªç”±æ°‘ä¸»å…š', 'è‡ªæ°‘å…š': 'è‡ªç”±æ°‘ä¸»å…š', 'LDP': 'è‡ªç”±æ°‘ä¸»å…š',
            'ç«‹æ†²æ°‘ä¸»å…š': 'ç«‹æ†²æ°‘ä¸»å…š', 'ç«‹æ°‘': 'ç«‹æ†²æ°‘ä¸»å…š', 'CDP': 'ç«‹æ†²æ°‘ä¸»å…š',
            'æ—¥æœ¬ç¶­æ–°ã®ä¼š': 'æ—¥æœ¬ç¶­æ–°ã®ä¼š', 'ç¶­æ–°': 'æ—¥æœ¬ç¶­æ–°ã®ä¼š', 'ç¶­æ–°ã®ä¼š': 'æ—¥æœ¬ç¶­æ–°ã®ä¼š',
            'å…¬æ˜å…š': 'å…¬æ˜å…š', 'å…¬æ˜': 'å…¬æ˜å…š',
            'æ—¥æœ¬å…±ç”£å…š': 'æ—¥æœ¬å…±ç”£å…š', 'å…±ç”£å…š': 'æ—¥æœ¬å…±ç”£å…š', 'å…±ç”£': 'æ—¥æœ¬å…±ç”£å…š', 'JCP': 'æ—¥æœ¬å…±ç”£å…š',
            'å›½æ°‘æ°‘ä¸»å…š': 'å›½æ°‘æ°‘ä¸»å…š', 'å›½æ°‘': 'å›½æ°‘æ°‘ä¸»å…š', 'DPFP': 'å›½æ°‘æ°‘ä¸»å…š',
            'ã‚Œã„ã‚æ–°é¸çµ„': 'ã‚Œã„ã‚æ–°é¸çµ„', 'ã‚Œã„ã‚': 'ã‚Œã„ã‚æ–°é¸çµ„',
            'ç¤¾ä¼šæ°‘ä¸»å…š': 'ç¤¾ä¼šæ°‘ä¸»å…š', 'ç¤¾æ°‘': 'ç¤¾ä¼šæ°‘ä¸»å…š', 'SDP': 'ç¤¾ä¼šæ°‘ä¸»å…š',
            'NHKå…š': 'NHKå…š', 'Nå›½': 'NHKå…š',
            'ç„¡æ‰€å±': 'ç„¡æ‰€å±'
        }
        
        # éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ï¼ˆå‚é™¢é¸é¸æŒ™åŒºï¼‰
        self.prefecture_codes = {
            "åŒ—æµ·é“": "hokkaido", "é’æ£®": "aomori", "å²©æ‰‹": "iwate", "å®®åŸ": "miyagi",
            "ç§‹ç”°": "akita", "å±±å½¢": "yamagata", "ç¦å³¶": "fukushima", "èŒ¨åŸ": "ibaraki",
            "æ ƒæœ¨": "tochigi", "ç¾¤é¦¬": "gunma", "åŸ¼ç‰": "saitama", "åƒè‘‰": "chiba",
            "æ±äº¬": "tokyo", "ç¥å¥ˆå·": "kanagawa", "æ–°æ½Ÿ": "niigata", "å¯Œå±±": "toyama",
            "çŸ³å·": "ishikawa", "ç¦äº•": "fukui", "å±±æ¢¨": "yamanashi", "é•·é‡": "nagano",
            "å²é˜œ": "gifu", "é™å²¡": "shizuoka", "æ„›çŸ¥": "aichi", "ä¸‰é‡": "mie",
            "æ»‹è³€": "shiga", "äº¬éƒ½": "kyoto", "å¤§é˜ª": "osaka", "å…µåº«": "hyogo",
            "å¥ˆè‰¯": "nara", "å’Œæ­Œå±±": "wakayama", "é³¥å–": "tottori", "å³¶æ ¹": "shimane",
            "å²¡å±±": "okayama", "åºƒå³¶": "hiroshima", "å±±å£": "yamaguchi", "å¾³å³¶": "tokushima",
            "é¦™å·": "kagawa", "æ„›åª›": "ehime", "é«˜çŸ¥": "kochi", "ç¦å²¡": "fukuoka",
            "ä½è³€": "saga", "é•·å´": "nagasaki", "ç†Šæœ¬": "kumamoto", "å¤§åˆ†": "oita",
            "å®®å´": "miyazaki", "é¹¿å…å³¶": "kagoshima", "æ²–ç¸„": "okinawa"
        }
        
        # 2025å¹´å‚é™¢é¸æ”¹é¸è­°å¸­ï¼ˆä»®ï¼‰
        self.sangiin_seats_2025 = {
            "åŒ—æµ·é“": 1, "é’æ£®": 1, "å²©æ‰‹": 1, "å®®åŸ": 1, "ç§‹ç”°": 1, "å±±å½¢": 1,
            "ç¦å³¶": 1, "èŒ¨åŸ": 2, "æ ƒæœ¨": 1, "ç¾¤é¦¬": 1, "åŸ¼ç‰": 3, "åƒè‘‰": 3,
            "æ±äº¬": 6, "ç¥å¥ˆå·": 4, "æ–°æ½Ÿ": 1, "å¯Œå±±": 1, "çŸ³å·": 1, "ç¦äº•": 1,
            "å±±æ¢¨": 1, "é•·é‡": 1, "å²é˜œ": 1, "é™å²¡": 2, "æ„›çŸ¥": 4, "ä¸‰é‡": 1,
            "æ»‹è³€": 1, "äº¬éƒ½": 2, "å¤§é˜ª": 4, "å…µåº«": 3, "å¥ˆè‰¯": 1, "å’Œæ­Œå±±": 1,
            "é³¥å–": 1, "å³¶æ ¹": 1, "å²¡å±±": 1, "åºƒå³¶": 2, "å±±å£": 1, "å¾³å³¶": 1,
            "é¦™å·": 1, "æ„›åª›": 1, "é«˜çŸ¥": 1, "ç¦å²¡": 3, "ä½è³€": 1, "é•·å´": 1,
            "ç†Šæœ¬": 1, "å¤§åˆ†": 1, "å®®å´": 1, "é¹¿å…å³¶": 1, "æ²–ç¸„": 1,
            "æ¯”ä¾‹ä»£è¡¨": 48  # å…¨å›½æ¯”ä¾‹
        }
    
    def update_headers(self):
        """User-Agentæ›´æ–°ã¨IPå½è£…"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
    
    def random_delay(self, min_seconds=2, max_seconds=5):
        """ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œï¼ˆå‚é™¢é¸ç”¨ã«é•·ã‚ã«è¨­å®šï¼‰"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
    
    def normalize_party_name(self, party_name: str) -> str:
        """æ”¿å…šåã‚’æ­£è¦åŒ–"""
        if not party_name:
            return "ç„¡æ‰€å±"
        
        for key, normalized in self.party_mapping.items():
            if key in party_name:
                return normalized
        return party_name
    
    def collect_all_candidates(self) -> List[Dict[str, Any]]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("ğŸš€ å‚é™¢é¸2025å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        
        all_candidates = []
        
        # 1. æ”¿å…šå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰åé›†
        party_candidates = self.collect_from_party_sites()
        all_candidates.extend(party_candidates)
        
        # 2. Go2senkyoã‹ã‚‰åé›†ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
        go2senkyo_candidates = self.collect_from_go2senkyo()
        all_candidates.extend(go2senkyo_candidates)
        
        # 3. ç·å‹™çœãƒ»åœ°æ–¹é¸ç®¡ã‹ã‚‰åé›†
        official_candidates = self.collect_from_official_sources()
        all_candidates.extend(official_candidates)
        
        # 4. é‡è¤‡é™¤å»ã¨çµ±åˆ
        unified_candidates = self.unify_candidate_data(all_candidates)
        
        # 5. æ”¿ç­–ãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæƒ…å ±ã®ä»˜ä¸
        enhanced_candidates = self.enhance_with_policy_data(unified_candidates)
        
        logger.info(f"âœ¨ å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(enhanced_candidates)}å")
        return enhanced_candidates
    
    def collect_from_party_sites(self) -> List[Dict[str, Any]]:
        """å„æ”¿å…šå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("ğŸ“Š æ”¿å…šå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        
        party_candidates = []
        
        for party_name, party_info in self.data_sources["parties"].items():
            try:
                logger.info(f"ğŸ” {party_name}ã®å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
                
                candidates = self.extract_party_candidates(party_name, party_info)
                party_candidates.extend(candidates)
                
                logger.info(f"âœ… {party_name}: {len(candidates)}ååé›†")
                self.random_delay()
                
            except Exception as e:
                logger.error(f"âŒ {party_name}ã®åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        logger.info(f"ğŸ“Š æ”¿å…šã‚µã‚¤ãƒˆåé›†å®Œäº†: {len(party_candidates)}å")
        return party_candidates
    
    def extract_party_candidates(self, party_name: str, party_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å€‹åˆ¥æ”¿å…šã‚µã‚¤ãƒˆã‹ã‚‰å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º"""
        candidates = []
        
        try:
            # å€™è£œè€…ä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLæ§‹ç¯‰
            base_url = party_info["url"]
            candidates_path = party_info.get("candidates_path", "/")
            candidates_url = urljoin(base_url, candidates_path)
            
            self.random_delay()
            response = self.session.get(candidates_url, timeout=30)
            
            # ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
            if response.status_code == 404:
                # 2025å¹´ãƒšãƒ¼ã‚¸ãŒã¾ã ãªã„å ´åˆã€ä¸€èˆ¬çš„ãªãƒ‘ã‚¹ã‚’è©¦è¡Œ
                alternative_paths = ["/senkyo/", "/candidates/", "/member/"]
                for alt_path in alternative_paths:
                    try:
                        alt_url = urljoin(base_url, alt_path)
                        response = self.session.get(alt_url, timeout=15)
                        if response.status_code == 200:
                            candidates_url = alt_url
                            break
                    except:
                        continue
            
            if response.status_code != 200:
                logger.warning(f"{party_name}ã®å€™è£œè€…ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“: {candidates_url}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ”¿å…šã”ã¨ã®æ§‹é€ ã«å¿œã˜ãŸå€™è£œè€…æŠ½å‡º
            if party_name == "è‡ªç”±æ°‘ä¸»å…š":
                candidates = self.extract_ldp_candidates(soup, base_url)
            elif party_name == "ç«‹æ†²æ°‘ä¸»å…š":
                candidates = self.extract_cdp_candidates(soup, base_url)
            elif party_name == "æ—¥æœ¬ç¶­æ–°ã®ä¼š":
                candidates = self.extract_ishin_candidates(soup, base_url)
            elif party_name == "å…¬æ˜å…š":
                candidates = self.extract_komei_candidates(soup, base_url)
            elif party_name == "æ—¥æœ¬å…±ç”£å…š":
                candidates = self.extract_jcp_candidates(soup, base_url)
            elif party_name == "å›½æ°‘æ°‘ä¸»å…š":
                candidates = self.extract_dpfp_candidates(soup, base_url)
            elif party_name == "ã‚Œã„ã‚æ–°é¸çµ„":
                candidates = self.extract_reiwa_candidates(soup, base_url)
            else:
                # æ±ç”¨çš„ãªæŠ½å‡º
                candidates = self.extract_generic_candidates(soup, base_url, party_name)
            
            # å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
            for candidate in candidates:
                candidate.update({
                    "party": party_name,
                    "party_normalized": self.normalize_party_name(party_name),
                    "source": "party_official",
                    "source_url": candidates_url,
                    "collected_at": datetime.now().isoformat()
                })
        
        except Exception as e:
            logger.error(f"{party_name}å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return candidates
    
    def extract_generic_candidates(self, soup: BeautifulSoup, base_url: str, party: str) -> List[Dict[str, Any]]:
        """æ±ç”¨çš„ãªå€™è£œè€…æƒ…å ±æŠ½å‡ºï¼ˆå…¨æ”¿å…šå¯¾å¿œï¼‰"""
        candidates = []
        
        try:
            # å€™è£œè€…åã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
            candidate_patterns = [
                # ãƒªãƒ³ã‚¯ã‹ã‚‰æŠ½å‡º
                soup.find_all('a', href=re.compile(r'candidate|member|profile')),
                # è¦‹å‡ºã—ã‹ã‚‰æŠ½å‡º
                soup.find_all(['h2', 'h3', 'h4'], string=re.compile(r'[ä¸€-é¾¯]{2,4}[ã€€\s]+[ä¸€-é¾¯]{2,4}')),
                # ãƒªã‚¹ãƒˆé …ç›®ã‹ã‚‰æŠ½å‡º
                soup.find_all('li', string=re.compile(r'[ä¸€-é¾¯]{2,4}[ã€€\s]+[ä¸€-é¾¯]{2,4}')),
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æŠ½å‡º
                soup.find_all('td', string=re.compile(r'[ä¸€-é¾¯]{2,4}[ã€€\s]+[ä¸€-é¾¯]{2,4}'))
            ]
            
            for pattern in candidate_patterns:
                for element in pattern:
                    try:
                        name_text = element.get_text(strip=True) if hasattr(element, 'get_text') else str(element)
                        
                        # å€™è£œè€…åã®æŠ½å‡ºã¨æ­£è¦åŒ–
                        name_match = re.search(r'([ä¸€-é¾¯]{2,4}[ã€€\s]+[ä¸€-é¾¯]{2,4})', name_text)
                        if name_match:
                            candidate_name = name_match.group(1).replace('\u3000', ' ').strip()
                            
                            # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
                            if any(c['name'] == candidate_name for c in candidates):
                                continue
                            
                            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLæŠ½å‡º
                            profile_url = ""
                            if hasattr(element, 'get') and element.get('href'):
                                href = element.get('href')
                                if href.startswith('/'):
                                    profile_url = urljoin(base_url, href)
                                elif href.startswith('http'):
                                    profile_url = href
                            
                            candidate = {
                                "candidate_id": f"{party.lower().replace(' ', '_')}_{len(candidates)+1:03d}",
                                "name": candidate_name,
                                "constituency": "æœªåˆ†é¡",
                                "constituency_type": "unknown",
                                "region": None,
                                "profile_url": profile_url,
                                "manifesto_summary": "",
                                "policy_positions": [],
                                "sns_accounts": {},
                                "status": "candidate"
                            }
                            
                            candidates.append(candidate)
                    
                    except Exception as e:
                        logger.debug(f"å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                        continue
        
        except Exception as e:
            logger.error(f"æ±ç”¨å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return candidates[:50]  # æœ€å¤§50åã«åˆ¶é™
    
    def extract_ldp_candidates(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """è‡ªç”±æ°‘ä¸»å…šå€™è£œè€…æŠ½å‡º"""
        # è‡ªæ°‘å…šç‰¹æœ‰ã®æ§‹é€ ã«å¯¾å¿œã—ãŸæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
        return self.extract_generic_candidates(soup, base_url, "è‡ªç”±æ°‘ä¸»å…š")
    
    def extract_cdp_candidates(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """ç«‹æ†²æ°‘ä¸»å…šå€™è£œè€…æŠ½å‡º"""
        return self.extract_generic_candidates(soup, base_url, "ç«‹æ†²æ°‘ä¸»å…š")
    
    def extract_ishin_candidates(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """æ—¥æœ¬ç¶­æ–°ã®ä¼šå€™è£œè€…æŠ½å‡º"""
        return self.extract_generic_candidates(soup, base_url, "æ—¥æœ¬ç¶­æ–°ã®ä¼š")
    
    def extract_komei_candidates(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """å…¬æ˜å…šå€™è£œè€…æŠ½å‡º"""
        return self.extract_generic_candidates(soup, base_url, "å…¬æ˜å…š")
    
    def extract_jcp_candidates(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """æ—¥æœ¬å…±ç”£å…šå€™è£œè€…æŠ½å‡º"""
        return self.extract_generic_candidates(soup, base_url, "æ—¥æœ¬å…±ç”£å…š")
    
    def extract_dpfp_candidates(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """å›½æ°‘æ°‘ä¸»å…šå€™è£œè€…æŠ½å‡º"""
        return self.extract_generic_candidates(soup, base_url, "å›½æ°‘æ°‘ä¸»å…š")
    
    def extract_reiwa_candidates(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """ã‚Œã„ã‚æ–°é¸çµ„å€™è£œè€…æŠ½å‡º"""
        return self.extract_generic_candidates(soup, base_url, "ã‚Œã„ã‚æ–°é¸çµ„")
    
    def collect_from_go2senkyo(self) -> List[Dict[str, Any]]:
        """Go2senkyoã‹ã‚‰å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("ğŸ” Go2senkyoå‚é™¢é¸ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
        
        candidates = []
        
        try:
            self.random_delay()
            response = self.session.get(self.data_sources["go2senkyo"], timeout=30)
            
            if response.status_code != 200:
                logger.warning("Go2senkyoã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Go2senkyoç‰¹æœ‰ã®æ§‹é€ ã‹ã‚‰å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º
            # ï¼ˆç¾æ™‚ç‚¹ã§ã¯è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚ã€å°†æ¥çš„ã«æ‹¡å¼µï¼‰
            candidate_elements = soup.find_all(['div', 'article'], class_=re.compile(r'candidate|member'))
            
            for element in candidate_elements:
                try:
                    candidate_data = self.parse_go2senkyo_candidate(element)
                    if candidate_data:
                        candidates.append(candidate_data)
                except Exception as e:
                    logger.debug(f"Go2senkyoå€™è£œè€…è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"Go2senkyoåé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        logger.info(f"ğŸ“Š Go2senkyoåé›†å®Œäº†: {len(candidates)}å")
        return candidates
    
    def parse_go2senkyo_candidate(self, element) -> Optional[Dict[str, Any]]:
        """Go2senkyoå€™è£œè€…è¦ç´ ã®è§£æ"""
        try:
            name_elem = element.find(['h2', 'h3', 'a'], string=re.compile(r'[ä¸€-é¾¯]{2,4}'))
            if not name_elem:
                return None
            
            name = name_elem.get_text(strip=True)
            
            # æ”¿å…šæƒ…å ±
            party_elem = element.find(class_=re.compile(r'party|political'))
            party = party_elem.get_text(strip=True) if party_elem else "æœªåˆ†é¡"
            
            # é¸æŒ™åŒºæƒ…å ±
            constituency_elem = element.find(class_=re.compile(r'constituency|district'))
            constituency = constituency_elem.get_text(strip=True) if constituency_elem else "æœªåˆ†é¡"
            
            return {
                "candidate_id": f"go2senkyo_{hash(name) % 1000:03d}",
                "name": name,
                "party": self.normalize_party_name(party),
                "constituency": constituency,
                "constituency_type": "proportional" if "æ¯”ä¾‹" in constituency else "single_member",
                "source": "go2senkyo",
                "collected_at": datetime.now().isoformat()
            }
        
        except Exception as e:
            logger.debug(f"Go2senkyoè¦ç´ è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def collect_from_official_sources(self) -> List[Dict[str, Any]]:
        """ç·å‹™çœãƒ»éƒ½é“åºœçœŒé¸ç®¡ã‹ã‚‰å…¬å¼ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("ğŸ›ï¸ ç·å‹™çœãƒ»åœ°æ–¹é¸ç®¡ã‹ã‚‰å…¬å¼ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
        
        official_candidates = []
        
        try:
            # ç·å‹™çœé¸æŒ™ç®¡ç†å§”å“¡ä¼š
            soumu_candidates = self.collect_from_soumu()
            official_candidates.extend(soumu_candidates)
            
            # éƒ½é“åºœçœŒé¸ç®¡ï¼ˆä¸»è¦çœŒã®ã¿ï¼‰
            major_prefectures = ["æ±äº¬", "å¤§é˜ª", "æ„›çŸ¥", "ç¥å¥ˆå·", "åŸ¼ç‰", "åƒè‘‰"]
            for prefecture in major_prefectures:
                try:
                    pref_candidates = self.collect_from_prefecture(prefecture)
                    official_candidates.extend(pref_candidates)
                    self.random_delay()
                except Exception as e:
                    logger.debug(f"{prefecture}çœŒãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"å…¬å¼ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        logger.info(f"ğŸ›ï¸ å…¬å¼ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(official_candidates)}å")
        return official_candidates
    
    def collect_from_soumu(self) -> List[Dict[str, Any]]:
        """ç·å‹™çœé¸æŒ™ç®¡ç†å§”å“¡ä¼šã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†"""
        candidates = []
        
        try:
            soumu_url = self.data_sources["soumu"]
            self.random_delay()
            response = self.session.get(soumu_url, timeout=30)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # é¸æŒ™é–¢é€£ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                election_links = soup.find_all('a', href=re.compile(r'sangiin|election|senkyo'))
                
                for link in election_links[:5]:  # æœ€å¤§5ã¤ã®ãƒªãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
                    try:
                        href = link.get('href')
                        if href:
                            full_url = urljoin(soumu_url, href)
                            self.random_delay()
                            sub_response = self.session.get(full_url, timeout=15)
                            
                            if sub_response.status_code == 200:
                                sub_soup = BeautifulSoup(sub_response.text, 'html.parser')
                                sub_candidates = self.extract_generic_candidates(sub_soup, full_url, "ç·å‹™çœãƒ‡ãƒ¼ã‚¿")
                                candidates.extend(sub_candidates)
                    except Exception as e:
                        logger.debug(f"ç·å‹™çœã‚µãƒ–ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
        
        except Exception as e:
            logger.error(f"ç·å‹™çœãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return candidates
    
    def collect_from_prefecture(self, prefecture: str) -> List[Dict[str, Any]]:
        """éƒ½é“åºœçœŒé¸ç®¡ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†"""
        candidates = []
        
        try:
            # éƒ½é“åºœçœŒé¸ç®¡URLã®æ¨å®š
            pref_code = self.prefecture_codes.get(prefecture, prefecture.lower())
            pref_urls = [
                f"https://www.pref.{pref_code}.lg.jp/senkyo/",
                f"https://{pref_code}.jp/senkyo/",
                f"https://www.{pref_code}-senkan.jp/"
            ]
            
            for url in pref_urls:
                try:
                    self.random_delay()
                    response = self.session.get(url, timeout=15)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        pref_candidates = self.extract_generic_candidates(soup, url, f"{prefecture}é¸ç®¡")
                        candidates.extend(pref_candidates)
                        break
                
                except Exception as e:
                    logger.debug(f"{prefecture}é¸ç®¡URLè©¦è¡Œã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                    continue
        
        except Exception as e:
            logger.error(f"{prefecture}ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return candidates
    
    def unify_candidate_data(self, all_candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡é™¤å»ã¨çµ±åˆ"""
        logger.info("ğŸ”„ å€™è£œè€…ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
        
        unified = {}
        
        for candidate in all_candidates:
            name = candidate.get('name', '').strip()
            party = candidate.get('party', '')
            
            # åå‰ã¨æ”¿å…šã®çµ„ã¿åˆã‚ã›ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
            key = f"{name}_{party}"
            
            if key not in unified:
                unified[key] = candidate
            else:
                # ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã§ä¸Šæ›¸ã
                existing = unified[key]
                if len(candidate.get('profile_url', '')) > len(existing.get('profile_url', '')):
                    unified[key] = candidate
                elif candidate.get('source') == 'party_official' and existing.get('source') != 'party_official':
                    unified[key] = candidate
        
        unified_list = list(unified.values())
        logger.info(f"ğŸ”„ çµ±åˆå®Œäº†: {len(all_candidates)}å â†’ {len(unified_list)}å")
        
        return unified_list
    
    def enhance_with_policy_data(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ”¿ç­–ãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæƒ…å ±ã§å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’å¼·åŒ–"""
        logger.info("ğŸ“‹ æ”¿ç­–ãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæƒ…å ±ä»˜ä¸ä¸­...")
        
        enhanced_candidates = []
        
        for candidate in candidates:
            try:
                enhanced = candidate.copy()
                
                # æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæƒ…å ±ã®ä»˜ä¸
                party_manifesto = self.get_party_manifesto(candidate.get('party', ''))
                if party_manifesto:
                    enhanced['party_manifesto'] = party_manifesto
                
                # å€‹äººãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ»æ”¿ç­–ã®åé›†
                profile_url = candidate.get('profile_url', '')
                if profile_url:
                    profile_data = self.extract_candidate_profile(profile_url)
                    enhanced.update(profile_data)
                
                # SNSã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¤œç´¢
                sns_accounts = self.find_candidate_sns(candidate.get('name', ''))
                if sns_accounts:
                    enhanced['sns_accounts'] = sns_accounts
                
                enhanced_candidates.append(enhanced)
                
            except Exception as e:
                logger.debug(f"å€™è£œè€…{candidate.get('name', 'unknown')}ã®å¼·åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                enhanced_candidates.append(candidate)
                continue
        
        logger.info("ğŸ“‹ æ”¿ç­–æƒ…å ±ä»˜ä¸å®Œäº†")
        return enhanced_candidates
    
    def get_party_manifesto(self, party: str) -> Dict[str, Any]:
        """æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—"""
        if not party or party == "ç„¡æ‰€å±":
            return {}
        
        try:
            party_info = self.data_sources["parties"].get(party, {})
            if not party_info:
                return {}
            
            manifesto_url = urljoin(party_info["url"], party_info.get("manifesto_path", "/"))
            
            self.random_delay()
            response = self.session.get(manifesto_url, timeout=20)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæœ¬æ–‡æŠ½å‡º
                content_selectors = ['main', 'article', '.content', '#content', '.manifesto']
                manifesto_text = ""
                
                for selector in content_selectors:
                    elements = soup.select(selector)
                    if elements:
                        manifesto_text = elements[0].get_text(strip=True)[:2000]  # 2000æ–‡å­—ã¾ã§
                        break
                
                return {
                    "party": party,
                    "url": manifesto_url,
                    "summary": manifesto_text,
                    "collected_at": datetime.now().isoformat()
                }
        
        except Exception as e:
            logger.debug(f"{party}ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        return {}
    
    def extract_candidate_profile(self, profile_url: str) -> Dict[str, Any]:
        """å€™è£œè€…å€‹äººãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ»æ”¿ç­–ã‚’æŠ½å‡º"""
        profile_data = {}
        
        try:
            self.random_delay()
            response = self.session.get(profile_url, timeout=15)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ”¿ç­–ãƒ»ä¸»å¼µã®æŠ½å‡º
                policy_keywords = ['æ”¿ç­–', 'ä¸»å¼µ', 'å…¬ç´„', 'ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ', 'æ”¿æ²»ä¿¡æ¡', 'å–ã‚Šçµ„ã¿']
                policy_text = ""
                
                for keyword in policy_keywords:
                    elements = soup.find_all(string=re.compile(keyword))
                    for element in elements:
                        parent = element.parent
                        if parent:
                            policy_text += parent.get_text(strip=True) + " "
                    
                    if len(policy_text) > 500:
                        break
                
                if policy_text:
                    profile_data['manifesto_summary'] = policy_text[:1000]
                
                # çµŒæ­´æƒ…å ±
                career_keywords = ['çµŒæ­´', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'ç•¥æ­´', 'ç”Ÿã„ç«‹ã¡']
                for keyword in career_keywords:
                    career_elem = soup.find(string=re.compile(keyword))
                    if career_elem and career_elem.parent:
                        career_text = career_elem.parent.get_text(strip=True)[:500]
                        profile_data['career'] = career_text
                        break
                
                # å†™çœŸURL
                img_elements = soup.find_all('img', alt=re.compile(r'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«|å†™çœŸ|å€™è£œè€…'))
                for img in img_elements:
                    src = img.get('src', '')
                    if src:
                        if src.startswith('/'):
                            photo_url = urljoin(profile_url, src)
                        elif src.startswith('http'):
                            photo_url = src
                        else:
                            continue
                        profile_data['photo_url'] = photo_url
                        break
        
        except Exception as e:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({profile_url}): {e}")
        
        return profile_data
    
    def find_candidate_sns(self, candidate_name: str) -> Dict[str, str]:
        """å€™è£œè€…ã®SNSã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¤œç´¢ï¼ˆåˆ¶é™çš„ï¼‰"""
        # å®Ÿè£…ã¯ç°¡æ˜“ç‰ˆï¼ˆAPIã‚­ãƒ¼ãŒå¿…è¦ãªæœ¬æ ¼æ¤œç´¢ã¯å¾Œæ—¥å®Ÿè£…ï¼‰
        sns_accounts = {}
        
        try:
            # å€™è£œè€…åã‹ã‚‰SNSã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¨å®šï¼ˆéå¸¸ã«åˆ¶é™çš„ï¼‰
            name_clean = re.sub(r'[ã€€\s]+', '', candidate_name)
            
            # Twitterã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ¨å®šãƒ‘ã‚¿ãƒ¼ãƒ³
            twitter_patterns = [
                f"https://twitter.com/{name_clean}",
                f"https://twitter.com/{name_clean}_official",
                f"https://x.com/{name_clean}"
            ]
            
            # å®Ÿéš›ã®SNSæ¤œç´¢ã¯åˆ¥é€”APIå®Ÿè£…ãŒå¿…è¦
            # ã“ã“ã§ã¯æ§‹é€ ã®ã¿å®šç¾©
            
        except Exception as e:
            logger.debug(f"SNSæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return sns_accounts
    
    def save_candidates_data(self, candidates: List[Dict[str, Any]]):
        """å‚é™¢é¸å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if not candidates:
            logger.warning("ä¿å­˜ã™ã‚‹å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        current_date = datetime.now()
        timestamp = current_date.strftime('%Y%m%d_%H%M%S')
        
        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        main_filename = f"sangiin_2025_candidates_{timestamp}.json"
        main_filepath = self.candidates_dir / main_filename
        
        # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        latest_filepath = self.candidates_dir / "sangiin_2025_candidates_latest.json"
        
        # æ”¿å…šåˆ¥çµ±è¨ˆ
        party_stats = {}
        constituency_stats = {}
        
        for candidate in candidates:
            party = candidate.get('party', 'ç„¡æ‰€å±')
            constituency = candidate.get('constituency', 'æœªåˆ†é¡')
            
            party_stats[party] = party_stats.get(party, 0) + 1
            constituency_stats[constituency] = constituency_stats.get(constituency, 0) + 1
        
        # ä¿å­˜ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        save_data = {
            "metadata": {
                "data_type": "sangiin_2025_candidates",
                "election_year": 2025,
                "total_candidates": len(candidates),
                "parties_count": len(party_stats),
                "generated_at": current_date.isoformat(),
                "data_sources": list(self.data_sources.keys()),
                "collection_version": "1.0.0",
                "next_update": (current_date + timedelta(days=7)).isoformat()
            },
            "statistics": {
                "by_party": party_stats,
                "by_constituency": constituency_stats
            },
            "data": candidates
        }
        
        # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(main_filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(latest_filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        # CSVå½¢å¼ã§ã‚‚ä¿å­˜
        csv_filepath = self.candidates_dir / f"sangiin_2025_candidates_{timestamp}.csv"
        self.save_candidates_csv(candidates, csv_filepath)
        
        logger.info(f"ğŸ“ å‚é™¢é¸å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  - ãƒ¡ã‚¤ãƒ³: {main_filepath}")
        logger.info(f"  - æœ€æ–°: {latest_filepath}")
        logger.info(f"  - CSV: {csv_filepath}")
        logger.info(f"  - å€™è£œè€…æ•°: {len(candidates)}å")
        
        # çµ±è¨ˆè¡¨ç¤º
        self.display_collection_stats(candidates, party_stats, constituency_stats)
    
    def save_candidates_csv(self, candidates: List[Dict[str, Any]], filepath: Path):
        """å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã§ä¿å­˜"""
        try:
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                if not candidates:
                    return
                
                # CSVãƒ˜ãƒƒãƒ€ãƒ¼
                fieldnames = [
                    'candidate_id', 'name', 'party', 'constituency', 'constituency_type',
                    'region', 'profile_url', 'manifesto_summary', 'career', 'photo_url',
                    'source', 'collected_at'
                ]
                
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for candidate in candidates:
                    # CSVç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
                    csv_row = {}
                    for field in fieldnames:
                        value = candidate.get(field, '')
                        # è¤‡é›‘ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯æ–‡å­—åˆ—åŒ–
                        if isinstance(value, (dict, list)):
                            value = str(value)
                        csv_row[field] = value
                    
                    writer.writerow(csv_row)
                    
        except Exception as e:
            logger.error(f"CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def display_collection_stats(self, candidates: List[Dict[str, Any]], 
                                party_stats: Dict[str, int], 
                                constituency_stats: Dict[str, int]):
        """åé›†çµ±è¨ˆã®è¡¨ç¤º"""
        logger.info("\nğŸ“Š å‚é™¢é¸2025å€™è£œè€…åé›†çµ±è¨ˆ:")
        logger.info(f"ç·å€™è£œè€…æ•°: {len(candidates)}å")
        
        # æ”¿å…šåˆ¥çµ±è¨ˆ
        logger.info("\nğŸ›ï¸ æ”¿å…šåˆ¥å€™è£œè€…æ•°:")
        for party, count in sorted(party_stats.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  {party}: {count}å")
        
        # é¸æŒ™åŒºåˆ¥çµ±è¨ˆï¼ˆä¸Šä½10é¸æŒ™åŒºï¼‰
        logger.info("\nğŸ—¾ é¸æŒ™åŒºåˆ¥å€™è£œè€…æ•°ï¼ˆä¸Šä½10ï¼‰:")
        top_constituencies = sorted(constituency_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        for constituency, count in top_constituencies:
            logger.info(f"  {constituency}: {count}å")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        source_stats = {}
        for candidate in candidates:
            source = candidate.get('source', 'unknown')
            source_stats[source] = source_stats.get(source, 0) + 1
        
        logger.info("\nğŸ“¡ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥:")
        for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  {source}: {count}å")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ å‚é™¢é¸2025å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ (Issue #83)")
    
    collector = Sangiin2025CandidatesCollector()
    
    try:
        # å…¨å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†
        candidates = collector.collect_all_candidates()
        
        if not candidates:
            logger.error("å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        collector.save_candidates_data(candidates)
        
        logger.info("âœ¨ å‚é™¢é¸2025å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†å‡¦ç†å®Œäº† (Issue #83)")
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()