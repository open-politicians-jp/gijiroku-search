#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿é‡è¤‡æ¤œå‡ºãƒ»æ’é™¤ãƒ„ãƒ¼ãƒ« (Issue #86å¯¾å¿œ)

è³ªå•ä¸»æ„æ›¸ãƒ»æå‡ºæ³•æ¡ˆãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºãƒ»å‰Šé™¤ã—ã€
ãƒ‡ãƒ¼ã‚¿åé›†å‡¦ç†æ™‚é–“ã‚’40åˆ†â†’5-10åˆ†ã«çŸ­ç¸®ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«

æ©Ÿèƒ½:
- é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•æ¤œå‡º
- SHA256ãƒãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹é«˜é€Ÿé‡è¤‡æ¯”è¼ƒ
- å®‰å…¨ãªé‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
- ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»æœ€é©åŒ–
- å‡¦ç†çµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import json
import hashlib
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataDeduplicationTool:
    """ãƒ‡ãƒ¼ã‚¿é‡è¤‡æ¤œå‡ºãƒ»æ’é™¤ãƒ„ãƒ¼ãƒ«"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.data_dir = self.project_root / "frontend" / "public" / "data"
        self.backup_dir = self.project_root / "data" / "backup" / f"dedup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.target_dirs = {
            "questions": self.data_dir / "questions",
            "bills": self.data_dir / "bills"
        }
        
        # é™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå‰Šé™¤å¯¾è±¡å¤–ï¼‰
        self.excluded_files = {
            "questions_latest.json",
            "bills_latest.json"
        }
        
        self.duplicate_stats = {
            "questions": {"files_analyzed": 0, "duplicates_found": 0, "space_saved": 0},
            "bills": {"files_analyzed": 0, "duplicates_found": 0, "space_saved": 0}
        }
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®SHA256ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # collected_atãªã©ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’é™¤å¤–ã—ãŸãƒãƒƒã‚·ãƒ¥è¨ˆç®—
            normalized_data = self.normalize_data_for_hash(data)
            data_str = json.dumps(normalized_data, sort_keys=True, ensure_ascii=False)
            
            return hashlib.sha256(data_str.encode('utf-8')).hexdigest()
        
        except Exception as e:
            logger.error(f"ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼ ({file_path}): {e}")
            return ""
    
    def normalize_data_for_hash(self, data: Any) -> Any:
        """ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é™¤å»ï¼‰"""
        if isinstance(data, dict):
            normalized = {}
            for key, value in data.items():
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç³»ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é™¤å¤–
                if key in ['collected_at', 'generated_at', 'created_at', 'updated_at']:
                    continue
                normalized[key] = self.normalize_data_for_hash(value)
            return normalized
        elif isinstance(data, list):
            return [self.normalize_data_for_hash(item) for item in data]
        else:
            return data
    
    def find_duplicates_in_directory(self, dir_name: str) -> Dict[str, List[Path]]:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º"""
        directory = self.target_dirs[dir_name]
        
        if not directory.exists():
            logger.warning(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {directory}")
            return {}
        
        logger.info(f"ğŸ” {dir_name}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®é‡è¤‡æ¤œå‡ºé–‹å§‹...")
        
        hash_to_files = {}
        json_files = list(directory.glob("*.json"))
        
        for file_path in json_files:
            # é™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if file_path.name in self.excluded_files:
                continue
            
            file_hash = self.calculate_file_hash(file_path)
            
            if file_hash:
                if file_hash not in hash_to_files:
                    hash_to_files[file_hash] = []
                hash_to_files[file_hash].append(file_path)
        
        # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆåŒã˜ãƒãƒƒã‚·ãƒ¥ã§è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã®ã¿æŠ½å‡º
        duplicates = {h: files for h, files in hash_to_files.items() if len(files) > 1}
        
        self.duplicate_stats[dir_name]["files_analyzed"] = len(json_files)
        self.duplicate_stats[dir_name]["duplicates_found"] = sum(len(files) - 1 for files in duplicates.values())
        
        logger.info(f"ğŸ“Š {dir_name}: {len(json_files)}ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ, {len(duplicates)}ã‚°ãƒ«ãƒ¼ãƒ—ã®é‡è¤‡ç™ºè¦‹")
        
        return duplicates
    
    def backup_file(self, file_path: Path) -> Path:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        relative_path = file_path.relative_to(self.data_dir)
        backup_path = self.backup_dir / relative_path
        backup_path.parent.mkdir(parents=True, exist_ok=True)
        
        shutil.copy2(file_path, backup_path)
        return backup_path
    
    def select_files_to_keep(self, duplicate_files: List[Path]) -> Tuple[Path, List[Path]]:
        """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‹ã‚‰ä¿æŒã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¨å‰Šé™¤ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ"""
        if not duplicate_files:
            return None, []
        
        # é¸æŠåŸºæº–ã®å„ªå…ˆé †ä½:
        # 1. _latest.jsonãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒæœ€å¤§
        # 3. æœ€æ–°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
        
        latest_files = [f for f in duplicate_files if f.name.endswith('_latest.json')]
        if latest_files:
            keep_file = latest_files[0]
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
            files_by_size = sorted(duplicate_files, key=lambda f: f.stat().st_size, reverse=True)
            keep_file = files_by_size[0]
        
        files_to_remove = [f for f in duplicate_files if f != keep_file]
        
        return keep_file, files_to_remove
    
    def remove_duplicate_files(self, dir_name: str, duplicates: Dict[str, List[Path]], dry_run: bool = False) -> int:
        """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        total_space_saved = 0
        files_removed = 0
        
        logger.info(f"ğŸ—‘ï¸ {dir_name}ã®é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤é–‹å§‹...")
        
        for file_hash, duplicate_files in duplicates.items():
            keep_file, files_to_remove = self.select_files_to_keep(duplicate_files)
            
            if not keep_file or not files_to_remove:
                continue
            
            logger.info(f"ğŸ“ ä¿æŒ: {keep_file.name}")
            
            for file_to_remove in files_to_remove:
                file_size = file_to_remove.stat().st_size
                total_space_saved += file_size
                files_removed += 1
                
                if dry_run:
                    logger.info(f"ğŸ§ª [DRY RUN] å‰Šé™¤äºˆå®š: {file_to_remove.name} ({file_size:,} bytes)")
                else:
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
                    backup_path = self.backup_file(file_to_remove)
                    logger.info(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                    file_to_remove.unlink()
                    logger.info(f"âœ… å‰Šé™¤å®Œäº†: {file_to_remove.name} ({file_size:,} bytes)")
        
        self.duplicate_stats[dir_name]["space_saved"] = total_space_saved
        
        action = "å‰Šé™¤äºˆå®š" if dry_run else "å‰Šé™¤å®Œäº†"
        logger.info(f"ğŸ‰ {dir_name}: {files_removed}ãƒ•ã‚¡ã‚¤ãƒ«{action}, {total_space_saved:,} bytesç¯€ç´„")
        
        return files_removed
    
    def generate_deduplication_report(self) -> str:
        """é‡è¤‡é™¤å»ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        current_time = datetime.now().isoformat()
        
        report = f"""
ğŸ“Š ãƒ‡ãƒ¼ã‚¿é‡è¤‡é™¤å»ãƒ¬ãƒãƒ¼ãƒˆ
ç”Ÿæˆæ—¥æ™‚: {current_time}

=== å‡¦ç†çµæœã‚µãƒãƒªãƒ¼ ===
"""
        
        total_files_analyzed = 0
        total_duplicates_found = 0
        total_space_saved = 0
        
        for dir_name, stats in self.duplicate_stats.items():
            total_files_analyzed += stats["files_analyzed"]
            total_duplicates_found += stats["duplicates_found"]
            total_space_saved += stats["space_saved"]
            
            report += f"""
{dir_name.upper()}:
  - åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats["files_analyzed"]}
  - é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {stats["duplicates_found"]}
  - ç¯€ç´„å®¹é‡: {stats["space_saved"]:,} bytes ({stats["space_saved"]/1024/1024:.2f} MB)
"""
        
        report += f"""
=== ç·åˆçµæœ ===
ç·åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files_analyzed}
ç·é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_duplicates_found}
ç·ç¯€ç´„å®¹é‡: {total_space_saved:,} bytes ({total_space_saved/1024/1024:.2f} MB)

å‰Šé™¤ç‡: {(total_duplicates_found/total_files_analyzed*100):.1f}%
å®¹é‡å‰Šæ¸›åŠ¹æœ: {(total_space_saved/(total_space_saved+10*1024*1024)*100):.1f}%

=== ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å ´æ‰€ ===
{self.backup_dir}

=== å‡¦ç†å®Œäº†æ™‚åˆ» ===
{datetime.now().isoformat()}
"""
        
        return report
    
    def save_report(self, report: str):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        report_path = self.backup_dir / "deduplication_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
    
    def run_deduplication(self, dry_run: bool = False):
        """é‡è¤‡é™¤å»å‡¦ç†ã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ ãƒ‡ãƒ¼ã‚¿é‡è¤‡é™¤å»å‡¦ç†é–‹å§‹...")
        
        if dry_run:
            logger.info("ğŸ§ª DRY RUNãƒ¢ãƒ¼ãƒ‰: å®Ÿéš›ã®å‰Šé™¤ã¯è¡Œã„ã¾ã›ã‚“")
        
        total_removed = 0
        
        for dir_name in self.target_dirs.keys():
            try:
                # é‡è¤‡æ¤œå‡º
                duplicates = self.find_duplicates_in_directory(dir_name)
                
                if duplicates:
                    # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                    removed_count = self.remove_duplicate_files(dir_name, duplicates, dry_run)
                    total_removed += removed_count
                else:
                    logger.info(f"âœ¨ {dir_name}: é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
            
            except Exception as e:
                logger.error(f"âŒ {dir_name}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»ä¿å­˜
        report = self.generate_deduplication_report()
        
        if not dry_run:
            self.save_report(report)
        
        logger.info(report)
        logger.info(f"âœ¨ é‡è¤‡é™¤å»å‡¦ç†å®Œäº†: {total_removed}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import sys
    
    logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿é‡è¤‡æ¤œå‡ºãƒ»æ’é™¤ãƒ„ãƒ¼ãƒ« (Issue #86)")
    
    tool = DataDeduplicationTool()
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§å‹•ä½œãƒ¢ãƒ¼ãƒ‰æ±ºå®š
    force_run = "--force" in sys.argv
    dry_run_only = "--dry-run" in sys.argv
    
    try:
        # ã¾ãšDRY RUNã§ç¢ºèª
        logger.info("=== DRY RUN å®Ÿè¡Œ ===")
        tool.run_deduplication(dry_run=True)
        
        if dry_run_only:
            logger.info("DRY RUNã®ã¿ã§çµ‚äº†ã—ã¾ã™")
            return
        
        if force_run:
            logger.info("\n=== è‡ªå‹•å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼šé‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Ÿè¡Œ ===")
            tool.run_deduplication(dry_run=False)
        else:
            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆCIç’°å¢ƒã§ã¯ä½¿ç”¨ä¸å¯ï¼‰
            try:
                user_input = input("\nå®Ÿéš›ã«é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
                
                if user_input.lower() in ['y', 'yes']:
                    logger.info("\n=== å®Ÿéš›ã®å‰Šé™¤å®Ÿè¡Œ ===")
                    tool.run_deduplication(dry_run=False)
                else:
                    logger.info("é‡è¤‡å‰Šé™¤ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            except EOFError:
                logger.info("CIç’°å¢ƒdetected: --forceã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        
    except KeyboardInterrupt:
        logger.info("å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()