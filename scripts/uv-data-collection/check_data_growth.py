#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿å¢—åŠ çŠ¶æ³ç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å„ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æœ€çµ‚æ›´æ–°æ—¥ã¨ä»¶æ•°ã‚’ç¢ºèªã—ã€ãƒ‡ãƒ¼ã‚¿ãŒå¢—åŠ ã—ã¦ã„ãªã„ç†ç”±ã‚’åˆ†æ
"""

import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataGrowthChecker:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.data_dir = self.project_root / "frontend" / "public" / "data"
        
    def check_all_data_types(self):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æˆé•·çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿æˆé•·çŠ¶æ³ãƒã‚§ãƒƒã‚¯é–‹å§‹...")
        
        data_types = {
            "speeches": "è­°äº‹éŒ²",
            "questions": "è³ªå•ä¸»æ„æ›¸", 
            "bills": "æå‡ºæ³•æ¡ˆ",
            "committee_news": "å§”å“¡ä¼šãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "manifestos": "ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ"
        }
        
        results = {}
        
        for data_type, display_name in data_types.items():
            logger.info(f"\nğŸ” {display_name}ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯...")
            result = self.check_data_type(data_type)
            results[data_type] = result
            self.display_result(display_name, result)
        
        # ç·åˆã‚µãƒãƒªãƒ¼
        self.display_summary(results)
        
        return results
    
    def check_data_type(self, data_type: str) -> Dict[str, Any]:
        """ç‰¹å®šãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æˆé•·çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯"""
        data_type_dir = self.data_dir / data_type
        
        if not data_type_dir.exists():
            return {
                "status": "missing",
                "message": "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“",
                "files": [],
                "latest_file": None,
                "total_records": 0,
                "last_update": None
            }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        json_files = list(data_type_dir.glob("*.json"))
        json_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        if not json_files:
            return {
                "status": "empty",
                "message": "JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“",
                "files": [],
                "latest_file": None,
                "total_records": 0,
                "last_update": None
            }
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        latest_file = json_files[0]
        latest_analysis = self.analyze_file(latest_file)
        
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        file_analysis = []
        for file_path in json_files[:10]:  # æœ€æ–°10ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
            analysis = self.analyze_file(file_path)
            file_analysis.append(analysis)
        
        # æ›´æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        update_pattern = self.analyze_update_pattern(file_analysis)
        
        return {
            "status": "active" if latest_analysis["records"] > 0 else "inactive",
            "files": file_analysis,
            "latest_file": latest_analysis,
            "total_files": len(json_files),
            "update_pattern": update_pattern,
            "last_update": latest_analysis["last_modified"],
            "total_records": latest_analysis["records"]
        }
    
    def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
            file_size = file_path.stat().st_size
            last_modified = datetime.fromtimestamp(file_path.stat().st_mtime)
            
            # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®å–å¾—
            records = 0
            data_structure = "unknown"
            
            if isinstance(data, list):
                records = len(data)
                data_structure = "array"
            elif isinstance(data, dict):
                if "data" in data and isinstance(data["data"], list):
                    records = len(data["data"])
                    data_structure = "object_with_data"
                elif "statistics" in data:
                    data_structure = "statistics"
                else:
                    data_structure = "object"
            
            return {
                "file_name": file_path.name,
                "file_path": str(file_path),
                "file_size": file_size,
                "file_size_mb": round(file_size / 1024 / 1024, 2),
                "last_modified": last_modified,
                "last_modified_str": last_modified.strftime("%Y-%m-%d %H:%M:%S"),
                "records": records,
                "data_structure": data_structure,
                "days_old": (datetime.now() - last_modified).days
            }
            
        except Exception as e:
            return {
                "file_name": file_path.name,
                "file_path": str(file_path),
                "error": str(e),
                "records": 0,
                "last_modified": None
            }
    
    def analyze_update_pattern(self, file_analysis: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ›´æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        if not file_analysis:
            return {"pattern": "no_data"}
        
        # æœ€æ–°3ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¯”è¼ƒ
        recent_files = file_analysis[:3]
        record_counts = [f["records"] for f in recent_files if "records" in f]
        
        if len(record_counts) < 2:
            return {"pattern": "insufficient_data"}
        
        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®å¤‰åŒ–
        if all(count == record_counts[0] for count in record_counts):
            pattern = "static"  # å¤‰åŒ–ãªã—
        elif record_counts[0] > record_counts[1]:
            pattern = "growing"  # å¢—åŠ 
        else:
            pattern = "decreasing"  # æ¸›å°‘
        
        # æ›´æ–°é »åº¦
        dates = [f["last_modified"] for f in recent_files if f["last_modified"]]
        if len(dates) >= 2:
            time_diff = (dates[0] - dates[1]).days
            if time_diff <= 1:
                frequency = "daily"
            elif time_diff <= 7:
                frequency = "weekly"
            else:
                frequency = "irregular"
        else:
            frequency = "unknown"
        
        return {
            "pattern": pattern,
            "frequency": frequency,
            "record_counts": record_counts,
            "latest_count": record_counts[0] if record_counts else 0
        }
    
    def display_result(self, data_name: str, result: Dict[str, Any]):
        """çµæœè¡¨ç¤º"""
        status = result["status"]
        
        if status == "missing":
            logger.warning(f"âŒ {data_name}: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãªã—")
        elif status == "empty":
            logger.warning(f"ğŸ“‚ {data_name}: ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
        elif status == "inactive":
            logger.warning(f"âš ï¸ {data_name}: ãƒ‡ãƒ¼ã‚¿ãªã—")
        else:
            latest = result["latest_file"]
            pattern = result["update_pattern"]
            
            logger.info(f"âœ… {data_name}:")
            logger.info(f"  ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {result['total_files']}")
            logger.info(f"  ğŸ“„ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«: {latest['file_name']}")
            logger.info(f"  ğŸ“Š ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {latest['records']:,}")
            logger.info(f"  ğŸ“… æœ€çµ‚æ›´æ–°: {latest['last_modified_str']} ({latest['days_old']}æ—¥å‰)")
            logger.info(f"  ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {latest['file_size_mb']}MB")
            logger.info(f"  ğŸ“ˆ æ›´æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern['pattern']} ({pattern['frequency']})")
    
    def display_summary(self, results: Dict[str, Any]):
        """ç·åˆã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æˆé•·çŠ¶æ³ã‚µãƒãƒªãƒ¼")
        logger.info("="*60)
        
        active_data = []
        inactive_data = []
        
        for data_type, result in results.items():
            if result["status"] == "active":
                latest = result["latest_file"]
                days_old = latest["days_old"]
                records = latest["records"]
                pattern = result["update_pattern"]["pattern"]
                
                active_data.append({
                    "type": data_type,
                    "days_old": days_old,
                    "records": records,
                    "pattern": pattern
                })
            else:
                inactive_data.append({
                    "type": data_type,
                    "status": result["status"]
                })
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ‡ãƒ¼ã‚¿
        if active_data:
            logger.info("\nâœ… ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ‡ãƒ¼ã‚¿:")
            for data in sorted(active_data, key=lambda x: x["days_old"]):
                logger.info(f"  {data['type']}: {data['records']:,}ä»¶ ({data['days_old']}æ—¥å‰, {data['pattern']})")
        
        # å•é¡Œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿
        if inactive_data:
            logger.info("\nâš ï¸ å•é¡Œã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿:")
            for data in inactive_data:
                logger.info(f"  {data['type']}: {data['status']}")
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        logger.info("\nğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        old_data = [d for d in active_data if d["days_old"] > 7]
        static_data = [d for d in active_data if d["pattern"] == "static"]
        
        if old_data:
            logger.info("  ğŸ“… 1é€±é–“ä»¥ä¸Šæ›´æ–°ã•ã‚Œã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ã®åé›†ã‚’å®Ÿè¡Œ")
            for data in old_data:
                logger.info(f"    - {data['type']}")
        
        if static_data:
            logger.info("  ğŸ”„ ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ãŒå¤‰åŒ–ã—ã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ã®åé›†æ–¹æ³•ã‚’è¦‹ç›´ã—")
            for data in static_data:
                logger.info(f"    - {data['type']}")
        
        if inactive_data:
            logger.info("  âŒ ãƒ‡ãƒ¼ã‚¿åé›†ãŒåœæ­¢ã—ã¦ã„ã‚‹ã‚‚ã®ã‚’å†é–‹")
            for data in inactive_data:
                logger.info(f"    - {data['type']}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    checker = DataGrowthChecker()
    results = checker.check_all_data_types()
    
    # çµæœã‚’JSONã§å‡ºåŠ›ï¼ˆGitHub Actionsã§ã®åˆ©ç”¨æƒ³å®šï¼‰
    output_file = Path(__file__).parent / "data_growth_report.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "generated_at": datetime.now().isoformat(),
            "results": results
        }, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_file}")

if __name__ == "__main__":
    main()