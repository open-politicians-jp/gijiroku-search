#!/usr/bin/env python3
"""
Go2senkyo æ±äº¬éƒ½ãƒ‡ãƒ¼ã‚¿åé›†ãƒ†ã‚¹ãƒˆ

æ±äº¬éƒ½ï¼ˆéƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰13ï¼‰ã®ã¿ã‚’ãƒ†ã‚¹ãƒˆã—ã¦
go2senkyo.comã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’æ¤œè¨¼
"""

import json
import requests
import time
import re
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from urllib.parse import urljoin
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_tokyo_data_collection():
    """æ±äº¬éƒ½ãƒ‡ãƒ¼ã‚¿åé›†ãƒ†ã‚¹ãƒˆ"""
    logger.info("ğŸ§ª Go2senkyo æ±äº¬éƒ½ãƒ‡ãƒ¼ã‚¿åé›†ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
    ua = UserAgent()
    session = requests.Session()
    session.headers.update({
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
        'Connection': 'keep-alive'
    })
    
    # æ±äº¬éƒ½ãƒšãƒ¼ã‚¸
    tokyo_url = "https://sangiin.go2senkyo.com/2025/prefecture/13"
    
    try:
        logger.info(f"ğŸ“ æ±äº¬éƒ½ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹: {tokyo_url}")
        response = session.get(tokyo_url, timeout=30)
        
        logger.info(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        logger.info(f"ğŸ“„ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚º: {len(response.text)} æ–‡å­—")
        
        if response.status_code != 200:
            logger.error(f"âŒ ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
            return
        
        # HTMLã‚’è§£æ
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«
        title = soup.find('title')
        if title:
            logger.info(f"ğŸ“ ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title.get_text()}")
        
        # å€™è£œè€…è¦ç´ ã®æ¤œç´¢
        logger.info("ğŸ” å€™è£œè€…è¦ç´ ã‚’æ¤œç´¢ä¸­...")
        
        # æ§˜ã€…ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§å€™è£œè€…è¦ç´ ã‚’æ¢ã™
        candidate_selectors = [
            '.candidate-item',
            '.candidate-card',
            '.person-item',
            '[class*="candidate"]',
            '[class*="person"]',
            'article',
            '.item'
        ]
        
        found_elements = []
        for selector in candidate_selectors:
            elements = soup.select(selector)
            if elements:
                logger.info(f"âœ… {selector}: {len(elements)}å€‹ã®è¦ç´ ç™ºè¦‹")
                found_elements.extend(elements)
            else:
                logger.debug(f"âŒ {selector}: è¦ç´ ãªã—")
        
        # é‡è¤‡é™¤å»
        unique_elements = list(set(found_elements))
        logger.info(f"ğŸ“Š ãƒ¦ãƒ‹ãƒ¼ã‚¯è¦ç´ æ•°: {len(unique_elements)}")
        
        # åå‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å€™è£œè€…ã‚’æ¢ã™
        name_pattern = re.compile(r'[ä¸€-é¾¯]{2,4}[\sã€€]+[ä¸€-é¾¯]{2,8}')
        potential_candidates = []
        
        # ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åå‰ã‚‰ã—ãã‚‚ã®ã‚’æŠ½å‡º
        all_text = soup.get_text()
        name_matches = name_pattern.findall(all_text)
        
        if name_matches:
            logger.info(f"ğŸ¯ åå‰ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ: {len(name_matches)}ä»¶")
            for i, name in enumerate(name_matches[:10]):  # æœ€åˆã®10ä»¶ã‚’è¡¨ç¤º
                clean_name = name.strip().replace('\u3000', ' ')
                logger.info(f"  {i+1}: {clean_name}")
                potential_candidates.append(clean_name)
        
        # ãƒªãƒ³ã‚¯ã‹ã‚‰å€™è£œè€…ãƒšãƒ¼ã‚¸ã‚’æ¢ã™
        candidate_links = soup.find_all('a', href=re.compile(r'candidate|person|profile'))
        logger.info(f"ğŸ”— å€™è£œè€…é–¢é€£ãƒªãƒ³ã‚¯: {len(candidate_links)}ä»¶")
        
        for i, link in enumerate(candidate_links[:5]):  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
            href = link.get('href', '')
            text = link.get_text(strip=True)
            full_url = urljoin(tokyo_url, href) if href.startswith('/') else href
            logger.info(f"  {i+1}: {text[:30]} -> {full_url}")
        
        # ã‚µãƒ³ãƒ—ãƒ«å€™è£œè€…è©³ç´°å–å¾—ãƒ†ã‚¹ãƒˆ
        if candidate_links:
            logger.info("ğŸ§ª ã‚µãƒ³ãƒ—ãƒ«å€™è£œè€…è©³ç´°ãƒšãƒ¼ã‚¸ãƒ†ã‚¹ãƒˆ...")
            sample_link = candidate_links[0]
            sample_href = sample_link.get('href', '')
            
            if sample_href:
                sample_url = urljoin(tokyo_url, sample_href) if sample_href.startswith('/') else sample_href
                
                try:
                    time.sleep(2)  # é–“éš”ã‚’ã‚ã‘ã‚‹
                    detail_response = session.get(sample_url, timeout=20)
                    
                    if detail_response.status_code == 200:
                        detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                        
                        # è©³ç´°ãƒšãƒ¼ã‚¸ã®æƒ…å ±
                        detail_title = detail_soup.find('title')
                        logger.info(f"ğŸ“‹ è©³ç´°ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {detail_title.get_text() if detail_title else 'ãªã—'}")
                        
                        # ç”»åƒã‚’æ¢ã™
                        images = detail_soup.find_all('img')
                        logger.info(f"ğŸ–¼ï¸ ç”»åƒæ•°: {len(images)}")
                        
                        # æ”¿ç­–ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±
                        policy_keywords = ['æ”¿ç­–', 'ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ', 'å…¬ç´„', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«']
                        found_keywords = []
                        
                        detail_text = detail_soup.get_text()
                        for keyword in policy_keywords:
                            if keyword in detail_text:
                                found_keywords.append(keyword)
                        
                        logger.info(f"ğŸ“œ ç™ºè¦‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(found_keywords)}")
                        
                        # SNSãƒªãƒ³ã‚¯
                        social_links = []
                        all_links = detail_soup.find_all('a', href=True)
                        
                        for link in all_links:
                            href = link['href'].lower()
                            if any(social in href for social in ['twitter', 'facebook', 'instagram', 'youtube']):
                                social_links.append(link['href'])
                        
                        logger.info(f"ğŸ”— SNSãƒªãƒ³ã‚¯: {len(social_links)}ä»¶")
                        for social in social_links[:3]:
                            logger.info(f"  - {social}")
                    
                    else:
                        logger.warning(f"âš ï¸ è©³ç´°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {detail_response.status_code}")
                
                except Exception as e:
                    logger.error(f"âŒ è©³ç´°ãƒšãƒ¼ã‚¸ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        
        # çµæœã‚µãƒãƒªãƒ¼
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š æ±äº¬éƒ½ãƒ‡ãƒ¼ã‚¿åé›†ãƒ†ã‚¹ãƒˆçµæœ")
        logger.info("="*50)
        logger.info(f"âœ… ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹: æˆåŠŸ")
        logger.info(f"ğŸ“Š ç™ºè¦‹è¦ç´ æ•°: {len(unique_elements)}")
        logger.info(f"ğŸ¯ å€™è£œè€…åå€™è£œ: {len(potential_candidates)}")
        logger.info(f"ğŸ”— å€™è£œè€…ãƒªãƒ³ã‚¯: {len(candidate_links)}")
        
        # ç°¡æ˜“ãƒ‡ãƒ¼ã‚¿æ§‹é€ ä½œæˆ
        sample_data = {
            "prefecture": "æ±äº¬éƒ½",
            "url": tokyo_url,
            "status": "success",
            "found_elements": len(unique_elements),
            "potential_candidates": potential_candidates[:10],
            "candidate_links": [
                {"text": link.get_text(strip=True)[:50], "href": link.get('href', '')}
                for link in candidate_links[:5]
            ],
            "collected_at": datetime.now().isoformat()
        }
        
        # ãƒ†ã‚¹ãƒˆçµæœä¿å­˜
        output_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        test_file = output_dir / "tokyo_test_result.json"
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ ãƒ†ã‚¹ãƒˆçµæœä¿å­˜: {test_file}")
        
    except Exception as e:
        logger.error(f"âŒ æ±äº¬éƒ½ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    test_tokyo_data_collection()