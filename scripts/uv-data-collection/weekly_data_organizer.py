#!/usr/bin/env python3
"""
é€±æ¬¡ãƒ‡ãƒ¼ã‚¿æ•´ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ—¢å­˜ã®æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡ã«å†ç·¨æˆã—ã€è»½é‡åŒ–
FlexSearchã¸ã®å½±éŸ¿ã‚’æœ€å°åŒ–ã—ã¤ã¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
"""

import json
import csv
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WeeklyDataOrganizer:
    """é€±æ¬¡ãƒ‡ãƒ¼ã‚¿æ•´ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        
        # æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.speeches_dir = self.project_root / "frontend" / "public" / "data" / "speeches"
        self.manifestos_dir = self.project_root / "frontend" / "public" / "data" / "manifestos"
        self.bills_dir = self.project_root / "frontend" / "public" / "data" / "bills"
        self.questions_dir = self.project_root / "frontend" / "public" / "data" / "questions"
        self.petitions_dir = self.project_root / "frontend" / "public" / "data" / "petitions"
        self.legislators_dir = self.project_root / "frontend" / "public" / "data" / "legislators"
        
        # é€±æ¬¡æ•´ç†æ¸ˆã¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.weekly_data_dir = self.project_root / "frontend" / "public" / "data" / "weekly"
        self.weekly_data_dir.mkdir(parents=True, exist_ok=True)
        
    def get_week_info(self, date_str: str) -> tuple:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‹ã‚‰å¹´ãƒ»é€±ç•ªå·ã‚’å–å¾—"""
        try:
            date_obj = datetime.strptime(date_str[:10], "%Y-%m-%d")
            year = date_obj.year
            week = date_obj.isocalendar()[1]
            return year, week
        except:
            current = datetime.now()
            return current.year, current.isocalendar()[1]
            
    def organize_speeches_by_week(self):
        """è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡ã§æ•´ç†"""
        logger.info("ğŸ“ è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡æ•´ç†ä¸­...")
        
        weekly_speeches = {}
        
        # æ—¢å­˜ã®è­°äº‹éŒ²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        for json_file in self.speeches_dir.glob("*.json"):
            if json_file.name.endswith('.backup'):
                continue
                
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                speeches = data.get('data', [])
                for speech in speeches:
                    date = speech.get('date', '')
                    if date:
                        year, week = self.get_week_info(date)
                        week_key = f"{year}_w{week:02d}"
                        
                        if week_key not in weekly_speeches:
                            weekly_speeches[week_key] = []
                        weekly_speeches[week_key].append(speech)
                        
            except Exception as e:
                logger.error(f"âŒ {json_file.name} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                
        # é€±æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        for week_key, speeches in weekly_speeches.items():
            if speeches:
                self.save_weekly_speeches(week_key, speeches)
                
        logger.info(f"âœ… è­°äº‹éŒ²: {len(weekly_speeches)}é€±åˆ†ã‚’æ•´ç†")
        
    def save_weekly_speeches(self, week_key: str, speeches: List[Dict[str, Any]]):
        """é€±æ¬¡è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        year, week_num = week_key.split('_w')
        year_dir = self.weekly_data_dir / year
        year_dir.mkdir(exist_ok=True)
        
        # JSONå½¢å¼ï¼ˆFlexSearchç”¨ï¼‰
        json_filepath = year_dir / f"speeches_{week_key}.json"
        json_data = {
            "metadata": {
                "data_type": "speeches_weekly",
                "year": int(year),
                "week": int(week_num),
                "total_count": len(speeches),
                "generated_at": datetime.now().isoformat(),
                "file_size_kb": 0  # å¾Œã§è¨ˆç®—
            },
            "data": speeches
        }
        
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
            
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
        file_size_kb = json_filepath.stat().st_size / 1024
        json_data["metadata"]["file_size_kb"] = round(file_size_kb, 1)
        
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
            
        # CSVå½¢å¼ï¼ˆè»½é‡ç‰ˆï¼‰
        csv_filepath = year_dir / f"speeches_{week_key}.csv"
        if speeches:
            # å¿…è¦æœ€å°é™ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿
            csv_speeches = []
            for speech in speeches:
                csv_speech = {
                    "date": speech.get("date", ""),
                    "speaker": speech.get("speaker", ""),
                    "party": speech.get("party", ""),
                    "committee": speech.get("committee", ""),
                    "text": speech.get("text", "")[:500] + "..." if len(speech.get("text", "")) > 500 else speech.get("text", ""),  # 500æ–‡å­—åˆ¶é™
                    "url": speech.get("url", "")
                }
                csv_speeches.append(csv_speech)
                
            fieldnames = ["date", "speaker", "party", "committee", "text", "url"]
            with open(csv_filepath, 'w', encoding='utf-8', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(csv_speeches)
                
        csv_size_kb = csv_filepath.stat().st_size / 1024
        logger.info(f"ğŸ’¾ {week_key}: JSON {file_size_kb:.1f}KB, CSV {csv_size_kb:.1f}KB")
        
    def organize_parliamentary_by_week(self):
        """å›½ä¼šãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡ã§æ•´ç†"""
        logger.info("ğŸ›ï¸ å›½ä¼šãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡æ•´ç†ä¸­...")
        
        data_types = ["bills", "questions", "petitions"]
        
        for data_type in data_types:
            data_dir = getattr(self, f"{data_type}_dir")
            if not data_dir.exists():
                continue
                
            weekly_data = {}
            
            for json_file in data_dir.glob("*.json"):
                if json_file.name.endswith('.backup'):
                    continue
                    
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    items = data.get('data', [])
                    for item in items:
                        collected_at = item.get('collected_at', '')
                        if collected_at:
                            year, week = self.get_week_info(collected_at)
                            week_key = f"{year}_w{week:02d}"
                            
                            if week_key not in weekly_data:
                                weekly_data[week_key] = []
                            weekly_data[week_key].append(item)
                            
                except Exception as e:
                    logger.error(f"âŒ {json_file.name} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    
            # é€±æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
            for week_key, items in weekly_data.items():
                if items:
                    self.save_weekly_parliamentary(week_key, data_type, items)
                    
            logger.info(f"âœ… {data_type}: {len(weekly_data)}é€±åˆ†ã‚’æ•´ç†")
            
    def save_weekly_parliamentary(self, week_key: str, data_type: str, items: List[Dict[str, Any]]):
        """é€±æ¬¡å›½ä¼šãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        year, week_num = week_key.split('_w')
        year_dir = self.weekly_data_dir / year
        year_dir.mkdir(exist_ok=True)
        
        # JSONå½¢å¼
        json_filepath = year_dir / f"{data_type}_{week_key}.json"
        json_data = {
            "metadata": {
                "data_type": f"{data_type}_weekly",
                "year": int(year),
                "week": int(week_num),
                "total_count": len(items),
                "generated_at": datetime.now().isoformat()
            },
            "data": items
        }
        
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
            
        # CSVå½¢å¼ï¼ˆè»½é‡ç‰ˆï¼‰
        csv_filepath = year_dir / f"{data_type}_{week_key}.csv"
        if items:
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰é¸æŠ
            if data_type == "bills":
                fieldnames = ["house", "bill_number", "bill_name", "status", "submission_date"]
            elif data_type == "questions":
                fieldnames = ["house", "title", "question_number", "url"]
            elif data_type == "petitions":
                fieldnames = ["house", "petition_number", "title", "petitioner", "status"]
            else:
                fieldnames = list(items[0].keys()) if items else []
                
            with open(csv_filepath, 'w', encoding='utf-8', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
                writer.writeheader()
                writer.writerows(items)
                
    def create_weekly_index(self):
        """é€±æ¬¡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        logger.info("ğŸ“‹ é€±æ¬¡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆä¸­...")
        
        index_data = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "description": "é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"
            },
            "weeks": {}
        }
        
        for year_dir in self.weekly_data_dir.glob("*/"):
            if year_dir.is_dir():
                year = year_dir.name
                
                for data_file in year_dir.glob("*.json"):
                    if "_w" in data_file.stem:
                        parts = data_file.stem.split('_')
                        if len(parts) >= 2:
                            data_type = parts[0]
                            week_info = f"{year}_{parts[1]}"
                            
                            if week_info not in index_data["weeks"]:
                                index_data["weeks"][week_info] = {
                                    "year": int(year),
                                    "week": int(parts[1][1:]),
                                    "files": {}
                                }
                                
                            file_size = data_file.stat().st_size / 1024
                            index_data["weeks"][week_info]["files"][data_type] = {
                                "json_file": data_file.name,
                                "csv_file": data_file.with_suffix('.csv').name,
                                "size_kb": round(file_size, 1)
                            }
                            
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜
        index_filepath = self.weekly_data_dir / "weekly_index.json"
        with open(index_filepath, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"ğŸ“‹ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†: {len(index_data['weeks'])}é€±åˆ†")
        
    def generate_summary_stats(self):
        """é€±æ¬¡ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        total_weeks = 0
        total_files = 0
        total_size_mb = 0
        
        for year_dir in self.weekly_data_dir.glob("*/"):
            if year_dir.is_dir():
                json_files = list(year_dir.glob("*.json"))
                total_files += len(json_files)
                
                for json_file in json_files:
                    if "_w" in json_file.name:
                        total_size_mb += json_file.stat().st_size / (1024 * 1024)
                        
        unique_weeks = set()
        for year_dir in self.weekly_data_dir.glob("*/"):
            for json_file in year_dir.glob("*_w*.json"):
                week_part = json_file.stem.split('_')[-1]
                unique_weeks.add(f"{year_dir.name}_{week_part}")
                
        total_weeks = len(unique_weeks)
        
        logger.info(f"ğŸ“Š é€±æ¬¡ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
        logger.info(f"   - ç·é€±æ•°: {total_weeks}é€±")
        logger.info(f"   - ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}å€‹")
        logger.info(f"   - ç·ã‚µã‚¤ã‚º: {total_size_mb:.1f}MB")
        logger.info(f"   - é€±å¹³å‡ã‚µã‚¤ã‚º: {(total_size_mb/total_weeks if total_weeks > 0 else 0):.1f}MB")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    organizer = WeeklyDataOrganizer()
    
    # è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡æ•´ç†
    organizer.organize_speeches_by_week()
    
    # å›½ä¼šãƒ‡ãƒ¼ã‚¿ã‚’é€±æ¬¡æ•´ç†
    organizer.organize_parliamentary_by_week()
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    organizer.create_weekly_index()
    
    # çµ±è¨ˆè¡¨ç¤º
    organizer.generate_summary_stats()
    
    logger.info("âœ¨ é€±æ¬¡ãƒ‡ãƒ¼ã‚¿æ•´ç†å®Œäº†!")

if __name__ == "__main__":
    main()