#!/usr/bin/env python3
"""
æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿å¾©å…ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ
èª¤ã£ã¦é™¤å»ã•ã‚ŒãŸå€™è£œè€…ã‚’å¾©å…ƒã—ã€çœŸã®é‡è¤‡ã®ã¿ã‚’é™¤å»
"""

import json
import logging
from datetime import datetime
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def restore_correct_data():
    """æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ"""
    logger.info("ğŸ”§ æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿å¾©å…ƒé–‹å§‹...")
    
    # å…ƒã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    complete_file = data_dir / "go2senkyo_complete_structured_20250702_133412.json"
    
    with open(complete_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_count = len(data['data'])
    logger.info(f"ğŸ“Š å…ƒã®å®Œå…¨ãƒ‡ãƒ¼ã‚¿: {original_count}å")
    
    # çœŸã®é‡è¤‡ã®ã¿ã‚’æ¤œå‡ºï¼ˆåŒä¸€äººç‰©ãŒå®Œå…¨ã«åŒã˜æƒ…å ±ã§é‡è¤‡ã—ã¦ã„ã‚‹å ´åˆã®ã¿ï¼‰
    seen = set()
    unique_candidates = []
    removed_duplicates = []
    
    for candidate in data['data']:
        name = candidate.get('name', '')
        prefecture = candidate.get('prefecture', '')
        party = candidate.get('party', '')
        profile_url = candidate.get('profile_url', '')
        
        # é‡è¤‡åˆ¤å®šã‚­ãƒ¼: åå‰ + éƒ½é“åºœçœŒ + æ”¿å…š + URL
        # ã“ã‚Œã«ã‚ˆã‚ŠçœŸã®é‡è¤‡ã®ã¿ã‚’æ¤œå‡º
        key = f"{name}_{prefecture}_{party}_{profile_url}"
        
        if key not in seen:
            seen.add(key)
            unique_candidates.append(candidate)
        else:
            removed_duplicates.append(candidate)
            logger.info(f"ğŸ—‘ï¸ çœŸã®é‡è¤‡é™¤å»: {name} ({prefecture}, {party})")
    
    # çµ±è¨ˆå†è¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    
    for candidate in unique_candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
    
    # æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    correct_data = {
        "metadata": {
            "data_type": "go2senkyo_complete_structured_sangiin_2025_correct",
            "collection_method": "structured_html_extraction_all_47_prefectures_correct_dedup",
            "total_candidates": len(unique_candidates),
            "candidates_with_kana": len([c for c in unique_candidates if c.get('name_kana')]),
            "successful_prefectures": 47,
            "failed_prefectures": 0,
            "true_duplicates_removed": len(removed_duplicates),
            "generated_at": datetime.now().isoformat(),
            "source_site": "sangiin.go2senkyo.com",
            "coverage": {
                "constituency_types": 1,
                "parties": len(party_stats),
                "prefectures": len(prefecture_stats)
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": prefecture_stats,
            "by_constituency_type": {"single_member": len(unique_candidates)}
        },
        "data": unique_candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = data_dir / f"go2senkyo_complete_correct_{timestamp}.json"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(correct_data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(correct_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ æ­£ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {output_file}")
    logger.info(f"ğŸ“ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {latest_file}")
    
    # çµæœå ±å‘Š
    logger.info(f"\nâœ… æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿å¾©å…ƒå®Œäº†:")
    logger.info(f"  å…ƒãƒ‡ãƒ¼ã‚¿: {original_count}å")
    logger.info(f"  çœŸã®é‡è¤‡é™¤å»æ•°: {len(removed_duplicates)}å")
    logger.info(f"  æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {len(unique_candidates)}å")
    logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
    logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_stats)}éƒ½é“åºœçœŒ")
    
    # ä¸»è¦éƒ½é“åºœçœŒã®ç¢ºèª
    logger.info(f"\nğŸ“ ä¸»è¦éƒ½é“åºœçœŒ:")
    major_prefs = sorted(prefecture_stats.items(), key=lambda x: x[1], reverse=True)[:10]
    for pref, count in major_prefs:
        logger.info(f"  {pref}: {count}å")
    
    # å³¶æ ¹çœŒãƒ»é«˜çŸ¥çœŒã®å¾©å…ƒç¢ºèª
    shimane_count = prefecture_stats.get('å³¶æ ¹çœŒ', 0)
    kochi_count = prefecture_stats.get('é«˜çŸ¥çœŒ', 0)
    logger.info(f"\nğŸ” å¾©å…ƒç¢ºèª:")
    logger.info(f"  å³¶æ ¹çœŒ: {shimane_count}å")
    logger.info(f"  é«˜çŸ¥çœŒ: {kochi_count}å")
    
    if removed_duplicates:
        logger.info(f"\nğŸ—‘ï¸ é™¤å»ã•ã‚ŒãŸçœŸã®é‡è¤‡:")
        for candidate in removed_duplicates:
            logger.info(f"  - {candidate['name']} ({candidate.get('prefecture', 'ä¸æ˜')})")
    
    return output_file

if __name__ == "__main__":
    restore_correct_data()