#!/usr/bin/env python3
"""
å€™è£œè€…ã®é–¢é€£ãƒªãƒ³ã‚¯åé›† - p_seijika_profle_data_sitelistæ´»ç”¨
"""

import json
import logging
import re
from datetime import datetime
from pathlib import Path
from collect_go2senkyo_optimized import Go2senkyoOptimizedCollector
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def collect_candidate_links():
    """æ—¢å­˜å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã«é–¢é€£ãƒªãƒ³ã‚¯ã‚’è¿½åŠ """
    logger.info("ğŸ”— å€™è£œè€…é–¢é€£ãƒªãƒ³ã‚¯åé›†é–‹å§‹...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    if not latest_file.exists():
        logger.error("æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    candidates = data.get('data', [])
    logger.info(f"ğŸ“Š å¯¾è±¡å€™è£œè€…: {len(candidates)}å")
    
    collector = Go2senkyoOptimizedCollector()
    updated_candidates = []
    success_count = 0
    
    for i, candidate in enumerate(candidates):
        try:
            profile_url = candidate.get('profile_url', '')
            name = candidate.get('name', '')
            
            logger.info(f"ğŸ“ {i+1}/{len(candidates)}: {name}")
            
            if not profile_url:
                logger.debug(f"  ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLãªã—: {name}")
                updated_candidates.append(candidate)
                continue
            
            # é–¢é€£ãƒªãƒ³ã‚¯å–å¾—
            links_info = get_candidate_links(profile_url, collector)
            
            if links_info:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«é–¢é€£ãƒªãƒ³ã‚¯æƒ…å ±ã‚’è¿½åŠ 
                candidate.update(links_info)
                success_count += 1
                logger.info(f"  âœ… ãƒªãƒ³ã‚¯å–å¾—æˆåŠŸ: {len(links_info.get('websites', []))}å€‹")
            else:
                logger.debug(f"  é–¢é€£ãƒªãƒ³ã‚¯ãªã—: {name}")
            
            updated_candidates.append(candidate)
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 10 == 0:
                logger.info(f"é€²æ—: {i+1}/{len(candidates)} ({(i+1)/len(candidates)*100:.1f}%)")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            collector.random_delay(0.3, 0.8)
            
        except Exception as e:
            logger.error(f"âŒ {name}ã‚¨ãƒ©ãƒ¼: {e}")
            updated_candidates.append(candidate)
            continue
    
    # ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    data['data'] = updated_candidates
    data['metadata']['collection_stats']['with_websites'] = success_count
    data['metadata']['quality_metrics']['website_coverage'] = f"{success_count/len(candidates)*100:.1f}%"
    data['metadata']['generated_at'] = datetime.now().isoformat()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    enhanced_file = data_dir / f"go2senkyo_enhanced_{timestamp}.json"
    
    with open(enhanced_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ¯ é–¢é€£ãƒªãƒ³ã‚¯åé›†å®Œäº†:")
    logger.info(f"  æˆåŠŸ: {success_count}/{len(candidates)}å ({success_count/len(candidates)*100:.1f}%)")
    logger.info(f"ğŸ“ ä¿å­˜: {enhanced_file}")

def get_candidate_links(profile_url, collector):
    """å€‹åˆ¥ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰é–¢é€£ãƒªãƒ³ã‚¯ã‚’å–å¾—"""
    links_info = {}
    
    try:
        if not profile_url or not profile_url.startswith('http'):
            return links_info
        
        response = collector.session.get(profile_url, timeout=15)
        if response.status_code != 200:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {response.status_code}")
            return links_info
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # p_seijika_profle_data_sitelist ã‚¯ãƒ©ã‚¹ã‹ã‚‰é–¢é€£ã‚µã‚¤ãƒˆã‚’å–å¾—
        sitelist_elem = soup.find(class_='p_seijika_profle_data_sitelist')
        if sitelist_elem:
            websites = []
            site_links = sitelist_elem.find_all('a', href=True)
            
            for link in site_links:
                url = link.get('href', '').strip()
                title = link.get_text(strip=True)
                
                if url and title and url.startswith('http'):
                    websites.append({
                        "url": url,
                        "title": title
                    })
            
            if websites:
                links_info["websites"] = websites
                # æœ€åˆã®ãƒªãƒ³ã‚¯ã‚’å…¬å¼ã‚µã‚¤ãƒˆã¨ã—ã¦è¨­å®š
                links_info["official_website"] = websites[0]["url"]
                logger.debug(f"é–¢é€£ã‚µã‚¤ãƒˆå–å¾—: {len(websites)}å€‹")
        
        # ãã®ä»–ã®è©³ç´°æƒ…å ±ã‚‚å–å¾—
        additional_info = get_additional_profile_info(soup)
        links_info.update(additional_info)
        
    except Exception as e:
        logger.debug(f"é–¢é€£ãƒªãƒ³ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return links_info

def get_additional_profile_info(soup):
    """è¿½åŠ ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—"""
    info = {}
    
    try:
        # å¹´é½¢æƒ…å ±
        age_elem = soup.find(string=re.compile(r'(\d+)æ­³'))
        if age_elem:
            age_match = re.search(r'(\d+)æ­³', age_elem)
            if age_match:
                info["age_info"] = age_match.group(1)
        
        # å‡ºèº«åœ°æƒ…å ±
        birthplace_patterns = [
            r'å‡ºèº«[ï¼š:\s]*([éƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘\w]+)',
            r'ç”Ÿã¾ã‚Œ[ï¼š:\s]*([éƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘\w]+)',
        ]
        
        text_content = soup.get_text()
        for pattern in birthplace_patterns:
            match = re.search(pattern, text_content)
            if match:
                birthplace = match.group(1).strip()
                if len(birthplace) <= 20:  # å¦¥å½“ãªé•·ã•ã‹ãƒã‚§ãƒƒã‚¯
                    info["birthplace"] = birthplace
                break
        
        # è·æ¥­ãƒ»è‚©æ›¸ãæƒ…å ±
        occupation_selectors = [
            '.job', '.occupation', '.title', '.position',
            '[class*="job"]', '[class*="occupation"]', '[class*="title"]'
        ]
        
        for selector in occupation_selectors:
            elem = soup.select_one(selector)
            if elem:
                occupation = elem.get_text(strip=True)
                if occupation and len(occupation) <= 50 and len(occupation) >= 2:
                    info["occupation"] = occupation
                    break
        
        # çµŒæ­´æƒ…å ±ï¼ˆæ¦‚è¦ã®ã¿ï¼‰
        career_selectors = [
            '.profile', '.career', '.history', '.biography',
            '[class*="profile"]', '[class*="career"]', '[class*="history"]'
        ]
        
        for selector in career_selectors:
            elem = soup.select_one(selector)
            if elem:
                career = elem.get_text(strip=True)
                if career and len(career) >= 50:  # æ„å‘³ã®ã‚ã‚‹çµŒæ­´æƒ…å ±
                    info["career"] = career[:300]  # 300æ–‡å­—ã¾ã§
                    break
        
    except Exception as e:
        logger.debug(f"è¿½åŠ æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return info

if __name__ == "__main__":
    collect_candidate_links()