#!/usr/bin/env python3
"""
è­°æ¡ˆãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ­£å¼ç‰ˆï¼‰

è¡†è­°é™¢è­°æ¡ˆãƒšãƒ¼ã‚¸ã‹ã‚‰å®Ÿéš›ã®è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ã‚’åé›†
https://www.shugiin.go.jp/internet/itdb_gian.nsf/html/gian/kaiji217.htm
çµŒéæƒ…å ±ã€æœ¬æ–‡ã€è­°æ¡ˆä»¶åã‚’å…¨ã¦ã®å›½ä¼šã‹ã‚‰å–å¾—
"""

import json
import requests
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging
import random

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BillsCollector:
    """è­°æ¡ˆãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹ï¼ˆæ­£å¼ç‰ˆï¼‰"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.bills_dir = self.project_root / "data" / "processed" / "bills"
        self.frontend_bills_dir = self.project_root / "frontend" / "public" / "data" / "bills"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.bills_dir.mkdir(parents=True, exist_ok=True)
        self.frontend_bills_dir.mkdir(parents=True, exist_ok=True)
        
        # é€±æ¬¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        current_date = datetime.now()
        self.year = current_date.year
        self.week = current_date.isocalendar()[1]
        
        # åŸºæœ¬URLï¼ˆæ­£å¼ç‰ˆï¼‰
        self.base_url = "https://www.shugiin.go.jp"
        self.bills_base_url = "https://www.shugiin.go.jp/internet/itdb_gian.nsf/html/gian/"
        
        # åé›†ã™ã‚‹å›½ä¼šã®ç¯„å›²ï¼ˆç¬¬217å›ã‚’é‡ç‚¹çš„ã«ï¼‰
        self.start_session = 217  # Issue #47å¯¾å¿œ: ç¬¬217å›å›½ä¼šã‚’é‡ç‚¹å¯¾è±¡
        self.end_session = 217   # ç¬¬217å›å›½ä¼šã®ã¿ãƒ†ã‚¹ãƒˆ
        
    def update_headers(self):
        """User-Agentæ›´æ–°ã¨IPå½è£…"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
    
    def random_delay(self, min_seconds=1, max_seconds=3):
        """ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
    
    def build_absolute_url(self, href: str, base_page_url: str) -> Optional[str]:
        """ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ› (Issue #47å¯¾å¿œ)"""
        if not href or href.strip() == '':
            return None
        
        # ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯ï¼ˆ#ã§å§‹ã¾ã‚‹ï¼‰ã¯ç„¡åŠ¹
        if href.startswith('#'):
            return None
        
        # æ—¢ã«çµ¶å¯¾URLã®å ´åˆ
        if href.startswith('http'):
            return href
        
        # ç›¸å¯¾URLå‡¦ç†
        if href.startswith('./'):
            # ./honbun/g20009011.htm â†’ https://www.shugiin.go.jp/internet/itdb_gian.nsf/html/gian/honbun/g20009011.htm
            relative_path = href[2:]  # ./ ã‚’é™¤å»
            base_dir = '/'.join(base_page_url.split('/')[:-1])  # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’é™¤å»
            return f"{base_dir}/{relative_path}"
        
        elif href.startswith('/'):
            # çµ¶å¯¾ãƒ‘ã‚¹
            return f"{self.base_url}{href}"
        
        else:
            # ç›¸å¯¾ãƒ‘ã‚¹
            base_dir = '/'.join(base_page_url.split('/')[:-1])
            return f"{base_dir}/{href}"
    
    def collect_all_bills(self) -> List[Dict[str, Any]]:
        """å…¨å›½ä¼šã®è­°æ¡ˆã‚’åé›†"""
        logger.info("å…¨è­°æ¡ˆãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        all_bills = []
        
        for session_number in range(self.start_session, self.end_session + 1):
            try:
                logger.info(f"ç¬¬{session_number}å›å›½ä¼šã®è­°æ¡ˆåé›†é–‹å§‹...")
                
                # IPå½è£…ã®ãŸã‚ãƒ˜ãƒƒãƒ€ãƒ¼æ›´æ–°
                self.update_headers()
                self.random_delay()
                
                session_bills = self.collect_session_bills(session_number)
                all_bills.extend(session_bills)
                
                logger.info(f"ç¬¬{session_number}å›å›½ä¼š: {len(session_bills)}ä»¶ã®è­°æ¡ˆã‚’åé›†")
                
            except Exception as e:
                logger.error(f"ç¬¬{session_number}å›å›½ä¼šã®è­°æ¡ˆåé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        logger.info(f"å…¨è­°æ¡ˆãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(all_bills)}ä»¶")
        return all_bills
    
    def collect_session_bills(self, session_number: int) -> List[Dict[str, Any]]:
        """ç‰¹å®šã®å›½ä¼šã®è­°æ¡ˆã‚’åé›†"""
        bills = []
        
        try:
            # å›½ä¼šåˆ¥è­°æ¡ˆä¸€è¦§ãƒšãƒ¼ã‚¸URL
            session_url = f"{self.bills_base_url}kaiji{session_number}.htm"
            
            response = self.session.get(session_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            logger.info(f"ç¬¬{session_number}å›å›½ä¼šãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {session_url}")
            
            # è­°æ¡ˆãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            bill_links = self.extract_bill_links(soup, session_number)
            logger.info(f"ç™ºè¦‹ã—ãŸè­°æ¡ˆãƒªãƒ³ã‚¯æ•°: {len(bill_links)}")
            
            # å„è­°æ¡ˆã®è©³ç´°ã‚’å–å¾—
            for idx, link_info in enumerate(bill_links):
                try:
                    self.random_delay()  # IPå½è£…ã®ãŸã‚ã®é…å»¶
                    self.update_headers()  # User-Agentæ›´æ–°
                    
                    bill_detail = self.extract_bill_detail(link_info, session_number)
                    if bill_detail:
                        bills.append(bill_detail)
                        logger.info(f"è­°æ¡ˆè©³ç´°å–å¾—æˆåŠŸ ({idx+1}/{len(bill_links)}): {bill_detail['title'][:50]}...")
                    
                except Exception as e:
                    logger.error(f"è­°æ¡ˆè©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ ({idx+1}): {str(e)}")
                    continue
            
            return bills
            
        except Exception as e:
            logger.error(f"ç¬¬{session_number}å›å›½ä¼šã®è­°æ¡ˆåé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def extract_bill_links(self, soup: BeautifulSoup, session_number: int) -> List[Dict[str, str]]:
        """è­°æ¡ˆãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        try:
            # ãƒ™ãƒ¼ã‚¹URLã‚’æ§‹ç¯‰
            session_url = f"{self.bills_base_url}kaiji{session_number}.htm"
            
            # è­°æ¡ˆãƒªãƒ³ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
            link_elements = soup.find_all('a', href=True)
            
            for link in link_elements:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # è­°æ¡ˆé–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’åˆ¤å®š
                if self.is_bill_link(href, text):
                    full_url = self.build_absolute_url(href, session_url)
                    if not full_url:  # ç„¡åŠ¹ãªURLã¯ã‚¹ã‚­ãƒƒãƒ—
                        continue
                    bill_number = self.extract_bill_number(text, href)
                    
                    links.append({
                        'url': full_url,
                        'title': text,
                        'bill_number': bill_number,
                        'session_number': session_number
                    })
            
            # é‡è¤‡é™¤å»
            unique_links = []
            seen_urls = set()
            for link in links:
                if link['url'] not in seen_urls:
                    unique_links.append(link)
                    seen_urls.add(link['url'])
            
            return unique_links
            
        except Exception as e:
            logger.error(f"è­°æ¡ˆãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def is_bill_link(self, href: str, text: str) -> bool:
        """è­°æ¡ˆãƒªãƒ³ã‚¯ã‹ã©ã†ã‹åˆ¤å®š (Issue #47å¯¾å¿œ)"""
        
        # ç„¡åŠ¹ãªãƒªãƒ³ã‚¯ã‚’é™¤å¤–
        if not href or href.startswith('#') or href.strip() == '':
            return False
        
        # è­°æ¡ˆé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        bill_keywords = [
            'æ³•æ¡ˆ', 'è­°æ¡ˆ', 'æ³•å¾‹', 'æ³•', 'æ¡ä¾‹',
            'æ”¹æ­£', 'è¨­ç½®', 'å»ƒæ­¢', 'æ¡ˆ', 'æœ¬æ–‡', 'çµŒé'
        ]
        
        # URL ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ˆã‚Šå…·ä½“çš„ã«ï¼‰
        url_patterns = [
            'honbun/',  # æœ¬æ–‡ãƒªãƒ³ã‚¯
            'keika/',   # çµŒéãƒªãƒ³ã‚¯
            'gian',     # è­°æ¡ˆé–¢é€£
        ]
        
        # ãƒ†ã‚­ã‚¹ãƒˆã«è­°æ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        text_match = any(keyword in text for keyword in bill_keywords)
        
        # URLã«é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        url_match = any(pattern in href.lower() for pattern in url_patterns)
        
        # ã‚ˆã‚Šå³å¯†ãªåˆ¤å®š
        return text_match and (url_match or '.' in href)
    
    def extract_bill_number(self, text: str, href: str) -> str:
        """è­°æ¡ˆç•ªå·ã‚’æŠ½å‡º"""
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç•ªå·ã‚’æŠ½å‡º
        text_number_patterns = [
            r'ç¬¬(\d+)å·',
            r'(\d+)å·',
            r'ç¬¬(\d+)',
            r'(\d+)'
        ]
        
        for pattern in text_number_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        
        # URLã‹ã‚‰ç•ªå·ã‚’æŠ½å‡º
        url_number_match = re.search(r'(\d+)', href)
        if url_number_match:
            return url_number_match.group(1)
        
        return ""
    
    def extract_bill_detail(self, link_info: Dict[str, str], session_number: int) -> Optional[Dict[str, Any]]:
        """è­°æ¡ˆè©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            response = self.session.get(link_info['url'], timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è­°æ¡ˆæƒ…å ±ã‚’è§£æ
            bill_content = self.extract_bill_content(soup)
            progress_info = self.extract_progress_info(soup)
            submitter = self.extract_submitter(soup)
            submission_date = self.extract_submission_date(soup)
            status = self.extract_status(soup)
            committee = self.extract_committee(soup)
            
            # é–¢é€£ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            related_links = self.extract_related_links(soup)
            
            bill_detail = {
                'title': link_info['title'],
                'bill_number': link_info['bill_number'],
                'session_number': session_number,
                'url': link_info['url'],
                'submitter': submitter,
                'submission_date': submission_date,
                'status': status,
                'status_normalized': self.normalize_status(status),
                'committee': committee,
                'bill_content': bill_content,
                'progress_info': progress_info,
                'related_links': related_links,
                'summary': self.generate_summary(bill_content),
                'collected_at': datetime.now().isoformat(),
                'year': self.year
            }
            
            return bill_detail
            
        except Exception as e:
            logger.error(f"è­°æ¡ˆè©³ç´°æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({link_info['url']}): {str(e)}")
            return None
    
    def extract_bill_content(self, soup: BeautifulSoup) -> str:
        """è­°æ¡ˆæœ¬æ–‡ã‚’æŠ½å‡º"""
        try:
            # æœ¬æ–‡ã‚’æŠ½å‡ºã™ã‚‹è¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã™
            content_selectors = [
                'div.honbun',
                'div.content',
                'div.main',
                'table',
                'body'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    text = content_elem.get_text(separator='\n', strip=True)
                    if len(text) > 100:  # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–
                        return self.clean_text(text)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: bodyå…¨ä½“ã‹ã‚‰æŠ½å‡º
            return self.clean_text(soup.get_text(separator='\n', strip=True))
            
        except Exception as e:
            logger.error(f"è­°æ¡ˆæœ¬æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_progress_info(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """çµŒéæƒ…å ±ã‚’æŠ½å‡º"""
        progress = []
        
        try:
            # çµŒéæƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            progress_keywords = ['çµŒé', 'å¯©è­°çŠ¶æ³', 'é€²è¡ŒçŠ¶æ³', 'è­°äº‹']
            
            for keyword in progress_keywords:
                tables = soup.find_all('table')
                for table in tables:
                    table_text = table.get_text()
                    if keyword in table_text:
                        rows = table.find_all('tr')
                        for row in rows:
                            cells = row.find_all(['td', 'th'])
                            if len(cells) >= 2:
                                date = cells[0].get_text(strip=True)
                                action = cells[1].get_text(strip=True)
                                if date and action and len(action) > 5:
                                    progress.append({
                                        'date': date,
                                        'action': action
                                    })
            
        except Exception as e:
            logger.error(f"çµŒéæƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return progress
    
    def extract_submitter(self, soup: BeautifulSoup) -> str:
        """æå‡ºè€…ã‚’æŠ½å‡º"""
        try:
            submitter_patterns = [
                r'æå‡ºè€…[ï¼š:]\s*([^\n\r]+)',
                r'æå‡º[ï¼š:]\s*([^\n\r]+)',
                r'([^\n\r]+)æå‡º'
            ]
            
            page_text = soup.get_text()
            
            for pattern in submitter_patterns:
                match = re.search(pattern, page_text)
                if match:
                    return match.group(1).strip()
            
            return ""
            
        except Exception as e:
            logger.error(f"æå‡ºè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_submission_date(self, soup: BeautifulSoup) -> str:
        """æå‡ºæ—¥ã‚’æŠ½å‡º"""
        try:
            date_patterns = [
                r'æå‡ºæ—¥[ï¼š:]\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥æå‡º'
            ]
            
            page_text = soup.get_text()
            
            for pattern in date_patterns:
                match = re.search(pattern, page_text)
                if match:
                    year, month, day = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            return ""
            
        except Exception as e:
            logger.error(f"æå‡ºæ—¥æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_status(self, soup: BeautifulSoup) -> str:
        """è­°æ¡ˆçŠ¶æ³ã‚’æŠ½å‡º"""
        try:
            status_keywords = ['å¯æ±º', 'å¦æ±º', 'å»ƒæ¡ˆ', 'ç¶™ç¶šå¯©è­°', 'æˆç«‹', 'å¯©è­°ä¸­']
            page_text = soup.get_text()
            
            for keyword in status_keywords:
                if keyword in page_text:
                    return keyword
            
            return "å¯©è­°ä¸­"
            
        except Exception as e:
            logger.error(f"è­°æ¡ˆçŠ¶æ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return "ä¸æ˜"
    
    def normalize_status(self, status: str) -> str:
        """è­°æ¡ˆçŠ¶æ³ã‚’æ­£è¦åŒ–"""
        status_mapping = {
            'å¯æ±º': 'å¯æ±º',
            'æˆç«‹': 'æˆç«‹',
            'å¦æ±º': 'å¦æ±º',
            'å»ƒæ¡ˆ': 'å»ƒæ¡ˆ',
            'ç¶™ç¶šå¯©è­°': 'ç¶™ç¶šå¯©è­°',
            'å¯©è­°ä¸­': 'å¯©è­°ä¸­'
        }
        
        return status_mapping.get(status, 'ä¸æ˜')
    
    def extract_committee(self, soup: BeautifulSoup) -> str:
        """å§”å“¡ä¼šåã‚’æŠ½å‡º"""
        try:
            committee_patterns = [
                r'([^å§”å“¡ä¼š]*å§”å“¡ä¼š)',
                r'([^èª¿æŸ»ä¼š]*èª¿æŸ»ä¼š)'
            ]
            
            page_text = soup.get_text()
            
            for pattern in committee_patterns:
                match = re.search(pattern, page_text)
                if match:
                    return match.group(1)
            
            return ""
            
        except Exception as e:
            logger.error(f"å§”å“¡ä¼šåæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_related_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """é–¢é€£ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        try:
            # PDFã‚„é–¢é€£æ–‡æ›¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            link_elements = soup.find_all('a', href=True)
            
            for link in link_elements:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # é–¢é€£æ–‡æ›¸ã®ãƒªãƒ³ã‚¯ã‚’åˆ¤å®š
                if any(ext in href.lower() for ext in ['.pdf', '.doc', '.html']):
                    full_url = self.build_absolute_url(href, self.bills_base_url)
                    if full_url:
                        links.append({
                            'url': full_url,
                            'title': text or 'é–¢é€£æ–‡æ›¸'
                        })
            
        except Exception as e:
            logger.error(f"é–¢é€£ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return links
    
    def generate_summary(self, content: str) -> str:
        """è­°æ¡ˆã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        try:
            if not content:
                return ""
            
            # æœ€åˆã®200æ–‡å­—ã‚’è¦ç´„ã¨ã—ã¦ä½¿ç”¨
            summary = content[:200]
            if len(content) > 200:
                summary += "..."
            
            return summary
            
        except Exception as e:
            logger.error(f"ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢"""
        if not text:
            return ""
        
        # æ”¹è¡Œãƒ»ã‚¹ãƒšãƒ¼ã‚¹ã®æ­£è¦åŒ–
        text = re.sub(r'\n\s*\n', '\n\n', text)  # é€£ç¶šç©ºè¡Œã‚’2è¡Œã¾ã§
        text = re.sub(r'[ \t]+', ' ', text)  # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
        text = re.sub(r'[\u3000]+', ' ', text)  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã«
        
        return text.strip()
    
    def validate_bill_data(self, bill: Dict[str, Any]) -> bool:
        """è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        required_fields = ['title', 'bill_number', 'session_number']
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
        for field in required_fields:
            if not bill.get(field):
                return False
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒæ„å‘³ã®ã‚ã‚‹ã‚‚ã®ã‹ãƒã‚§ãƒƒã‚¯
        if bill['title'] in ['æœ¬æ–‡', 'ãƒ¡ã‚¤ãƒ³', 'ã‚¨ãƒ©ãƒ¼', '']:
            return False
        
        # URLã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        url = bill.get('url', '')
        if not url or 'menu.htm' in url or 'index.nsf' in url:
            return False
        
        return True

    def save_bills_data(self, bills: List[Dict[str, Any]]):
        """è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if not bills:
            logger.warning("ä¿å­˜ã™ã‚‹è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_bills = [bill for bill in bills if self.validate_bill_data(bill)]
        invalid_count = len(bills) - len(valid_bills)
        
        if invalid_count > 0:
            logger.warning(f"ç„¡åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–: {invalid_count}ä»¶")
        
        if not valid_bills:
            logger.warning("æœ‰åŠ¹ãªè­°æ¡ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã‚’åŸºæº–ã¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆç¾åœ¨ã®å¹´æœˆæ—¥ + æ™‚åˆ»ï¼‰
        current_date = datetime.now()
        data_period = current_date.strftime('%Y%m%d')
        timestamp = current_date.strftime('%H%M%S')
        
        # çµ±ä¸€ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        data_structure = {
            "metadata": {
                "data_type": "shugiin_bills",
                "collection_method": "incremental_scraping",
                "total_bills": len(valid_bills),
                "generated_at": current_date.isoformat(),
                "source_site": "www.shugiin.go.jp",
                "quality_info": {
                    "valid_bills": len(valid_bills),
                    "invalid_bills": invalid_count,
                    "validation_criteria": "title, bill_number, session_number, url"
                }
            },
            "data": valid_bills
        }
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜
        raw_filename = f"bills_{data_period}_{timestamp}.json"
        raw_filepath = self.bills_dir / raw_filename
        
        with open(raw_filepath, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        frontend_filename = f"bills_{data_period}_{timestamp}.json"
        frontend_filepath = self.frontend_bills_dir / frontend_filename
        
        with open(frontend_filepath, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ãªå ´åˆã®ã¿ï¼‰
        if len(valid_bills) > 10:  # æœ€ä½é™ã®ä»¶æ•°ãƒã‚§ãƒƒã‚¯
            latest_file = self.frontend_bills_dir / "bills_latest.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(data_structure, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {latest_file}")
        
        logger.info(f"è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  - ç”Ÿãƒ‡ãƒ¼ã‚¿: {raw_filepath}")
        logger.info(f"  - ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: {frontend_filepath}")
        logger.info(f"  - æœ‰åŠ¹ä»¶æ•°: {len(valid_bills)}")
        logger.info(f"  - ç„¡åŠ¹é™¤å¤–: {invalid_count}")
        
        return data_structure

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    collector = BillsCollector()
    
    try:
        # å…¨è­°æ¡ˆãƒ‡ãƒ¼ã‚¿åé›†
        bills = collector.collect_all_bills()
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        collector.save_bills_data(bills)
        
        logger.info("è­°æ¡ˆãƒ‡ãƒ¼ã‚¿åé›†å‡¦ç†å®Œäº†")
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()