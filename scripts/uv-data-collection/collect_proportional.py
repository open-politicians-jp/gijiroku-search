#!/usr/bin/env python3
"""
æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›†
"""

import json
import logging
import re
from datetime import datetime
from pathlib import Path
from collect_go2senkyo_optimized import Go2senkyoOptimizedCollector

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def collect_proportional():
    """æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›†"""
    logger.info("ğŸš€ æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
    
    collector = Go2senkyoOptimizedCollector()
    all_proportional_candidates = []
    
    # æ¯”ä¾‹ä»£è¡¨æ”¿å…šãƒªã‚¹ãƒˆï¼ˆç™ºè¦‹ã•ã‚ŒãŸã‚‚ã®ï¼‰
    proportional_parties = {
        "å…¨æ”¿å…š": "all",
        "è‡ªç”±æ°‘ä¸»å…š": "4",
        "ç«‹æ†²æ°‘ä¸»å…š": "616", 
        "å…¬æ˜å…š": "3",
        "æ—¥æœ¬ç¶­æ–°ã®ä¼š": "19",
        "æ—¥æœ¬å…±ç”£å…š": "2",
        "å›½æ°‘æ°‘ä¸»å…š": "617",
        "ã‚Œã„ã‚æ–°é¸çµ„": "618",
        "å‚æ”¿å…š": "619",
        "ç¤¾ä¼šæ°‘ä¸»å…š": "1",
        "NHKå…š": "620"
    }
    
    try:
        # å„æ”¿å…šã®æ¯”ä¾‹ä»£è¡¨å€™è£œè€…ã‚’åé›†
        for party_name, party_id in proportional_parties.items():
            try:
                logger.info(f"ğŸ“ {party_name} æ¯”ä¾‹ä»£è¡¨åé›†ä¸­...")
                
                if party_id == "all":
                    # å…¨æ”¿å…šãƒšãƒ¼ã‚¸
                    url = f"{collector.base_url}/2025/hirei/all"
                else:
                    # æ”¿å…šåˆ¥ãƒšãƒ¼ã‚¸
                    url = f"{collector.base_url}/2025/hirei/{party_id}"
                
                collector.random_delay(1, 2)
                response = collector.session.get(url, timeout=30)
                
                if response.status_code != 200:
                    logger.warning(f"{party_name}: HTTP {response.status_code}")
                    continue
                
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ¯”ä¾‹ä»£è¡¨å€™è£œè€…ãƒ–ãƒ­ãƒƒã‚¯å–å¾—
                candidate_blocks = soup.find_all('div', class_='p_senkyoku_list_block')
                
                if not candidate_blocks:
                    # åˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚‚è©¦ã™
                    candidate_blocks = soup.find_all(['div', 'li'], class_=re.compile(r'candidate|person|member'))
                
                logger.info(f"{party_name}: {len(candidate_blocks)}å€‹ãƒ–ãƒ­ãƒƒã‚¯ç™ºè¦‹")
                
                party_candidates = []
                for i, block in enumerate(candidate_blocks):
                    try:
                        # åŸºæœ¬æƒ…å ±æŠ½å‡ºï¼ˆéƒ½é“åºœçœŒã®ä»£ã‚ã‚Šã«æ¯”ä¾‹ä»£è¡¨ï¼‰
                        candidate_info = extract_proportional_candidate(block, party_name, url, i, collector)
                        if candidate_info:
                            party_candidates.append(candidate_info)
                    except Exception as e:
                        logger.debug(f"{party_name} ãƒ–ãƒ­ãƒƒã‚¯{i}ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                all_proportional_candidates.extend(party_candidates)
                logger.info(f"âœ… {party_name}: {len(party_candidates)}å")
                
                if party_candidates:
                    sample = party_candidates[0]
                    logger.info(f"  ã‚µãƒ³ãƒ—ãƒ«: {sample.get('name')} ({sample.get('party')})")
                
                # å…¨æ”¿å…šãƒšãƒ¼ã‚¸ã‹ã‚‰ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒå–ã‚ŒãŸå ´åˆã€ä»–ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if party_id == "all" and len(party_candidates) > 50:
                    logger.info("å…¨æ”¿å…šãƒšãƒ¼ã‚¸ã‹ã‚‰ååˆ†ãªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã€å€‹åˆ¥æ”¿å…šã‚’ã‚¹ã‚­ãƒƒãƒ—")
                    break
                
            except Exception as e:
                logger.error(f"âŒ {party_name}ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        logger.info(f"ğŸ¯ æ¯”ä¾‹ä»£è¡¨åé›†å®Œäº†: {len(all_proportional_candidates)}å")
        
        return all_proportional_candidates
        
    except Exception as e:
        logger.error(f"âŒ æ¯”ä¾‹ä»£è¡¨åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def extract_proportional_candidate(block, party_name, page_url, idx, collector):
    """æ¯”ä¾‹ä»£è¡¨å€™è£œè€…æƒ…å ±æŠ½å‡º"""
    try:
        # åå‰æŠ½å‡º
        name_elem = block.find(class_='p_senkyoku_list_block_text_name')
        if not name_elem:
            # ä»–ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚‚è©¦ã™
            name_elem = block.find(['h2', 'h3', 'h4'], string=re.compile(r'[ä¸€-é¾¯ã‚¡-ãƒ¶]+'))
        
        if name_elem:
            full_name = name_elem.get_text(strip=True)
        else:
            # ãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã‹ã‚‰åå‰ã‚‰ã—ãã‚‚ã®ã‚’æ¢ã™
            import re
            text = block.get_text()
            name_match = re.search(r'([ä¸€-é¾¯]{2,4}[\\sã€€]*[ä¸€-é¾¯]{2,8}[ã‚¡-ãƒ¶]*)', text)
            if name_match:
                full_name = name_match.group(1).strip()
            else:
                return None
        
        # åå‰ã¨ã‚«ã‚¿ã‚«ãƒŠã‚’åˆ†é›¢
        name, name_kana = collector.separate_name_and_kana(full_name)
        
        # æ”¿å…šæŠ½å‡ºï¼ˆãƒšãƒ¼ã‚¸ã‹ã‚‰æ¨æ¸¬ã¾ãŸã¯è¦ç´ ã‹ã‚‰æŠ½å‡ºï¼‰
        party_elem = block.find(class_='p_senkyoku_list_block_text_party')
        if party_elem:
            party = party_elem.get_text(strip=True)
        else:
            # ãƒšãƒ¼ã‚¸ã®æ”¿å…šåã‚’ä½¿ç”¨
            party = party_name if party_name != "å…¨æ”¿å…š" else "æœªåˆ†é¡"
        
        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯æŠ½å‡º
        profile_link = block.find('a', href=re.compile(r'/seijika/\\d+'))
        profile_url = ""
        candidate_id = f"hirei_{party_name}_{idx}"
        
        if profile_link:
            href = profile_link.get('href', '')
            if href.startswith('/'):
                profile_url = f"https://go2senkyo.com{href}"
            else:
                profile_url = href
            
            # å€™è£œè€…IDæŠ½å‡º
            import re
            match = re.search(r'/seijika/(\\d+)', href)
            if match:
                candidate_id = f"hirei_{match.group(1)}"
        
        candidate_data = {
            "candidate_id": candidate_id,
            "name": name,
            "prefecture": "æ¯”ä¾‹ä»£è¡¨",
            "constituency": "å…¨å›½æ¯”ä¾‹",
            "constituency_type": "proportional",
            "party": party,
            "party_normalized": collector.normalize_party_name(party),
            "profile_url": profile_url,
            "source_page": page_url,
            "source": "go2senkyo_proportional",
            "collected_at": datetime.now().isoformat()
        }
        
        # ã‚«ã‚¿ã‚«ãƒŠåå‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
        if name_kana:
            candidate_data["name_kana"] = name_kana
        
        return candidate_data
        
    except Exception as e:
        logger.debug(f"æ¯”ä¾‹ä»£è¡¨å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return None

def merge_with_constituency_data():
    """é¸æŒ™åŒºãƒ‡ãƒ¼ã‚¿ã¨æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ"""
    logger.info("ğŸ”— é¸æŒ™åŒºãƒ‡ãƒ¼ã‚¿ã¨æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ...")
    
    try:
        # æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›†
        proportional_candidates = collect_proportional()
        
        # æ—¢å­˜ã®é¸æŒ™åŒºãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
        latest_file = data_dir / "go2senkyo_optimized_latest.json"
        
        if latest_file.exists():
            with open(latest_file, 'r', encoding='utf-8') as f:
                constituency_data = json.load(f)
                constituency_candidates = constituency_data.get('data', [])
        else:
            constituency_candidates = []
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        all_candidates = constituency_candidates + proportional_candidates
        
        logger.info(f"ğŸ“Š çµ±åˆçµæœ:")
        logger.info(f"  é¸æŒ™åŒºå€™è£œè€…: {len(constituency_candidates)}å")
        logger.info(f"  æ¯”ä¾‹ä»£è¡¨å€™è£œè€…: {len(proportional_candidates)}å")
        logger.info(f"  ç·å€™è£œè€…æ•°: {len(all_candidates)}å")
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
        save_merged_data(all_candidates, data_dir)
        
        return all_candidates
        
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
        raise

def save_merged_data(candidates, output_dir):
    """çµ±åˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    main_file = output_dir / f"go2senkyo_merged_{timestamp}.json"
    latest_file = output_dir / "go2senkyo_optimized_latest.json"
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    constituency_stats = {}
    
    for candidate in candidates:
        party = candidate.get('party', 'ç„¡æ‰€å±')
        constituency_type = candidate.get('constituency_type', 'unknown')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        constituency_stats[constituency_type] = constituency_stats.get(constituency_type, 0) + 1
    
    save_data = {
        "metadata": {
            "data_type": "go2senkyo_merged_sangiin_2025",
            "collection_method": "constituency_and_proportional_scraping",
            "total_candidates": len(candidates),
            "generated_at": datetime.now().isoformat(),
            "source_site": "sangiin.go2senkyo.com",
            "coverage": {
                "constituency_types": len(constituency_stats),
                "parties": len(party_stats)
            },
            "collection_stats": {
                "total_candidates": len(candidates),
                "detailed_profiles": 0,
                "with_photos": 0,
                "with_policies": 0,
                "errors": 0
            },
            "quality_metrics": {
                "detail_coverage": "0%",
                "photo_coverage": "0%",
                "policy_coverage": "0%"
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_constituency_type": constituency_stats
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    with open(main_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {main_file}")

if __name__ == "__main__":
    merge_with_constituency_data()