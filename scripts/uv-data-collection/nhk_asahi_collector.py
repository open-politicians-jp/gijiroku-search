#!/usr/bin/env python3
"""
NHKãƒ»æœæ—¥æ–°èé¸æŒ™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®å€™è£œè€…åé›†
IPå½è£…æ©Ÿèƒ½ä»˜ã
"""

import json
import logging
import requests
import time
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import random

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NHKAsahiCollector:
    def __init__(self):
        self.session = requests.Session()
        
        # æ—¥æœ¬ã®ãƒ—ãƒ­ã‚­ã‚·IPãƒªã‚¹ãƒˆï¼ˆä¾‹ï¼‰- å®Ÿéš›ã¯æœ‰åŠ¹ãªãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨
        self.japanese_proxies = [
            # å®Ÿéš›ã®ãƒ—ãƒ­ã‚­ã‚·ã‚µãƒ¼ãƒ“ã‚¹ã®IPã‚’è¨­å®š
            # {"http": "http://proxy1:port", "https": "https://proxy1:port"},
            # {"http": "http://proxy2:port", "https": "https://proxy2:port"},
        ]
        
        # æ—¥æœ¬ã®User-Agentãƒªã‚¹ãƒˆ
        self.japanese_user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0",
        ]
        
        self.setup_session()
        
        # éƒ½é“åºœçœŒãƒãƒƒãƒ”ãƒ³ã‚°
        self.prefectures = {
            1: "åŒ—æµ·é“", 2: "é’æ£®çœŒ", 3: "å²©æ‰‹çœŒ", 4: "å®®åŸçœŒ", 5: "ç§‹ç”°çœŒ", 6: "å±±å½¢çœŒ", 7: "ç¦å³¶çœŒ",
            8: "èŒ¨åŸçœŒ", 9: "æ ƒæœ¨çœŒ", 10: "ç¾¤é¦¬çœŒ", 11: "åŸ¼ç‰çœŒ", 12: "åƒè‘‰çœŒ", 13: "æ±äº¬éƒ½", 14: "ç¥å¥ˆå·çœŒ",
            15: "æ–°æ½ŸçœŒ", 16: "å¯Œå±±çœŒ", 17: "çŸ³å·çœŒ", 18: "ç¦äº•çœŒ", 19: "å±±æ¢¨çœŒ", 20: "é•·é‡çœŒ", 21: "å²é˜œçœŒ",
            22: "é™å²¡çœŒ", 23: "æ„›çŸ¥çœŒ", 24: "ä¸‰é‡çœŒ", 25: "æ»‹è³€çœŒ", 26: "äº¬éƒ½åºœ", 27: "å¤§é˜ªåºœ", 28: "å…µåº«çœŒ",
            29: "å¥ˆè‰¯çœŒ", 30: "å’Œæ­Œå±±çœŒ", 31: "é³¥å–çœŒ", 32: "å³¶æ ¹çœŒ", 33: "å²¡å±±çœŒ", 34: "åºƒå³¶çœŒ", 35: "å±±å£çœŒ",
            36: "å¾³å³¶çœŒ", 37: "é¦™å·çœŒ", 38: "æ„›åª›çœŒ", 39: "é«˜çŸ¥çœŒ", 40: "ç¦å²¡çœŒ", 41: "ä½è³€çœŒ", 42: "é•·å´çœŒ",
            43: "ç†Šæœ¬çœŒ", 44: "å¤§åˆ†çœŒ", 45: "å®®å´çœŒ", 46: "é¹¿å…å³¶çœŒ", 47: "æ²–ç¸„çœŒ"
        }

    def setup_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šã¨IPå½è£…"""
        # ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentè¨­å®š
        user_agent = random.choice(self.japanese_user_agents)
        
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,ja-JP;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://www.google.co.jp/',
        })
        
        # ãƒ—ãƒ­ã‚­ã‚·è¨­å®šï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if self.japanese_proxies:
            proxy = random.choice(self.japanese_proxies)
            self.session.proxies.update(proxy)
            logger.info(f"ãƒ—ãƒ­ã‚­ã‚·è¨­å®š: {proxy}")

    def collect_nhk_candidates(self):
        """NHKé¸æŒ™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å€™è£œè€…åé›†"""
        logger.info("ğŸ” NHKé¸æŒ™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åé›†é–‹å§‹...")
        
        base_url = "https://www.nhk.or.jp/senkyo/database/sangiin/2025/expected-candidates/"
        all_candidates = []
        
        try:
            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã¸ã‚¢ã‚¯ã‚»ã‚¹
            logger.info("ğŸ“Š NHKãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
            response = self.session.get(base_url, timeout=30)
            
            if response.status_code != 200:
                logger.error(f"âŒ NHKãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            logger.info(f"âœ… NHKãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {len(response.text):,} æ–‡å­—")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # éƒ½é“åºœçœŒãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
            prefecture_links = self.extract_nhk_prefecture_links(soup)
            logger.info(f"ğŸ“‹ NHKéƒ½é“åºœçœŒãƒªãƒ³ã‚¯ç™ºè¦‹: {len(prefecture_links)}ä»¶")
            
            # å„éƒ½é“åºœçœŒã®å€™è£œè€…ã‚’åé›†
            for i, (pref_name, pref_url) in enumerate(prefecture_links):
                logger.info(f"\n=== [{i+1}/{len(prefecture_links)}] {pref_name} NHKåé›† ===")
                
                try:
                    pref_candidates = self.collect_nhk_prefecture(pref_name, pref_url)
                    all_candidates.extend(pref_candidates)
                    logger.info(f"âœ… {pref_name} NHKåé›†å®Œäº†: {len(pref_candidates)}å")
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    time.sleep(random.uniform(2, 4))
                    
                except Exception as e:
                    logger.error(f"âŒ {pref_name} NHKåé›†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            logger.info(f"ğŸ¯ NHKåé›†å®Œäº†: ç·è¨ˆ {len(all_candidates)}å")
            return all_candidates
            
        except Exception as e:
            logger.error(f"âŒ NHKåé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def extract_nhk_prefecture_links(self, soup):
        """NHKãƒšãƒ¼ã‚¸ã‹ã‚‰éƒ½é“åºœçœŒãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        try:
            # NHKã®å…¸å‹çš„ãªæ§‹é€ ã‚’æ¢ç´¢
            # éƒ½é“åºœçœŒé¸æŠã‚¨ãƒªã‚¢ã‚„ãƒªãƒ³ã‚¯ãƒªã‚¹ãƒˆã‚’æ¢ã™
            prefecture_elements = soup.find_all(['a', 'li', 'div'], text=re.compile(r'[éƒ½é“åºœçœŒ]'))
            
            for element in prefecture_elements:
                if element.name == 'a' and element.get('href'):
                    href = element.get('href')
                    text = element.get_text(strip=True)
                    
                    # éƒ½é“åºœçœŒåã®æŠ½å‡º
                    pref_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãª]+[éƒ½é“åºœçœŒ])', text)
                    if pref_match:
                        pref_name = pref_match.group(1)
                        full_url = self.resolve_url(href, "https://www.nhk.or.jp")
                        links.append((pref_name, full_url))
            
            # é‡è¤‡é™¤å»
            unique_links = list(dict(links).items())
            return unique_links
            
        except Exception as e:
            logger.error(f"NHKéƒ½é“åºœçœŒãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def collect_nhk_prefecture(self, pref_name, pref_url):
        """NHKéƒ½é“åºœçœŒãƒšãƒ¼ã‚¸ã‹ã‚‰å€™è£œè€…åé›†"""
        candidates = []
        
        try:
            response = self.session.get(pref_url, timeout=30)
            if response.status_code != 200:
                logger.error(f"âŒ {pref_name} NHKãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å€™è£œè€…æƒ…å ±ã®æŠ½å‡º
            candidate_elements = soup.find_all(['div', 'li', 'tr'], class_=re.compile(r'candidate|koho|person'))
            
            for element in candidate_elements:
                try:
                    candidate = self.extract_nhk_candidate_info(element, pref_name, pref_url)
                    if candidate:
                        candidates.append(candidate)
                except Exception as e:
                    logger.debug(f"å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            return candidates
            
        except Exception as e:
            logger.error(f"{pref_name} NHKåé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def extract_nhk_candidate_info(self, element, pref_name, source_url):
        """NHKå€™è£œè€…è¦ç´ ã‹ã‚‰æƒ…å ±æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": "",
                "name": "",
                "name_kana": "",
                "prefecture": pref_name,
                "constituency": pref_name.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', ''),
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "age": "",
                "profile_url": "",
                "source_page": source_url,
                "source": "nhk_database",
                "collected_at": datetime.now().isoformat()
            }
            
            # åå‰ã®æŠ½å‡º
            name_elem = element.find(['h3', 'h4', 'span', 'div'], class_=re.compile(r'name|åå‰'))
            if name_elem:
                candidate["name"] = name_elem.get_text(strip=True)
            
            # èª­ã¿ã®æŠ½å‡º
            kana_elem = element.find(['span', 'div'], class_=re.compile(r'kana|èª­ã¿|ãµã‚ŠãŒãª'))
            if kana_elem:
                candidate["name_kana"] = kana_elem.get_text(strip=True)
            
            # æ”¿å…šã®æŠ½å‡º
            party_elem = element.find(['span', 'div'], class_=re.compile(r'party|æ”¿å…š|æ‰€å±'))
            if party_elem:
                candidate["party"] = party_elem.get_text(strip=True)
                candidate["party_normalized"] = candidate["party"]
            
            # å¹´é½¢ã®æŠ½å‡º
            age_elem = element.find(['span', 'div'], text=re.compile(r'\d+æ­³'))
            if age_elem:
                age_match = re.search(r'(\d+)æ­³', age_elem.get_text())
                if age_match:
                    candidate["age"] = age_match.group(1)
            
            # å€™è£œè€…IDã®ç”Ÿæˆ
            if candidate["name"]:
                candidate["candidate_id"] = f"nhk_{hash(candidate['name'] + pref_name) % 1000000}"
                return candidate
            
        except Exception as e:
            logger.debug(f"NHKå€™è£œè€…æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return None

    def collect_asahi_candidates(self):
        """æœæ—¥æ–°èé¸æŒ™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å€™è£œè€…åé›†"""
        logger.info("ğŸ” æœæ—¥æ–°èé¸æŒ™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åé›†é–‹å§‹...")
        
        base_url = "https://www.asahi.com/senkyo/saninsen/koho/"
        all_candidates = []
        
        try:
            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã¸ã‚¢ã‚¯ã‚»ã‚¹
            logger.info("ğŸ“Š æœæ—¥æ–°èãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
            response = self.session.get(base_url, timeout=30)
            
            if response.status_code != 200:
                logger.error(f"âŒ æœæ—¥æ–°èãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            logger.info(f"âœ… æœæ—¥æ–°èãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {len(response.text):,} æ–‡å­—")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # éƒ½é“åºœçœŒãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
            prefecture_links = self.extract_asahi_prefecture_links(soup)
            logger.info(f"ğŸ“‹ æœæ—¥æ–°èéƒ½é“åºœçœŒãƒªãƒ³ã‚¯ç™ºè¦‹: {len(prefecture_links)}ä»¶")
            
            # å„éƒ½é“åºœçœŒã®å€™è£œè€…ã‚’åé›†
            for i, (pref_name, pref_url) in enumerate(prefecture_links):
                logger.info(f"\n=== [{i+1}/{len(prefecture_links)}] {pref_name} æœæ—¥æ–°èåé›† ===")
                
                try:
                    pref_candidates = self.collect_asahi_prefecture(pref_name, pref_url)
                    all_candidates.extend(pref_candidates)
                    logger.info(f"âœ… {pref_name} æœæ—¥æ–°èåé›†å®Œäº†: {len(pref_candidates)}å")
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                    time.sleep(random.uniform(2, 4))
                    
                except Exception as e:
                    logger.error(f"âŒ {pref_name} æœæ—¥æ–°èåé›†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            logger.info(f"ğŸ¯ æœæ—¥æ–°èåé›†å®Œäº†: ç·è¨ˆ {len(all_candidates)}å")
            return all_candidates
            
        except Exception as e:
            logger.error(f"âŒ æœæ—¥æ–°èåé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def extract_asahi_prefecture_links(self, soup):
        """æœæ—¥æ–°èãƒšãƒ¼ã‚¸ã‹ã‚‰éƒ½é“åºœçœŒãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        try:
            # æœæ—¥æ–°èã®å…¸å‹çš„ãªæ§‹é€ ã‚’æ¢ç´¢
            prefecture_elements = soup.find_all(['a', 'li', 'div'], text=re.compile(r'[éƒ½é“åºœçœŒ]'))
            
            for element in prefecture_elements:
                if element.name == 'a' and element.get('href'):
                    href = element.get('href')
                    text = element.get_text(strip=True)
                    
                    pref_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãª]+[éƒ½é“åºœçœŒ])', text)
                    if pref_match:
                        pref_name = pref_match.group(1)
                        full_url = self.resolve_url(href, "https://www.asahi.com")
                        links.append((pref_name, full_url))
            
            unique_links = list(dict(links).items())
            return unique_links
            
        except Exception as e:
            logger.error(f"æœæ—¥æ–°èéƒ½é“åºœçœŒãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def collect_asahi_prefecture(self, pref_name, pref_url):
        """æœæ—¥æ–°èéƒ½é“åºœçœŒãƒšãƒ¼ã‚¸ã‹ã‚‰å€™è£œè€…åé›†"""
        candidates = []
        
        try:
            response = self.session.get(pref_url, timeout=30)
            if response.status_code != 200:
                logger.error(f"âŒ {pref_name} æœæ—¥æ–°èãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å€™è£œè€…æƒ…å ±ã®æŠ½å‡º
            candidate_elements = soup.find_all(['div', 'li', 'tr'], class_=re.compile(r'candidate|koho|person'))
            
            for element in candidate_elements:
                try:
                    candidate = self.extract_asahi_candidate_info(element, pref_name, pref_url)
                    if candidate:
                        candidates.append(candidate)
                except Exception as e:
                    logger.debug(f"å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            return candidates
            
        except Exception as e:
            logger.error(f"{pref_name} æœæ—¥æ–°èåé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def extract_asahi_candidate_info(self, element, pref_name, source_url):
        """æœæ—¥æ–°èå€™è£œè€…è¦ç´ ã‹ã‚‰æƒ…å ±æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": "",
                "name": "",
                "name_kana": "",
                "prefecture": pref_name,
                "constituency": pref_name.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', ''),
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "age": "",
                "profile_url": "",
                "source_page": source_url,
                "source": "asahi_database",
                "collected_at": datetime.now().isoformat()
            }
            
            # åå‰ã®æŠ½å‡º
            name_elem = element.find(['h3', 'h4', 'span', 'div'], class_=re.compile(r'name|åå‰'))
            if name_elem:
                candidate["name"] = name_elem.get_text(strip=True)
            
            # èª­ã¿ã®æŠ½å‡º  
            kana_elem = element.find(['span', 'div'], class_=re.compile(r'kana|èª­ã¿|ãµã‚ŠãŒãª'))
            if kana_elem:
                candidate["name_kana"] = kana_elem.get_text(strip=True)
            
            # æ”¿å…šã®æŠ½å‡º
            party_elem = element.find(['span', 'div'], class_=re.compile(r'party|æ”¿å…š|æ‰€å±'))
            if party_elem:
                candidate["party"] = party_elem.get_text(strip=True)
                candidate["party_normalized"] = candidate["party"]
            
            # å¹´é½¢ã®æŠ½å‡º
            age_elem = element.find(['span', 'div'], text=re.compile(r'\d+æ­³'))
            if age_elem:
                age_match = re.search(r'(\d+)æ­³', age_elem.get_text())
                if age_match:
                    candidate["age"] = age_match.group(1)
            
            # å€™è£œè€…IDã®ç”Ÿæˆ
            if candidate["name"]:
                candidate["candidate_id"] = f"asahi_{hash(candidate['name'] + pref_name) % 1000000}"
                return candidate
            
        except Exception as e:
            logger.debug(f"æœæ—¥æ–°èå€™è£œè€…æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return None

    def resolve_url(self, href, base_url):
        """ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›"""
        if href.startswith('http'):
            return href
        elif href.startswith('/'):
            return base_url + href
        else:
            return base_url + '/' + href

    def merge_and_deduplicate(self, nhk_candidates, asahi_candidates):
        """NHKã¨æœæ—¥æ–°èã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆãƒ»é‡è¤‡é™¤å»"""
        logger.info("ğŸ”§ ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»é‡è¤‡é™¤å»é–‹å§‹...")
        
        all_candidates = nhk_candidates + asahi_candidates
        logger.info(f"ğŸ“Š çµ±åˆå‰ç·æ•°: {len(all_candidates)}å")
        
        # é‡è¤‡é™¤å»ï¼ˆåå‰+éƒ½é“åºœçœŒã§åˆ¤å®šï¼‰
        seen = set()
        unique_candidates = []
        
        for candidate in all_candidates:
            key = f"{candidate['name']}_{candidate['prefecture']}"
            if key not in seen:
                seen.add(key)
                unique_candidates.append(candidate)
        
        logger.info(f"ğŸ“Š é‡è¤‡é™¤å»å¾Œ: {len(unique_candidates)}å")
        return unique_candidates

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("ğŸš€ NHKãƒ»æœæ—¥æ–°èé¸æŒ™ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
    
    collector = NHKAsahiCollector()
    
    # NHKãƒ‡ãƒ¼ã‚¿åé›†
    nhk_candidates = collector.collect_nhk_candidates()
    
    # æœæ—¥æ–°èãƒ‡ãƒ¼ã‚¿åé›†
    asahi_candidates = collector.collect_asahi_candidates()
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    all_candidates = collector.merge_and_deduplicate(nhk_candidates, asahi_candidates)
    
    # çµæœä¿å­˜
    save_merged_results(all_candidates, nhk_candidates, asahi_candidates)

def save_merged_results(all_candidates, nhk_candidates, asahi_candidates):
    """çµ±åˆçµæœã®ä¿å­˜"""
    logger.info("ğŸ’¾ çµ±åˆçµæœã®ä¿å­˜...")
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    source_stats = {"nhk_database": 0, "asahi_database": 0}
    
    for candidate in all_candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        source = candidate.get('source', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
        source_stats[source] = source_stats.get(source, 0) + 1
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    data = {
        "metadata": {
            "data_type": "nhk_asahi_merged_sangiin_2025",
            "collection_method": "official_databases_with_ip_masking",
            "total_candidates": len(all_candidates),
            "nhk_candidates": len(nhk_candidates),
            "asahi_candidates": len(asahi_candidates),
            "generated_at": datetime.now().isoformat(),
            "sources": ["NHKé¸æŒ™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "æœæ—¥æ–°èé¸æŒ™ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"],
            "coverage": {
                "constituency_types": 1,
                "parties": len(party_stats),
                "prefectures": len(prefecture_stats)
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": prefecture_stats,
            "by_source": source_stats,
            "by_constituency_type": {"single_member": len(all_candidates)}
        },
        "data": all_candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    merged_file = data_dir / f"nhk_asahi_merged_{timestamp}.json"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(merged_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ãŒæˆåŠŸã—ãŸå ´åˆã®ã¿æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
    if len(all_candidates) > 200:  # åˆç†çš„ãªå€™è£œè€…æ•°ã®é–¾å€¤
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {latest_file}")
    
    logger.info(f"ğŸ“ çµ±åˆçµæœä¿å­˜: {merged_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    logger.info("\nğŸ“Š NHKãƒ»æœæ—¥æ–°èçµ±åˆåé›†çµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(all_candidates)}å")
    logger.info(f"  NHKãƒ‡ãƒ¼ã‚¿: {len(nhk_candidates)}å")
    logger.info(f"  æœæ—¥æ–°èãƒ‡ãƒ¼ã‚¿: {len(asahi_candidates)}å")
    logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
    logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_stats)}éƒ½é“åºœçœŒ")
    
    # ä¸»è¦çœŒã®ç¢ºèª
    major_prefs = ["ç¥å¥ˆå·çœŒ", "äº¬éƒ½åºœ", "ä¸‰é‡çœŒ", "æ±äº¬éƒ½", "å¤§é˜ªåºœ"]
    logger.info(f"\nğŸ” ä¸»è¦çœŒç¢ºèª:")
    for pref in major_prefs:
        count = prefecture_stats.get(pref, 0)
        logger.info(f"  {pref}: {count}å")

if __name__ == "__main__":
    main()