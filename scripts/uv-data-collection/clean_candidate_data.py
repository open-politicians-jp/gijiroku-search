#!/usr/bin/env python3
"""
å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
ä¸é©åˆ‡ãªåå‰ã‚„ãƒã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ã®é™¤å»
"""

import json
import logging
import re
from datetime import datetime
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CandidateDataCleaner:
    def __init__(self):
        # é™¤å¤–ã™ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆå€™è£œè€…åã¨ã—ã¦ä¸é©åˆ‡ï¼‰
        self.invalid_keywords = [
            # çµ„ç¹”ãƒ»å›£ä½“å
            'æ”¯éƒ¨', 'æœ¬éƒ¨', 'äº‹å‹™æ‰€', 'äº‹å‹™å±€', 'å§”å“¡ä¼š', 'å”ä¼š', 'å›£ä½“', 'çµ„ç¹”',
            'å±€', 'éƒ¨', 'èª²', 'å®¤', 'ä¼š', 'å…š', 'æ”¿æ²»', 'é¸æŒ™', 'è­°å“¡',
            
            # ä¸€èˆ¬çš„ãªèªå¥
            'ã«ã¤ã„ã¦', 'ã“ã¡ã‚‰', 'ãƒšãƒ¼ã‚¸', 'ã‚µã‚¤ãƒˆ', 'ãƒ›ãƒ¼ãƒ ', 'ãƒˆãƒƒãƒ—',
            'ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒŠãƒ“', 'ãƒªãƒ³ã‚¯', 'è©³ç´°', 'ä¸€è¦§', 'ãƒªã‚¹ãƒˆ',
            
            # æ”¿æ²»ç”¨èª
            'æ”¿ç­–', 'å…¬ç´„', 'å…¬å ±', 'ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ', 'æŠ•ç¥¨', 'é–‹ç¥¨', 'å½“é¸',
            'è½é¸', 'æ¯”ä¾‹', 'é¸æŒ™åŒº', 'éƒ½é“åºœçœŒ', 'å¸‚åŒºç”ºæ‘',
            
            # ãã®ä»–
            'å¤§å­¦', 'å­¦æ ¡', 'ç ”ç©¶', 'èª¿æŸ»', 'å ±å‘Š', 'ç™ºè¡¨', 'è¨˜è€…',
            'å¥³æ€§', 'é’å¹´', 'å­¦ç”Ÿ', 'å¸‚æ°‘', 'å›½æ°‘', 'çœŒæ°‘', 'éƒ½æ°‘'
        ]
        
        # é©åˆ‡ãªæ—¥æœ¬äººåã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.valid_name_patterns = [
            r'^[ä¸€-é¾¯]{1,3}\s*[ä¸€-é¾¯]{1,3}$',  # æ¼¢å­—ã®ã¿ï¼ˆå§“+åï¼‰
            r'^[ä¸€-é¾¯ã²ã‚‰ãŒãª]{2,10}$',        # æ¼¢å­—ãƒ»ã²ã‚‰ãŒãªæ··åœ¨
            r'^[ã‚¡-ãƒ¶ãƒ¼]{2,10}$',              # ã‚«ã‚¿ã‚«ãƒŠã®ã¿
        ]

    def clean_official_sources_data(self):
        """å…¬å¼ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        logger.info("ğŸ§¹ å…¬å¼ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
        
        # æœ€æ–°ã®å…¬å¼ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        official_files = list(data_dir.glob("official_sources_*.json"))
        if not official_files:
            logger.error("âŒ å…¬å¼ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        latest_file = max(official_files, key=lambda f: f.stat().st_mtime)
        logger.info(f"ğŸ“ å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {latest_file}")
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        original_candidates = data.get('data', [])
        logger.info(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {len(original_candidates)}å")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        cleaned_candidates = self.clean_candidates(original_candidates)
        
        # çµ±è¨ˆå†è¨ˆç®—
        party_stats = {}
        prefecture_stats = {}
        
        for candidate in cleaned_candidates:
            party = candidate.get('party', 'æœªåˆ†é¡')
            prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
            
            party_stats[party] = party_stats.get(party, 0) + 1
            if prefecture != 'æœªåˆ†é¡':
                prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
        
        # ã‚¯ãƒªãƒ¼ãƒ³ç‰ˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        cleaned_data = {
            "metadata": {
                "data_type": "official_sources_cleaned_sangiin_2025",
                "collection_method": "soumu_party_official_sources_cleaned",
                "total_candidates": len(cleaned_candidates),
                "original_candidates": len(original_candidates),
                "removed_candidates": len(original_candidates) - len(cleaned_candidates),
                "generated_at": datetime.now().isoformat(),
                "sources": ["ç·å‹™çœé¸æŒ™éƒ¨", "å„æ”¿å…šå…¬å¼ã‚µã‚¤ãƒˆ"],
                "coverage": {
                    "parties": len(party_stats),
                    "prefectures": len(prefecture_stats) if prefecture_stats else 0
                }
            },
            "statistics": {
                "by_party": party_stats,
                "by_prefecture": prefecture_stats,
                "by_constituency_type": {"single_member": len(cleaned_candidates)}
            },
            "data": cleaned_candidates
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        cleaned_file = data_dir / f"official_sources_cleaned_{timestamp}.json"
        latest_cleaned_file = data_dir / "go2senkyo_optimized_latest.json"
        
        with open(cleaned_file, 'w', encoding='utf-8') as f:
            json.dump(cleaned_data, f, ensure_ascii=False, indent=2)
        
        # å€™è£œè€…æ•°ãŒåˆç†çš„ãªå ´åˆã®ã¿æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        if len(cleaned_candidates) >= 100:  # æœ€ä½100åã®å€™è£œè€…ãŒå¿…è¦
            with open(latest_cleaned_file, 'w', encoding='utf-8') as f:
                json.dump(cleaned_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {latest_cleaned_file}")
        
        logger.info(f"ğŸ“ ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {cleaned_file}")
        
        # çµæœè¡¨ç¤º
        logger.info("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°çµæœ:")
        logger.info(f"  å…ƒãƒ‡ãƒ¼ã‚¿: {len(original_candidates)}å")
        logger.info(f"  ã‚¯ãƒªãƒ¼ãƒ³å¾Œ: {len(cleaned_candidates)}å")
        logger.info(f"  é™¤å»æ•°: {len(original_candidates) - len(cleaned_candidates)}å")
        logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
        logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_stats)}éƒ½é“åºœçœŒ")
        
        for party, count in party_stats.items():
            logger.info(f"    {party}: {count}å")
        
        return cleaned_file

    def clean_candidates(self, candidates):
        """å€™è£œè€…ãƒªã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        cleaned = []
        removed_count = 0
        removal_reasons = {}
        
        for candidate in candidates:
            name = candidate.get('name', '').strip()
            
            # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°åˆ¤å®š
            is_valid, reason = self.is_valid_candidate(name)
            
            if is_valid:
                # åå‰ã®æ­£è¦åŒ–
                candidate['name'] = self.normalize_name(name)
                cleaned.append(candidate)
            else:
                removed_count += 1
                removal_reasons[reason] = removal_reasons.get(reason, 0) + 1
                logger.debug(f"é™¤å»: {name} (ç†ç”±: {reason})")
        
        logger.info(f"ğŸ—‘ï¸ é™¤å»è©³ç´°:")
        for reason, count in removal_reasons.items():
            logger.info(f"  {reason}: {count}ä»¶")
        
        return cleaned

    def is_valid_candidate(self, name):
        """å€™è£œè€…åã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        if not name:
            return False, "ç©ºã®åå‰"
        
        if len(name) < 2 or len(name) > 15:
            return False, "åå‰ã®é•·ã•ãŒä¸é©åˆ‡"
        
        # ç„¡åŠ¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        for keyword in self.invalid_keywords:
            if keyword in name:
                return False, f"ç„¡åŠ¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰: {keyword}"
        
        # æ•°å­—ã‚„è¨˜å·ã®ãƒã‚§ãƒƒã‚¯
        if re.search(r'[0-9]', name):
            return False, "æ•°å­—å«æœ‰"
        
        if re.search(r'[!@#$%^&*()_+=\[\]{}|;:,.<>?]', name):
            return False, "è¨˜å·å«æœ‰"
        
        # URLã‚„è‹±èªã®ãƒã‚§ãƒƒã‚¯
        if re.search(r'[a-zA-Z]', name):
            return False, "è‹±èªå«æœ‰"
        
        if 'http' in name.lower() or 'www' in name.lower():
            return False, "URLå«æœ‰"
        
        # æ—¥æœ¬äººåãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        for pattern in self.valid_name_patterns:
            if re.match(pattern, name):
                return True, "æœ‰åŠ¹ãªåå‰"
        
        # ãã®ä»–ã®æ—¥æœ¬èªæ–‡å­—ãƒã‚§ãƒƒã‚¯
        if re.match(r'^[ä¸€-é¾¯ã²ã‚‰ãŒãªã‚¡-ãƒ¶ãƒ¼\s]+$', name):
            # ç‰¹æ®Šãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ãƒã‚§ãƒƒã‚¯
            if not self.has_suspicious_patterns(name):
                return True, "æœ‰åŠ¹ãªåå‰"
        
        return False, "ç„¡åŠ¹ãªãƒ‘ã‚¿ãƒ¼ãƒ³"

    def has_suspicious_patterns(self, name):
        """ç–‘ã‚ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯"""
        suspicious_patterns = [
            r'.*é‹å–¶.*',
            r'.*ç®¡ç†.*',
            r'.*ä»£è¡¨.*',
            r'.*è²¬ä»»è€….*',
            r'.*æ‹…å½“.*',
            r'.*é€£çµ¡.*',
            r'.*çª“å£.*',
            r'.*ç›¸è«‡.*',
            r'.*å•ã„åˆã‚ã›.*',
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, name):
                return True
        
        return False

    def normalize_name(self, name):
        """åå‰ã®æ­£è¦åŒ–"""
        # ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»
        name = re.sub(r'\s+', ' ', name).strip()
        
        # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
        name = name.replace('ã€€', ' ')
        
        # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«çµ±ä¸€
        name = re.sub(r' +', ' ', name)
        
        return name

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("ğŸ§¹ å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    
    cleaner = CandidateDataCleaner()
    cleaned_file = cleaner.clean_official_sources_data()
    
    if cleaned_file:
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {cleaned_file}")
    else:
        logger.error("âŒ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¤±æ•—")

if __name__ == "__main__":
    main()