#!/usr/bin/env python3
"""
Go2senkyo æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

HTMLæ§‹é€ åˆ†æçµæœã‚’åŸºã«ã€åŠ¹ç‡çš„ã«å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
"""

import json
import requests
import time
import re
import random
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Go2senkyoOptimizedCollector:
    """Go2senkyo æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        
        # URLè¨­å®š (update_headersã§ä½¿ç”¨ã•ã‚Œã‚‹ãŸã‚å…ˆã«è¨­å®š)
        self.base_url = "https://sangiin.go2senkyo.com"
        self.profile_base_url = "https://go2senkyo.com"
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼åˆæœŸåŒ– (base_urlè¨­å®šå¾Œ)
        self.update_headers()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.project_root = Path(__file__).parent.parent.parent
        self.output_dir = self.project_root / "frontend" / "public" / "data" / "sangiin_candidates"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å„ªå…ˆå‡¦ç†ã™ã‚‹ä¸»è¦éƒ½é“åºœçœŒ
        self.priority_prefectures = {
            "æ±äº¬éƒ½": 13, "å¤§é˜ªåºœ": 27, "ç¥å¥ˆå·çœŒ": 14, "æ„›çŸ¥çœŒ": 23, 
            "åŸ¼ç‰çœŒ": 11, "åƒè‘‰çœŒ": 12, "å…µåº«çœŒ": 28, "ç¦å²¡çœŒ": 40
        }
        
        # çµ±è¨ˆ
        self.stats = {
            "total_candidates": 0,
            "detailed_profiles": 0,
            "with_photos": 0,
            "with_policies": 0,
            "errors": 0
        }
    
    def update_headers(self):
        """ãƒ˜ãƒƒãƒ€ãƒ¼æ›´æ–° (ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ãƒ–ãƒ©ã‚¦ã‚¶å„ªå…ˆã€Refererå‰Šé™¤)"""
        # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ãƒ–ãƒ©ã‚¦ã‚¶ User-Agent å¼·åˆ¶ä½¿ç”¨
        desktop_ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        
        self.session.headers.update({
            'User-Agent': desktop_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
            # Refererãƒ˜ãƒƒãƒ€ãƒ¼å‰Šé™¤ - ã‚µãƒ¼ãƒãƒ¼ã§ç•°ãªã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è¿”ã™åŸå› 
        })
    
    def random_delay(self, min_sec=1, max_sec=3):
        """ãƒ©ãƒ³ãƒ€ãƒ é…å»¶"""
        time.sleep(random.uniform(min_sec, max_sec))
    
    def separate_name_and_kana(self, full_name: str) -> tuple:
        """åå‰ã¨ã‚«ã‚¿ã‚«ãƒŠã‚’åˆ†é›¢"""
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ¼¢å­—ã¨ã‚«ã‚¿ã‚«ãƒŠãŒæ··åœ¨ (ä¾‹: "å±±ç”°å¤ªéƒãƒ¤ãƒãƒ€ã‚¿ãƒ­ã‚¦")
            if re.search(r'[ä¸€-é¾¯]+[ã‚¡-ãƒ¶]+', full_name):
                match = re.match(r'([ä¸€-é¾¯\s]+)([ã‚¡-ãƒ¶\s]+)', full_name)
                if match:
                    name = match.group(1).strip()
                    kana = match.group(2).strip()
                    return name, kana
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†é›¢ (ä¾‹: "å±±ç”°å¤ªéƒ ãƒ¤ãƒãƒ€ã‚¿ãƒ­ã‚¦")
            if ' ' in full_name and re.search(r'[ã‚¡-ãƒ¶]', full_name):
                parts = full_name.split()
                if len(parts) >= 2:
                    name_part = parts[0]
                    kana_part = parts[1] if re.search(r'[ã‚¡-ãƒ¶]', parts[1]) else ''
                    if kana_part:
                        return name_part, kana_part
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚«ã‚¿ã‚«ãƒŠã®ã¿ã®å ´åˆã¯åå‰ã¨ã—ã¦æ‰±ã†
            if re.match(r'^[ã‚¡-ãƒ¶\s]+$', full_name):
                return full_name, ''
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³4: æ¼¢å­—ã®ã¿ã¾ãŸã¯æ··åˆã®å ´åˆ
            return full_name, ''
            
        except Exception as e:
            logger.debug(f"åå‰åˆ†é›¢ã‚¨ãƒ©ãƒ¼: {e}")
            return full_name, ''
    
    def collect_priority_prefectures(self) -> List[Dict[str, Any]]:
        """ì£¼ìš” ë„ë„ë¶€í˜„ í›„ë³´ì ìˆ˜ì§‘"""
        logger.info("ğŸš€ ì£¼ìš” ë„ë„ë¶€í˜„ í›„ë³´ì ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        all_candidates = []
        
        for prefecture, code in self.priority_prefectures.items():
            try:
                logger.info(f"ğŸ“ {prefecture} (ì½”ë“œ: {code}) ì²˜ë¦¬ ì¤‘...")
                candidates = self.collect_prefecture_data(prefecture, code)
                
                if candidates:
                    all_candidates.extend(candidates)
                    logger.info(f"âœ… {prefecture}: {len(candidates)}ëª… ìˆ˜ì§‘")
                else:
                    logger.warning(f"âš ï¸ {prefecture}: í›„ë³´ì ì—†ìŒ")
                
                self.random_delay(2, 4)  # ë„ë„ë¶€í˜„ ê°„ ë”œë ˆì´
                
            except Exception as e:
                logger.error(f"âŒ {prefecture} ì—ëŸ¬: {e}")
                self.stats["errors"] += 1
                continue
        
        logger.info(f"ğŸ¯ ì£¼ìš” ë„ë„ë¶€í˜„ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_candidates)}ëª…")
        self.stats["total_candidates"] = len(all_candidates)
        
        return all_candidates
    
    def collect_prefecture_data(self, prefecture: str, code: int) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë„ë„ë¶€í˜„ì˜ í›„ë³´ì ë°ì´í„° ìˆ˜ì§‘"""
        url = f"{self.base_url}/2025/prefecture/{code}"
        
        try:
            self.random_delay()
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                logger.warning(f"{prefecture} í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            candidates = self.parse_candidate_list(soup, prefecture, url)
            
            # í›„ë³´ì ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘
            enhanced_candidates = []
            for candidate in candidates:
                try:
                    enhanced = self.enhance_candidate_profile(candidate)
                    enhanced_candidates.append(enhanced)
                    self.random_delay(1, 2)  # í”„ë¡œí•„ ìˆ˜ì§‘ ê°„ê²©
                    
                except Exception as e:
                    logger.debug(f"í›„ë³´ì {candidate.get('name', 'unknown')} ì„¸ë¶€ì •ë³´ ì—ëŸ¬: {e}")
                    enhanced_candidates.append(candidate)
                    continue
            
            return enhanced_candidates
            
        except Exception as e:
            logger.error(f"{prefecture} ë°ì´í„° ìˆ˜ì§‘ ì—ëŸ¬: {e}")
            return []
    
    def parse_candidate_list(self, soup: BeautifulSoup, prefecture: str, page_url: str) -> List[Dict[str, Any]]:
        """í›„ë³´ì ë¦¬ìŠ¤íŠ¸ íŒŒì‹± (êµ¬ì¡° ë¶„ì„ ê²°ê³¼ í™œìš©)"""
        candidates = []
        
        try:
            # ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì„ íƒì ì‚¬ìš©
            # 1. ì£¼ìš” ë°©ë²•: í›„ë³´ì ë¸”ë¡ì—ì„œ ì¶”ì¶œ
            candidate_blocks = soup.find_all('div', class_='p_senkyoku_list_block')
            logger.info(f"{prefecture}: {len(candidate_blocks)}ê°œ í›„ë³´ì ë¸”ë¡ ë°œê²¬")
            
            for i, block in enumerate(candidate_blocks):
                try:
                    candidate_info = self.extract_candidate_from_block(block, prefecture, page_url, i)
                    if candidate_info:
                        candidates.append(candidate_info)
                except Exception as e:
                    logger.debug(f"ë¸”ë¡ {i} íŒŒì‹± ì—ëŸ¬: {e}")
                    continue
            
            # 2. ë°±ì—…: í”„ë¡œí•„ ë§í¬ì—ì„œ ì§ì ‘ ì¶”ì¶œ
            if not candidates:
                profile_links = soup.find_all('a', href=re.compile(r'/seijika/\d+'))
                logger.debug(f"{prefecture}: {len(profile_links)}ê°œ í”„ë¡œí•„ ë§í¬ ë°œê²¬")
                
                for i, link in enumerate(profile_links):
                    try:
                        candidate_info = self.extract_candidate_from_link(link, prefecture, page_url, i)
                        if candidate_info:
                            candidates.append(candidate_info)
                    except Exception as e:
                        logger.debug(f"ë§í¬ {i} íŒŒì‹± ì—ëŸ¬: {e}")
                        continue
            
            logger.info(f"{prefecture}: {len(candidates)}ëª… ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ")
            
        except Exception as e:
            logger.error(f"{prefecture} í›„ë³´ì ë¦¬ìŠ¤íŠ¸ íŒŒì‹± ì—ëŸ¬: {e}")
        
        return candidates
    
    def extract_candidate_from_link(self, link, prefecture: str, page_url: str, idx: int) -> Optional[Dict[str, Any]]:
        """í”„ë¡œí•„ ë§í¬ì—ì„œ í›„ë³´ì ì •ë³´ ì¶”ì¶œ"""
        try:
            href = link.get('href', '')
            if not href:
                return None
            
            # í”„ë¡œí•„ URL ìƒì„±
            if href.startswith('/'):
                profile_url = urljoin(self.profile_base_url, href)
            else:
                profile_url = href
            
            # í›„ë³´ì ID ì¶”ì¶œ
            match = re.search(r'/seijika/(\d+)', href)
            candidate_id = match.group(1) if match else f"{prefecture}_{idx}"
            
            # ì£¼ë³€ ìš”ì†Œì—ì„œ ì´ë¦„ê³¼ ì •ë‹¹ ì •ë³´ ì°¾ê¸°
            name = ""
            party = ""
            
            # ë§í¬ í…ìŠ¤íŠ¸ í™•ì¸
            link_text = link.get_text(strip=True)
            if link_text and link_text != "è©³ç´°ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«":
                name = link_text
            
            # ë¶€ëª¨/í˜•ì œ ìš”ì†Œì—ì„œ ì´ë¦„ ì°¾ê¸°
            if not name:
                parent = link.parent
                while parent and not name:
                    # ì´ë¦„ íŒ¨í„´ ì°¾ê¸°
                    name_elem = parent.find(class_=re.compile(r'name|candidate'))
                    if name_elem:
                        name_text = name_elem.get_text(strip=True)
                        name_match = re.search(r'([ä¸€-é¾¯]{2,4}[\sã€€]+[ä¸€-é¾¯]{2,8})', name_text)
                        if name_match:
                            name = name_match.group(1).strip().replace('\u3000', ' ')
                            break
                    
                    # ìƒìœ„ ìš”ì†Œë¡œ ì´ë™
                    parent = parent.parent
                    if not parent or parent.name == 'body':
                        break
            
            # ì •ë‹¹ ì •ë³´ ì°¾ê¸°
            if link.parent:
                party_elem = link.parent.find(class_=re.compile(r'party'))
                if party_elem:
                    party = party_elem.get_text(strip=True)
            
            # ê¸°ë³¸ê°’ ì„¤ì •
            if not name:
                name = f"å€™è£œè€…{idx+1}"
            if not party:
                party = "ë¯¸ë¶„ë¥˜"
            
            return {
                "candidate_id": f"go2s_{candidate_id}",
                "name": name,
                "prefecture": prefecture,
                "constituency": prefecture.replace('éƒ½', '').replace('åºœ', '').replace('çœŒ', ''),
                "constituency_type": "single_member",
                "party": party,
                "party_normalized": self.normalize_party_name(party),
                "profile_url": profile_url,
                "source_page": page_url,
                "source": "go2senkyo_optimized",
                "collected_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.debug(f"ë§í¬ ì¶”ì¶œ ì—ëŸ¬: {e}")
            return None
    
    def extract_candidate_from_block(self, block, prefecture: str, page_url: str, idx: int) -> Optional[Dict[str, Any]]:
        """í›„ë³´ì ë¸”ë¡ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            # åå‰æŠ½å‡º
            name_elem = block.find(class_='p_senkyoku_list_block_text_name')
            full_name = name_elem.get_text(strip=True) if name_elem else f"å€™è£œè€…{idx+1}"
            
            # åå‰ã¨ã‚«ã‚¿ã‚«ãƒŠã‚’åˆ†é›¢
            name, name_kana = self.separate_name_and_kana(full_name)
            
            # æ”¿å…šæŠ½å‡º
            party_elem = block.find(class_='p_senkyoku_list_block_text_party')
            party = party_elem.get_text(strip=True) if party_elem else "æœªåˆ†é¡"
            
            # í”„ë¡œí•„ ë§í¬ ì¶”ì¶œ
            profile_link = block.find('a', href=re.compile(r'/seijika/\d+'))
            profile_url = ""
            candidate_id = f"{prefecture}_{idx}"
            
            if profile_link:
                href = profile_link.get('href', '')
                if href.startswith('/'):
                    profile_url = urljoin(self.profile_base_url, href)
                else:
                    profile_url = href
                
                # í›„ë³´ì ID ì¶”ì¶œ
                match = re.search(r'/seijika/(\d+)', href)
                if match:
                    candidate_id = f"go2s_{match.group(1)}"
            
            candidate_data = {
                "candidate_id": candidate_id,
                "name": name,
                "prefecture": prefecture,
                "constituency": prefecture.replace('éƒ½', '').replace('åºœ', '').replace('çœŒ', ''),
                "constituency_type": "single_member",
                "party": party,
                "party_normalized": self.normalize_party_name(party),
                "profile_url": profile_url,
                "source_page": page_url,
                "source": "go2senkyo_optimized",
                "collected_at": datetime.now().isoformat()
            }
            
            # ã‚«ã‚¿ã‚«ãƒŠåå‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
            if name_kana:
                candidate_data["name_kana"] = name_kana
            
            return candidate_data
            
        except Exception as e:
            logger.debug(f"ë¸”ë¡ ì¶”ì¶œ ì—ëŸ¬: {e}")
            return None
    
    def normalize_party_name(self, party: str) -> str:
        """ì •ë‹¹ëª… ì •ê·œí™”"""
        party_mapping = {
            'è‡ªæ°‘': 'è‡ªç”±æ°‘ä¸»å…š', 'ç«‹æ†²': 'ç«‹æ†²æ°‘ä¸»å…š', 'ç¶­æ–°': 'æ—¥æœ¬ç¶­æ–°ã®ä¼š',
            'å…¬æ˜': 'å…¬æ˜å…š', 'å…±ç”£': 'æ—¥æœ¬å…±ç”£å…š', 'å›½æ°‘': 'å›½æ°‘æ°‘ä¸»å…š',
            'ã‚Œã„ã‚': 'ã‚Œã„ã‚æ–°é¸çµ„', 'ç¤¾æ°‘': 'ç¤¾ä¼šæ°‘ä¸»å…š', 
            'Nå›½': 'NHKå…š', 'å‚æ”¿': 'å‚æ”¿å…š', 'ç„¡æ‰€å±': 'ç„¡æ‰€å±'
        }
        
        for key, normalized in party_mapping.items():
            if key in party:
                return normalized
        
        return party if party else "ë¯¸ë¶„ë¥˜"
    
    def enhance_candidate_profile(self, candidate: Dict[str, Any]) -> Dict[str, Any]:
        """í›„ë³´ì í”„ë¡œí•„ ì„¸ë¶€ ì •ë³´ ìˆ˜ì§‘"""
        enhanced = candidate.copy()
        profile_url = candidate.get('profile_url', '')
        
        if not profile_url:
            return enhanced
        
        try:
            logger.debug(f"í”„ë¡œí•„ ìˆ˜ì§‘: {candidate.get('name')} - {profile_url}")
            
            self.random_delay(1, 2)
            response = self.session.get(profile_url, timeout=20)
            
            if response.status_code != 200:
                logger.debug(f"í”„ë¡œí•„ ì ‘ê·¼ ì‹¤íŒ¨: {profile_url}")
                return enhanced
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # í”„ë¡œí•„ ì •ë³´ ì¶”ì¶œ
            profile_details = self.extract_profile_details(soup)
            enhanced.update(profile_details)
            
            # ì •ì±… ì •ë³´ ì¶”ì¶œ
            policy_info = self.extract_policy_info(soup)
            enhanced.update(policy_info)
            
            # ì‚¬ì§„ ì¶”ì¶œ
            photo_info = self.extract_photo_info(soup, profile_url)
            enhanced.update(photo_info)
            
            # SNS ë§í¬ ì¶”ì¶œ
            social_info = self.extract_social_links(soup)
            enhanced.update(social_info)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats["detailed_profiles"] += 1
            if photo_info.get('photo_url'):
                self.stats["with_photos"] += 1
            if policy_info.get('manifesto_summary'):
                self.stats["with_policies"] += 1
            
        except Exception as e:
            logger.debug(f"í”„ë¡œí•„ ì„¸ë¶€ì •ë³´ ì—ëŸ¬ ({candidate.get('name')}): {e}")
        
        return enhanced
    
    def extract_profile_details(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """í”„ë¡œí•„ ì„¸ë¶€ ì •ë³´ ì¶”ì¶œ"""
        details = {}
        
        try:
            # ë‚˜ì´/ìƒë…„ì›”ì¼
            age_patterns = [
                re.compile(r'(\d{1,2})æ­³'),
                re.compile(r'([æ˜­å¹³ä»¤å’Œ]\w*\d+å¹´)'),
                re.compile(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)')
            ]
            
            page_text = soup.get_text()
            for pattern in age_patterns:
                match = pattern.search(page_text)
                if match:
                    details['age_info'] = match.group(1)
                    break
            
            # ê²½ë ¥
            career_keywords = ['çµŒæ­´', 'å­¦æ­´', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'ç•¥æ­´']
            for keyword in career_keywords:
                section = soup.find(string=re.compile(keyword))
                if section and section.parent:
                    career_text = section.parent.get_text(strip=True)
                    if len(career_text) > 50:
                        details['career'] = career_text[:500]
                        break
            
            # ì§ì—…
            title_keywords = ['è·æ¥­', 'ç¾è·', 'è‚©æ›¸']
            for keyword in title_keywords:
                section = soup.find(string=re.compile(keyword))
                if section and section.parent:
                    title_text = section.parent.get_text(strip=True)
                    details['occupation'] = title_text[:200]
                    break
        
        except Exception as e:
            logger.debug(f"í”„ë¡œí•„ ìƒì„¸ ì¶”ì¶œ ì—ëŸ¬: {e}")
        
        return details
    
    def extract_policy_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ì •ì±… ì •ë³´ ì¶”ì¶œ"""
        policy = {}
        
        try:
            policy_keywords = ['æ”¿ç­–', 'ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ', 'å…¬ç´„', 'ä¸»å¼µ', 'å–ã‚Šçµ„ã¿']
            policy_texts = []
            
            for keyword in policy_keywords:
                elements = soup.find_all(string=re.compile(keyword))
                for element in elements:
                    if element.parent:
                        text = element.parent.get_text(strip=True)
                        if len(text) > 30:
                            policy_texts.append(text)
                
                if len(' '.join(policy_texts)) > 300:
                    break
            
            if policy_texts:
                combined = ' '.join(policy_texts)
                sentences = [s.strip() for s in combined.split('ã€‚') if s.strip()]
                policy['manifesto_summary'] = 'ã€‚'.join(sentences[:8])[:800]
        
        except Exception as e:
            logger.debug(f"ì •ì±… ì •ë³´ ì¶”ì¶œ ì—ëŸ¬: {e}")
        
        return policy
    
    def extract_photo_info(self, soup: BeautifulSoup, page_url: str) -> Dict[str, Any]:
        """ì‚¬ì§„ ì •ë³´ ì¶”ì¶œ"""
        photo = {}
        
        try:
            # í”„ë¡œí•„ ì‚¬ì§„ í›„ë³´
            img_candidates = soup.find_all('img')
            
            for img in img_candidates:
                alt = img.get('alt', '').lower()
                src = img.get('src', '')
                
                if (any(kw in alt for kw in ['profile', 'candidate', 'å€™è£œ', 'å†™çœŸ']) or
                    any(kw in src.lower() for kw in ['profile', 'candidate', 'å€™è£œ'])):
                    
                    if src.startswith('/'):
                        photo_url = urljoin(page_url, src)
                    elif src.startswith('http'):
                        photo_url = src
                    else:
                        continue
                    
                    photo['photo_url'] = photo_url
                    photo['photo_alt'] = img.get('alt', '')
                    break
        
        except Exception as e:
            logger.debug(f"ì‚¬ì§„ ì¶”ì¶œ ì—ëŸ¬: {e}")
        
        return photo
    
    def extract_social_links(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """SNS ë§í¬ ì¶”ì¶œ"""
        social = {"sns_accounts": {}}
        
        try:
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href'].lower()
                
                if 'twitter.com' in href or 'x.com' in href:
                    social["sns_accounts"]["twitter"] = link['href']
                elif 'facebook.com' in href:
                    social["sns_accounts"]["facebook"] = link['href']
                elif 'instagram.com' in href:
                    social["sns_accounts"]["instagram"] = link['href']
                elif 'youtube.com' in href:
                    social["sns_accounts"]["youtube"] = link['href']
        
        except Exception as e:
            logger.debug(f"SNS ë§í¬ ì¶”ì¶œ ì—ëŸ¬: {e}")
        
        return social
    
    def save_optimized_data(self, candidates: List[Dict[str, Any]]):
        """ë°ì´í„° ì €ì¥"""
        if not candidates:
            logger.warning("ì €ì¥í•  í›„ë³´ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # íŒŒì¼ ê²½ë¡œ
        main_file = self.output_dir / f"go2senkyo_optimized_{timestamp}.json"
        latest_file = self.output_dir / "go2senkyo_optimized_latest.json"
        
        # í†µê³„ ê³„ì‚°
        party_stats = {}
        pref_stats = {}
        
        for candidate in candidates:
            party = candidate.get('party', 'ë¬´ì†Œì†')
            prefecture = candidate.get('prefecture', 'ë¯¸ë¶„ë¥˜')
            
            party_stats[party] = party_stats.get(party, 0) + 1
            pref_stats[prefecture] = pref_stats.get(prefecture, 0) + 1
        
        # ì €ì¥ ë°ì´í„°
        save_data = {
            "metadata": {
                "data_type": "go2senkyo_optimized_sangiin_2025",
                "collection_method": "structure_based_scraping",
                "total_candidates": len(candidates),
                "generated_at": datetime.now().isoformat(),
                "source_site": "sangiin.go2senkyo.com + go2senkyo.com",
                "collection_stats": self.stats,
                "quality_metrics": {
                    "detail_coverage": f"{(self.stats['detailed_profiles']/max(len(candidates), 1)*100):.1f}%",
                    "photo_coverage": f"{(self.stats['with_photos']/max(len(candidates), 1)*100):.1f}%",
                    "policy_coverage": f"{(self.stats['with_policies']/max(len(candidates), 1)*100):.1f}%"
                }
            },
            "statistics": {
                "by_party": party_stats,
                "by_prefecture": pref_stats
            },
            "data": candidates
        }
        
        # íŒŒì¼ ì €ì¥
        with open(main_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ ìµœì í™” ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  - íŒŒì¼: {main_file}")
        logger.info(f"  - í›„ë³´ì: {len(candidates)}ëª…")
        logger.info(f"  - ì„¸ë¶€ì •ë³´: {self.stats['detailed_profiles']}ëª…")
        logger.info(f"  - ì‚¬ì§„: {self.stats['with_photos']}ëª…")
        logger.info(f"  - ì •ì±…: {self.stats['with_policies']}ëª…")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logger.info("ğŸš€ Go2senkyo ìµœì í™” ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    collector = Go2senkyoOptimizedCollector()
    
    try:
        # ì£¼ìš” ë„ë„ë¶€í˜„ ìˆ˜ì§‘
        candidates = collector.collect_priority_prefectures()
        
        if not candidates:
            logger.error("âŒ í›„ë³´ì ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # ë°ì´í„° ì €ì¥
        collector.save_optimized_data(candidates)
        
        logger.info("âœ¨ Go2senkyo ìµœì í™” ìˆ˜ì§‘ ì™„ë£Œ")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        logger.error(f"âŒ ë©”ì¸ ì—ëŸ¬: {e}")
        raise

if __name__ == "__main__":
    main()