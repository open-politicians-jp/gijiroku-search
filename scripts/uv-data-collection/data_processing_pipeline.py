#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿åŠ å·¥ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä»¥ä¸‹ã®æµã‚Œã§å‡¦ç†ï¼š
1. data/raw/ ã‹ã‚‰ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
2. ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ”¹è¡Œãƒ»ã‚¹ãƒšãƒ¼ã‚¹æ•´ç†ï¼‰
3. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
4. data/processed/ ã«ä¿å­˜
5. å¿…è¦ã«å¿œã˜ã¦ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
"""

import os
import json
import re
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataProcessingPipeline:
    """ãƒ‡ãƒ¼ã‚¿åŠ å·¥ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.raw_dir = self.project_root / "data" / "raw"
        self.processed_dir = self.project_root / "data" / "processed"
        self.frontend_dir = self.project_root / "frontend" / "public" / "data"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for dir_path in [self.processed_dir, self.frontend_dir]:
            for subdir in ["speeches", "manifestos", "analysis"]:
                (dir_path / subdir).mkdir(parents=True, exist_ok=True)
                
    def enhanced_text_cleaning(self, text: str) -> str:
        """
        å¼·åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        
        - å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’å®Œå…¨é™¤å»ã¾ãŸã¯åŠè§’1å€‹ã«çµ±ä¸€
        - é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«çµ±ä¸€
        - éåº¦ãªæ”¹è¡Œã‚’æ•´ç†
        - æ—¥æœ¬èªæ–‡ã®è‡ªç„¶ãªæ”¹è¡Œã¯ä¿æŒ
        - ã‚¿ãƒ–æ–‡å­—ã®é™¤å»
        - è­°äº‹éŒ²ç‰¹æœ‰ã®ç½«ç·šãƒ»è¨˜å·ã®æ•´ç†
        """
        if not text:
            return text
            
        # è­°äº‹éŒ²ç‰¹æœ‰ã®ç½«ç·šæ–‡å­—ã‚’å‰Šé™¤ã¾ãŸã¯ç°¡ç•¥åŒ–
        text = re.sub(r'â”€{3,}', '---', text)  # é•·ã„ç½«ç·šã‚’çŸ­ã
        text = re.sub(r'â€¦{3,}', '...', text)  # é•·ã„ç‚¹ç·šã‚’çŸ­ã
        
        # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã®å‡¦ç†ï¼š
        # 1. é€£ç¶šã™ã‚‹å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã®åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
        text = re.sub(r'ã€€+', ' ', text)
        
        # 2. æ—¥æœ¬èªæ–‡å­—é–“ã®å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã¯é™¤å»ï¼ˆåå‰ã®é–“ãªã©ï¼‰
        text = re.sub(r'([ã‚-ã‚“ä¸€-é¾¯])\s+([ã‚-ã‚“ä¸€-é¾¯])', r'\1\2', text)
        
        # ã‚¿ãƒ–æ–‡å­—ã‚’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
        text = text.replace('\t', ' ')
        
        # é€£ç¶šã™ã‚‹åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«çµ±ä¸€
        text = re.sub(r'[ ]+', ' ', text)
        
        # è¡Œé ­ãƒ»è¡Œæœ«ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤
        lines = []
        for line in text.split('\n'):
            # å„è¡Œã®å‰å¾Œç©ºç™½é™¤å»
            cleaned_line = line.strip()
            # è¡Œå†…ã®éåº¦ãªã‚¹ãƒšãƒ¼ã‚¹ã‚‚æ•´ç†
            cleaned_line = re.sub(r'\s+', ' ', cleaned_line)
            lines.append(cleaned_line)
        
        # ç©ºè¡Œã®æ•´ç†
        cleaned_lines = []
        prev_empty = False
        for line in lines:
            if line == '':
                # é€£ç¶šã™ã‚‹ç©ºè¡Œã¯1ã¤ã ã‘ä¿æŒ
                if not prev_empty:
                    cleaned_lines.append(line)
                prev_empty = True
            else:
                cleaned_lines.append(line)
                prev_empty = False
        
        # æœ€çµ‚çµæœã®çµ„ã¿ç«‹ã¦
        result = '\n'.join(cleaned_lines).strip()
        
        # æœ€çµ‚çš„ãªç´°ã‹ã„èª¿æ•´
        # æ”¹è¡Œå‰å¾Œã®ä¸è¦ãªç©ºç™½é™¤å»
        result = re.sub(r'\s*\n\s*', '\n', result)
        
        # è¨˜å·ã¨æ–‡å­—ã®é–“ã®éåº¦ãªç©ºç™½èª¿æ•´
        result = re.sub(r'([ã€‚ã€])\s+', r'\1', result)  # å¥èª­ç‚¹å¾Œã®ä¸è¦ç©ºç™½
        result = re.sub(r'\s+([ã€‚ã€])', r'\1', result)  # å¥èª­ç‚¹å‰ã®ä¸è¦ç©ºç™½
        
        # æ‹¬å¼§ã¨å†…å®¹ã®é–“ã®ç©ºç™½èª¿æ•´
        result = re.sub(r'(\()\s+', r'\1', result)
        result = re.sub(r'\s+(\))', r'\1', result)
        result = re.sub(r'(ï¼ˆ)\s+', r'\1', result)
        result = re.sub(r'\s+(ï¼‰)', r'\1', result)
        
        return result
        
    def process_speech_data(self, speech_data: Dict[str, Any]) -> Dict[str, Any]:
        """å€‹åˆ¥ã®ç™ºè¨€ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
        processed_data = speech_data.copy()
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if 'text' in processed_data and processed_data['text']:
            processed_data['text'] = self.enhanced_text_cleaning(processed_data['text'])
            
        # ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚å‡¦ç†
        text_fields = ['speaker', 'committee', 'party', 'position']
        for field in text_fields:
            if field in processed_data and processed_data[field]:
                processed_data[field] = self.enhanced_text_cleaning(processed_data[field])
        
        return processed_data
        
    def process_raw_file(self, raw_file_path: Path) -> Optional[Dict[str, Any]]:
        """ç”Ÿãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
        logger.info(f"ğŸ“ å‡¦ç†ä¸­: {raw_file_path.name}")
        
        try:
            with open(raw_file_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
                
            if 'data' not in raw_data:
                logger.warning(f"âš ï¸ 'data' ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {raw_file_path.name}")
                return None
                
            speeches = raw_data['data']
            original_count = len(speeches)
            
            # å„ç™ºè¨€ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
            processed_speeches = []
            for speech in speeches:
                processed_speech = self.process_speech_data(speech)
                processed_speeches.append(processed_speech)
                
            # å‡¦ç†æ¸ˆã¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            processed_data = {
                "metadata": {
                    "data_type": "speeches_processed",
                    "total_count": len(processed_speeches),
                    "generated_at": datetime.now().isoformat(),
                    "source": "https://kokkai.ndl.go.jp/api.html",
                    "processed_from": str(raw_file_path),
                    "processing_method": "enhanced_text_cleaning_pipeline",
                    "original_count": original_count
                },
                "data": processed_speeches
            }
            
            logger.info(f"âœ… å‡¦ç†å®Œäº†: {original_count}ä»¶")
            return processed_data
            
        except Exception as e:
            logger.error(f"âŒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {raw_file_path.name} - {e}")
            return None
            
    def save_processed_data(self, processed_data: Dict[str, Any], output_path: Path):
        """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
        file_size = output_path.stat().st_size / (1024 * 1024)
        logger.info(f"ğŸ’¾ ä¿å­˜: {output_path.name} ({file_size:.1f} MB)")
        
    def process_speeches(self) -> List[Path]:
        """è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
        logger.info("ğŸ¯ è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹...")
        
        speeches_raw_dir = self.raw_dir / "speeches"
        speeches_processed_dir = self.processed_dir / "speeches"
        
        if not speeches_raw_dir.exists():
            logger.warning(f"âš ï¸ ç”Ÿãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {speeches_raw_dir}")
            return []
            
        raw_files = list(speeches_raw_dir.glob("speeches_*.json"))
        if not raw_files:
            logger.warning(f"âš ï¸ å‡¦ç†å¯¾è±¡ã®ç”Ÿãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return []
            
        logger.info(f"ğŸ“Š å‡¦ç†å¯¾è±¡: {len(raw_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        processed_files = []
        for raw_file in sorted(raw_files):
            # æ—¢å­˜ã®å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
            processed_filename = f"processed_{raw_file.name}"
            processed_filepath = speeches_processed_dir / processed_filename
            
            process_all = os.getenv('PROCESS_ALL', 'false').lower() == 'true'
            
            if processed_filepath.exists() and not process_all:
                logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰: {processed_filename}")
                processed_files.append(processed_filepath)
                continue
                
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            processed_data = self.process_raw_file(raw_file)
            if processed_data:
                self.save_processed_data(processed_data, processed_filepath)
                processed_files.append(processed_filepath)
                
        logger.info(f"ğŸ‰ è­°äº‹éŒ²å‡¦ç†å®Œäº†: {len(processed_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        return processed_files
        
    def create_unified_dataset(self, processed_files: List[Path]) -> Optional[Path]:
        """çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        if not processed_files:
            logger.warning("âš ï¸ çµ±åˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return None
            
        logger.info("ğŸ”— çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
        
        all_speeches = []
        total_files = 0
        
        for processed_file in processed_files:
            try:
                with open(processed_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                if 'data' in data:
                    all_speeches.extend(data['data'])
                    total_files += 1
                    
            except Exception as e:
                logger.error(f"âŒ çµ±åˆã‚¨ãƒ©ãƒ¼: {processed_file.name} - {e}")
                
        if not all_speeches:
            logger.warning("âš ï¸ çµ±åˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return None
            
        # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
        all_speeches.sort(key=lambda x: x.get('date', ''), reverse=True)
        
        # ç¾åœ¨ã®å¹´æœˆã‚’å–å¾—
        current_date = datetime.now()
        year_month = current_date.strftime("%Y%m")
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        unified_data = {
            "metadata": {
                "data_type": "speeches_unified_processed",
                "total_count": len(all_speeches),
                "generated_at": current_date.isoformat(),
                "source": "https://kokkai.ndl.go.jp/api.html",
                "processing_method": "enhanced_text_cleaning_pipeline",
                "source_files": total_files,
                "filename_format": f"speeches_unified_{year_month}.json"
            },
            "data": all_speeches
        }
        
        # ä¿å­˜
        unified_filename = f"speeches_unified_{year_month}.json"
        unified_filepath = self.processed_dir / "speeches" / unified_filename
        
        self.save_processed_data(unified_data, unified_filepath)
        
        logger.info(f"ğŸ‰ çµ±åˆå®Œäº†: {len(all_speeches)}ä»¶ -> {unified_filename}")
        return unified_filepath
        
    def deploy_to_frontend(self, unified_file: Optional[Path]):
        """ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«é…ç½®"""
        if not unified_file or not unified_file.exists():
            logger.warning("âš ï¸ é…ç½®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return
            
        logger.info("ğŸš€ ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«é…ç½®ä¸­...")
        
        # ãƒ¡ã‚¤ãƒ³ã®çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
        frontend_main_file = self.frontend_dir / "speeches" / "speeches_latest.json"
        shutil.copy2(unified_file, frontend_main_file)
        logger.info(f"ğŸ“ ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®: {frontend_main_file.name}")
        
        # ä¸‹ä½äº’æ›æ€§ã®ãŸã‚ã€æ—§ãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚‚ã‚³ãƒ”ãƒ¼
        frontend_compat_file = self.frontend_dir / "speeches_processed.json"
        shutil.copy2(unified_file, frontend_compat_file)
        logger.info(f"ğŸ“ äº’æ›ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®: {frontend_compat_file.name}")
        
        logger.info("âœ… ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰é…ç½®å®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("ğŸš€ ãƒ‡ãƒ¼ã‚¿åŠ å·¥ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹...")
    
    pipeline = DataProcessingPipeline()
    
    # 1. è­°äº‹éŒ²ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    processed_files = pipeline.process_speeches()
    
    # 2. çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    unified_file = pipeline.create_unified_dataset(processed_files)
    
    # 3. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«é…ç½®
    pipeline.deploy_to_frontend(unified_file)
    
    logger.info("âœ¨ ãƒ‡ãƒ¼ã‚¿åŠ å·¥ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")

if __name__ == "__main__":
    main()