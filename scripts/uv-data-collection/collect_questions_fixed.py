#!/usr/bin/env python3
"""
è³ªå•ä¸»æ„æ›¸åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ­£å¼ç‰ˆï¼‰

è¡†è­°é™¢è³ªå•ä¸»æ„æ›¸ãƒšãƒ¼ã‚¸ã‹ã‚‰å®Ÿéš›ã®è³ªå•ç­”å¼æƒ…å ±ã‚’åé›†
https://www.shugiin.go.jp/internet/itdb_shitsumon.nsf/html/shitsumon/menu_m.htm
æå‡ºè€…ã€è³ªå•ä»¶åã€HTML/PDFãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import json
import requests
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging
import random
from urllib.parse import urljoin

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class QuestionsCollector:
    """è³ªå•ä¸»æ„æ›¸åé›†ã‚¯ãƒ©ã‚¹ï¼ˆæ­£å¼ç‰ˆï¼‰"""
    
    def __init__(self, start_date: Optional[str] = None, end_date: Optional[str] = None):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        
        # æ—¥ä»˜ç¯„å›²è¨­å®š
        self.start_date = start_date or (datetime.now() - timedelta(days=60)).strftime("%Y-%m-%d")
        self.end_date = end_date or datetime.now().strftime("%Y-%m-%d")
        logger.info(f"åé›†æœŸé–“: {self.start_date} ã‹ã‚‰ {self.end_date}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.questions_dir = self.project_root / "data" / "processed" / "questions"
        self.frontend_questions_dir = self.project_root / "frontend" / "public" / "data" / "questions"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.questions_dir.mkdir(parents=True, exist_ok=True)
        self.frontend_questions_dir.mkdir(parents=True, exist_ok=True)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        self.existing_questions = self.load_existing_questions()
        
        # é€±æ¬¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        current_date = datetime.now()
        self.year = current_date.year
        self.week = current_date.isocalendar()[1]
        
        # åŸºæœ¬URLï¼ˆæ­£å¼ç‰ˆï¼‰
        self.base_url = "https://www.shugiin.go.jp"
        self.questions_main_url = "https://www.shugiin.go.jp/internet/itdb_shitsumon.nsf/html/shitsumon/menu_m.htm"
        
    def update_headers(self):
        """User-Agentæ›´æ–°ã¨IPå½è£…"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
    
    def random_delay(self, min_seconds=1, max_seconds=3):
        """ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
    
    def load_existing_questions(self) -> set:
        """æ—¢å­˜ã®è³ªå•ä¸»æ„æ›¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è³ªå•ç•ªå·ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
        existing_ids = set()
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        latest_file = self.frontend_questions_dir / "questions_latest.json"
        if latest_file.exists():
            try:
                with open(latest_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if 'data' in data:
                        for question in data['data']:
                            # è³ªå•ç•ªå·ã‚„è³ªå•ä»¶åã‚’ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨
                            if 'question_number' in question:
                                existing_ids.add(question['question_number'])
                            elif 'title' in question:
                                existing_ids.add(question['title'])
                logger.info(f"æ—¢å­˜è³ªå•ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(existing_ids)}ä»¶")
            except Exception as e:
                logger.warning(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚‚èª­ã¿è¾¼ã¿
        for file_path in self.questions_dir.glob("questions_*.json"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if 'data' in data:
                        for question in data['data']:
                            if 'question_number' in question:
                                existing_ids.add(question['question_number'])
                            elif 'title' in question:
                                existing_ids.add(question['title'])
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        logger.info(f"æ—¢å­˜è³ªå•ç·æ•°: {len(existing_ids)}ä»¶")
        return existing_ids
    
    def is_duplicate_question(self, question_data: Dict[str, Any]) -> bool:
        """è³ªå•ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
        question_id = question_data.get('question_number') or question_data.get('title')
        if question_id and question_id in self.existing_questions:
            return True
        return False
    
    def is_within_date_range(self, question_date: str) -> bool:
        """è³ªå•æ—¥ä»˜ãŒæŒ‡å®šç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            question_dt = datetime.strptime(question_date, "%Y-%m-%d")
            start_dt = datetime.strptime(self.start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(self.end_date, "%Y-%m-%d")
            return start_dt <= question_dt <= end_dt
        except (ValueError, TypeError):
            # æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯åé›†å¯¾è±¡ã¨ã™ã‚‹
            return True
    
    def collect_questions(self) -> List[Dict[str, Any]]:
        """è³ªå•ä¸»æ„æ›¸åé›†ï¼ˆãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ï¼‰"""
        logger.info("è³ªå•ä¸»æ„æ›¸åé›†é–‹å§‹...")
        questions = []
        
        try:
            # IPå½è£…ã®ãŸã‚ãƒ˜ãƒƒãƒ€ãƒ¼æ›´æ–°
            self.update_headers()
            self.random_delay()
            
            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸å–å¾—
            response = self.session.get(self.questions_main_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            logger.info(f"ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {self.questions_main_url}")
            
            # è³ªå•ä¸»æ„æ›¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            question_links = self.extract_question_links(soup)
            logger.info(f"ç™ºè¦‹ã—ãŸè³ªå•ä¸»æ„æ›¸ãƒªãƒ³ã‚¯æ•°: {len(question_links)}")
            
            # å„è³ªå•ä¸»æ„æ›¸ã®è©³ç´°ã‚’å–å¾—
            for idx, link_info in enumerate(question_links):
                try:
                    self.random_delay()  # IPå½è£…ã®ãŸã‚ã®é…å»¶
                    self.update_headers()  # User-Agentæ›´æ–°
                    
                    question_detail = self.extract_question_detail(link_info)
                    if question_detail:
                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if self.is_duplicate_question(question_detail):
                            logger.info(f"é‡è¤‡è³ªå•ã‚’ã‚¹ã‚­ãƒƒãƒ— ({idx+1}/{len(question_links)}): {question_detail['title'][:50]}...")
                            continue
                        
                        # æ—¥ä»˜ç¯„å›²ãƒã‚§ãƒƒã‚¯
                        if 'date' in question_detail and not self.is_within_date_range(question_detail['date']):
                            logger.info(f"ç¯„å›²å¤–è³ªå•ã‚’ã‚¹ã‚­ãƒƒãƒ— ({idx+1}/{len(question_links)}): {question_detail['date']} - {question_detail['title'][:50]}...")
                            continue
                        
                        questions.append(question_detail)
                        logger.info(f"è³ªå•è©³ç´°å–å¾—æˆåŠŸ ({idx+1}/{len(question_links)}): {question_detail['title'][:50]}...")
                    
                except Exception as e:
                    logger.error(f"è³ªå•è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ ({idx+1}): {str(e)}")
                    continue
            
            logger.info(f"è³ªå•ä¸»æ„æ›¸åé›†å®Œäº†: {len(questions)}ä»¶")
            return questions
            
        except Exception as e:
            logger.error(f"è³ªå•ä¸»æ„æ›¸åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def extract_question_links(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰è³ªå•ä¸»æ„æ›¸ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        try:
            # è³ªå•ä¸»æ„æ›¸ã®ãƒªãƒ³ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
            link_elements = soup.find_all('a', href=True)
            
            for link in link_elements:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # è³ªå•ä¸»æ„æ›¸é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’åˆ¤å®š
                if self.is_question_link(href, text):
                    full_url = urljoin(self.questions_main_url, href)
                    links.append({
                        'url': full_url,
                        'title': text,
                        'question_number': self.extract_question_number(text)
                    })
            
            # é‡è¤‡é™¤å»
            unique_links = []
            seen_urls = set()
            for link in links:
                if link['url'] not in seen_urls:
                    unique_links.append(link)
                    seen_urls.add(link['url'])
            
            return unique_links
            
        except Exception as e:
            logger.error(f"è³ªå•ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def is_question_link(self, href: str, text: str) -> bool:
        """è³ªå•ä¸»æ„æ›¸ãƒªãƒ³ã‚¯ã‹ã©ã†ã‹åˆ¤å®š"""
        # è³ªå•ä¸»æ„æ›¸é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        question_keywords = [
            'è³ªå•ä¸»æ„æ›¸', 'è³ªå•', 'question', 'shitsumon',
            'ç¬¬', 'å·', 'ç­”å¼æ›¸'
        ]
        
        # URL ãƒ‘ã‚¿ãƒ¼ãƒ³
        url_patterns = [
            'itdb_shitsumon', 'shitsumon', 'question',
            'syuisyo', 'toushin'
        ]
        
        # ãƒ†ã‚­ã‚¹ãƒˆã«è³ªå•ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        text_match = any(keyword in text for keyword in question_keywords)
        
        # URLã«é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        url_match = any(pattern in href.lower() for pattern in url_patterns)
        
        return text_match or url_match
    
    def extract_question_number(self, text: str) -> str:
        """è³ªå•ç•ªå·ã‚’æŠ½å‡º"""
        # ã€Œç¬¬ã€‡å·ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        number_match = re.search(r'ç¬¬(\d+)å·', text)
        if number_match:
            return number_match.group(1)
        
        # æ•°å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        digit_match = re.search(r'(\d+)', text)
        if digit_match:
            return digit_match.group(1)
        
        return ""
    
    def extract_question_detail(self, link_info: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """è³ªå•è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            response = self.session.get(link_info['url'], timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è³ªå•å†…å®¹ã‚’è§£æ
            question_content = self.extract_question_content(soup)
            answer_content = self.extract_answer_content(soup)
            questioner = self.extract_questioner(soup)
            submission_date = self.extract_submission_date(soup)
            answer_date = self.extract_answer_date(soup)
            
            # HTML/PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
            html_links = self.extract_html_links(soup, link_info['url'])
            pdf_links = self.extract_pdf_links(soup, link_info['url'])
            
            question_detail = {
                'title': link_info['title'],
                'question_number': link_info['question_number'],
                'url': link_info['url'],
                'questioner': questioner,
                'submission_date': submission_date,
                'answer_date': answer_date,
                'question_content': question_content,
                'answer_content': answer_content,
                'html_links': html_links,
                'pdf_links': pdf_links,
                'house': 'è¡†è­°é™¢',
                'category': self.classify_question_category(link_info['title'] + " " + question_content),
                'collected_at': datetime.now().isoformat(),
                'year': self.year,
                'week': self.week
            }
            
            return question_detail
            
        except Exception as e:
            logger.error(f"è³ªå•è©³ç´°æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({link_info['url']}): {str(e)}")
            return None
    
    def extract_question_content(self, soup: BeautifulSoup) -> str:
        """è³ªå•å†…å®¹ã‚’æŠ½å‡º"""
        try:
            # è³ªå•å†…å®¹ã‚’æ¢ã™ãƒ‘ã‚¿ãƒ¼ãƒ³
            content_selectors = [
                'div:contains("è³ªå•")',
                'td:contains("è³ªå•")',
                'p:contains("è³ªå•")',
                'div.question',
                'div.content'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text(strip=True)
                    if len(text) > 50:  # çŸ­ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯é™¤å¤–
                        return self.clean_text(text)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è³ªå•éƒ¨åˆ†ã‚’æŠ½å‡º
            full_text = soup.get_text()
            question_start = full_text.find('è³ªå•')
            if question_start != -1:
                question_text = full_text[question_start:question_start+500]  # 500æ–‡å­—ã¾ã§
                return self.clean_text(question_text)
            
            return ""
            
        except Exception as e:
            logger.error(f"è³ªå•å†…å®¹æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_answer_content(self, soup: BeautifulSoup) -> str:
        """ç­”å¼å†…å®¹ã‚’æŠ½å‡º"""
        try:
            # ç­”å¼å†…å®¹ã‚’æ¢ã™ãƒ‘ã‚¿ãƒ¼ãƒ³
            answer_keywords = ['ç­”å¼', 'å›ç­”', 'æ”¿åºœç­”å¼æ›¸']
            
            for keyword in answer_keywords:
                elements = soup.find_all(text=re.compile(keyword))
                for elem in elements:
                    parent = elem.parent
                    if parent:
                        text = parent.get_text(strip=True)
                        if len(text) > 50:
                            return self.clean_text(text)
            
            return ""
            
        except Exception as e:
            logger.error(f"ç­”å¼å†…å®¹æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_questioner(self, soup: BeautifulSoup) -> str:
        """è³ªå•è€…ã‚’æŠ½å‡º"""
        try:
            # è³ªå•è€…åã‚’æ¢ã™ãƒ‘ã‚¿ãƒ¼ãƒ³
            questioner_patterns = [
                r'æå‡ºè€…[ï¼š:]\s*([^\s\n]+)',
                r'è³ªå•è€…[ï¼š:]\s*([^\s\n]+)',
                r'([^\s\n]+)å›æå‡º'
            ]
            
            page_text = soup.get_text()
            
            for pattern in questioner_patterns:
                match = re.search(pattern, page_text)
                if match:
                    return match.group(1).strip()
            
            return ""
            
        except Exception as e:
            logger.error(f"è³ªå•è€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_submission_date(self, soup: BeautifulSoup) -> str:
        """æå‡ºæ—¥ã‚’æŠ½å‡º"""
        try:
            date_patterns = [
                r'æå‡ºæ—¥[ï¼š:]\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥æå‡º'
            ]
            
            page_text = soup.get_text()
            
            for pattern in date_patterns:
                match = re.search(pattern, page_text)
                if match:
                    year, month, day = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            return ""
            
        except Exception as e:
            logger.error(f"æå‡ºæ—¥æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_answer_date(self, soup: BeautifulSoup) -> str:
        """ç­”å¼æ—¥ã‚’æŠ½å‡º"""
        try:
            date_patterns = [
                r'ç­”å¼æ—¥[ï¼š:]\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥ç­”å¼'
            ]
            
            page_text = soup.get_text()
            
            for pattern in date_patterns:
                match = re.search(pattern, page_text)
                if match:
                    year, month, day = match.groups()
                    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            return ""
            
        except Exception as e:
            logger.error(f"ç­”å¼æ—¥æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_html_links(self, soup: BeautifulSoup, base_url: str = None) -> List[Dict[str, str]]:
        """HTMLãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        try:
            html_links = soup.find_all('a', href=re.compile(r'\.html?$'))
            
            for link in html_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if href:
                    full_url = urljoin(base_url or self.base_url, href)
                    links.append({
                        'url': full_url,
                        'title': text or 'HTMLæ–‡æ›¸'
                    })
            
        except Exception as e:
            logger.error(f"HTMLãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return links
    
    def extract_pdf_links(self, soup: BeautifulSoup, base_url: str = None) -> List[Dict[str, str]]:
        """PDFãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        try:
            pdf_links = soup.find_all('a', href=re.compile(r'\.pdf$'))
            
            for link in pdf_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if href:
                    full_url = urljoin(base_url or self.base_url, href)
                    links.append({
                        'url': full_url,
                        'title': text or 'PDFæ–‡æ›¸'
                    })
            
        except Exception as e:
            logger.error(f"PDFãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return links
    
    def classify_question_category(self, text: str) -> str:
        """è³ªå•ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é¡"""
        text_lower = text.lower()
        
        categories = {
            'å¤–äº¤': ['å¤–äº¤', 'å¤–å‹™', 'å›½éš›', 'å®‰ä¿', 'å®‰å…¨ä¿éšœ', 'æ¡ç´„'],
            'å†…æ”¿': ['å†…æ”¿', 'è¡Œæ”¿', 'æ”¿åºœ', 'çœåº', 'å…¬å‹™å“¡'],
            'çµŒæ¸ˆ': ['çµŒæ¸ˆ', 'è²¡æ”¿', 'äºˆç®—', 'ç¨åˆ¶', 'é‡‘è', 'ç”£æ¥­'],
            'ç¤¾ä¼šä¿éšœ': ['å¹´é‡‘', 'åŒ»ç™‚', 'ä»‹è­·', 'ç¦ç¥‰', 'ç¤¾ä¼šä¿éšœ'],
            'æ•™è‚²': ['æ•™è‚²', 'å­¦æ ¡', 'å¤§å­¦', 'æ–‡éƒ¨ç§‘å­¦'],
            'ç’°å¢ƒ': ['ç’°å¢ƒ', 'æ°—å€™', 'ã‚¨ãƒãƒ«ã‚®ãƒ¼', 'åŸå­åŠ›'],
            'åŠ´åƒ': ['åŠ´åƒ', 'é›‡ç”¨', 'åƒãæ–¹', 'è³ƒé‡‘'],
            'é˜²è¡›': ['é˜²è¡›', 'è‡ªè¡›éšŠ', 'è»äº‹'],
            'å¸æ³•': ['å¸æ³•', 'è£åˆ¤', 'æ³•å‹™', 'åˆ‘äº‹'],
            'åœ°æ–¹': ['åœ°æ–¹', 'è‡ªæ²»ä½“', 'éƒ½é“åºœçœŒ', 'å¸‚ç”ºæ‘']
        }
        
        for category, keywords in categories.items():
            if any(keyword in text_lower for keyword in keywords):
                return category
        
        return 'ä¸€èˆ¬'
    
    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢"""
        if not text:
            return ""
        
        # æ”¹è¡Œãƒ»ã‚¹ãƒšãƒ¼ã‚¹ã®æ­£è¦åŒ–
        text = re.sub(r'\n\s*\n', '\n\n', text)  # é€£ç¶šç©ºè¡Œã‚’2è¡Œã¾ã§
        text = re.sub(r'[ \t]+', ' ', text)  # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
        text = re.sub(r'[\u3000]+', ' ', text)  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã«
        
        return text.strip()
    
    def save_questions_data(self, questions: List[Dict[str, Any]]):
        """è³ªå•ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if not questions:
            logger.warning("ä¿å­˜ã™ã‚‹è³ªå•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã‚’åŸºæº–ã¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆç¾åœ¨ã®å¹´æœˆ + æ™‚åˆ»ï¼‰
        current_date = datetime.now()
        data_period = current_date.strftime('%Y%m%d')  # å½“æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
        timestamp = current_date.strftime('%H%M%S')
        
        # çµ±ä¸€ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        data_structure = {
            "metadata": {
                "data_type": "shugiin_questions",
                "collection_method": "incremental_scraping",
                "total_questions": len(questions),
                "generated_at": current_date.isoformat(),
                "source_site": "www.shugiin.go.jp",
                "collection_period": {
                    "start_date": self.start_date,
                    "end_date": self.end_date
                }
            },
            "data": questions
        }
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜
        raw_filename = f"questions_{data_period}_{timestamp}.json"
        raw_filepath = self.questions_dir / raw_filename
        
        with open(raw_filepath, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        frontend_filename = f"questions_{data_period}_{timestamp}.json"
        frontend_filepath = self.frontend_questions_dir / frontend_filename
        
        with open(frontend_filepath, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒæ­£å¸¸ãªå ´åˆã®ã¿ï¼‰
        if len(questions) > 0:
            latest_file = self.frontend_questions_dir / "questions_latest.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(data_structure, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {latest_file}")
        
        logger.info(f"è³ªå•ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  - ç”Ÿãƒ‡ãƒ¼ã‚¿: {raw_filepath}")
        logger.info(f"  - ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: {frontend_filepath}")
        logger.info(f"  - ä»¶æ•°: {len(questions)}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è³ªå•ä¸»æ„æ›¸åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--start-date', type=str, help='åé›†é–‹å§‹æ—¥ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, help='åé›†çµ‚äº†æ—¥ (YYYY-MM-DD)')
    parser.add_argument('--days', type=int, default=60, help='éå»ä½•æ—¥åˆ†ã‚’åé›†ã™ã‚‹ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 60æ—¥)')
    
    args = parser.parse_args()
    
    # æ—¥ä»˜ç¯„å›²ã®è¨­å®š
    if args.start_date and args.end_date:
        start_date = args.start_date
        end_date = args.end_date
    elif args.days:
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=args.days)).strftime("%Y-%m-%d")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: éå»60æ—¥
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=60)).strftime("%Y-%m-%d")
    
    collector = QuestionsCollector(start_date=start_date, end_date=end_date)
    
    try:
        # è³ªå•ä¸»æ„æ›¸åé›†
        questions = collector.collect_questions()
        
        if questions:
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            collector.save_questions_data(questions)
            logger.info(f"æ–°è¦è³ªå•ä¸»æ„æ›¸åé›†å®Œäº†: {len(questions)}ä»¶")
        else:
            logger.info("æ–°è¦è³ªå•ä¸»æ„æ›¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()