#!/usr/bin/env python3
"""
æå‡ºæ³•æ¡ˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå¼·åŒ–ç‰ˆï¼‰

è¡†è­°é™¢è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å®Ÿéš›ã®è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«åé›†
https://www.shugiin.go.jp/internet/itdb_gian.nsf/html/gian/kaiji217.htm
é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨ãƒªãƒ³ã‚¯ä¿®æ­£ã‚’å®Ÿè£…
"""

import json
import requests
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging
import random
from urllib.parse import urljoin, urlparse

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BillsEnhancedCollector:
    """æå‡ºæ³•æ¡ˆåé›†ã‚¯ãƒ©ã‚¹ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, max_bills: int = 50):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        
        # åé›†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.max_bills = max_bills
        
        # å¯¾è±¡ã¨ãªã‚‹å›½ä¼šå›æ¬¡
        self.target_sessions = [217, 216]
        
        logger.info(f"æœ€å¤§åé›†ä»¶æ•°: {self.max_bills}ä»¶")
        logger.info(f"å¯¾è±¡å›½ä¼š: {', '.join(f'ç¬¬{s}å›' for s in self.target_sessions)}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.bills_dir = self.project_root / "data" / "processed" / "bills"
        self.frontend_bills_dir = self.project_root / "frontend" / "public" / "data" / "bills"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.bills_dir.mkdir(parents=True, exist_ok=True)
        self.frontend_bills_dir.mkdir(parents=True, exist_ok=True)
        
        # åŸºæœ¬URLè¨­å®š
        self.base_url = "https://www.shugiin.go.jp"
        
    def update_headers(self):
        """User-Agentæ›´æ–°ã¨IPå½è£…"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
    
    def random_delay(self, min_seconds=1, max_seconds=3):
        """ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
    
    def build_absolute_url(self, href: str, base_page_url: str) -> Optional[str]:
        """ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›"""
        if not href or href.strip() == '':
            return None
        
        # ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯ï¼ˆ#ã§å§‹ã¾ã‚‹ï¼‰ã¯ç„¡åŠ¹
        if href.startswith('#'):
            return None
        
        # æ—¢ã«çµ¶å¯¾URLã®å ´åˆ
        if href.startswith('http'):
            return href
        
        # ç›¸å¯¾URLå‡¦ç†
        if href.startswith('./'):
            # ./honbun/g20009011.htm
            relative_path = href[2:]
            base_dir = '/'.join(base_page_url.split('/')[:-1])
            return f"{base_dir}/{relative_path}"
        
        elif href.startswith('/'):
            # çµ¶å¯¾ãƒ‘ã‚¹
            return f"{self.base_url}{href}"
        
        else:
            # ç›¸å¯¾ãƒ‘ã‚¹
            base_dir = '/'.join(base_page_url.split('/')[:-1])
            return f"{base_dir}/{href}"
    
    def collect_bills_from_session(self, session_number: int) -> List[Dict[str, Any]]:
        """ç‰¹å®šã®å›½ä¼šã‹ã‚‰è­°æ¡ˆã‚’åé›†"""
        bills = []
        
        try:
            # å›½ä¼šåˆ¥è­°æ¡ˆä¸€è¦§ãƒšãƒ¼ã‚¸URL
            session_url = f"{self.base_url}/internet/itdb_gian.nsf/html/gian/kaiji{session_number}.htm"
            
            self.update_headers()
            self.random_delay()
            
            response = self.session.get(session_url, timeout=30)
            response.raise_for_status()
            response.encoding = 'shift_jis'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            logger.info(f"ç¬¬{session_number}å›å›½ä¼šãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {session_url}")
            
            # è­°æ¡ˆãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚Šå³å¯†ã«ï¼‰
            bill_links = self.extract_bill_links(soup, session_url, session_number)
            logger.info(f"æœ‰åŠ¹ãªè­°æ¡ˆãƒªãƒ³ã‚¯æ•°: {len(bill_links)}")
            
            # å„è­°æ¡ˆã®è©³ç´°ã‚’å–å¾—ï¼ˆåˆ¶é™ä»˜ãï¼‰
            collected_count = 0
            for idx, link_info in enumerate(bill_links):
                if collected_count >= self.max_bills // len(self.target_sessions):
                    logger.info(f"ç¬¬{session_number}å›å›½ä¼šã®åé›†ä¸Šé™({self.max_bills // len(self.target_sessions)})ã«åˆ°é”")
                    break
                
                try:
                    self.random_delay()
                    self.update_headers()
                    
                    bill_detail = self.extract_bill_detail(link_info, session_number)
                    if bill_detail and self.validate_bill_data(bill_detail):
                        bills.append(bill_detail)
                        collected_count += 1
                        logger.info(f"è­°æ¡ˆå–å¾—æˆåŠŸ ({collected_count}): {bill_detail['title'][:50]}...")
                    
                except Exception as e:
                    logger.error(f"è­°æ¡ˆè©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ ({idx+1}): {str(e)}")
                    continue
            
            return bills
            
        except Exception as e:
            logger.error(f"ç¬¬{session_number}å›å›½ä¼šã®è­°æ¡ˆåé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def extract_bill_links(self, soup: BeautifulSoup, base_url: str, session_number: int) -> List[Dict[str, str]]:
        """è­°æ¡ˆãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚Šå³å¯†ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰"""
        links = []
        
        try:
            # ã‚ˆã‚Šå³å¯†ãªè­°æ¡ˆãƒªãƒ³ã‚¯ã®åˆ¤å®š
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # å®Ÿéš›ã®è­°æ¡ˆãƒšãƒ¼ã‚¸ã‹ã©ã†ã‹ã‚’å³å¯†ã«åˆ¤å®š
                if self.is_valid_bill_link(href, text):
                    full_url = self.build_absolute_url(href, base_url)
                    if not full_url:
                        continue
                    
                    bill_number = self.extract_bill_number(text, href)
                    
                    links.append({
                        'url': full_url,
                        'title': text,
                        'bill_number': bill_number,
                        'session_number': session_number
                    })
            
            # é‡è¤‡é™¤å»ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            unique_links = []
            seen_urls = set()
            
            for link in links:
                # ç„¡åŠ¹ãªãƒªãƒ³ã‚¯ã‚’é™¤å¤–
                if (link['url'] not in seen_urls and 
                    len(link['title']) > 10 and  # ã‚¿ã‚¤ãƒˆãƒ«ãŒçŸ­ã™ãã‚‹ã‚‚ã®ã‚’é™¤å¤–
                    not any(invalid in link['title'].lower() for invalid in ['ãƒ¡ãƒ‹ãƒ¥ãƒ¼', 'ãƒˆãƒƒãƒ—', 'ãƒªãƒ³ã‚¯', 'ãƒšãƒ¼ã‚¸'])):
                    unique_links.append(link)
                    seen_urls.add(link['url'])
            
            return unique_links[:50]  # æœ€å¤§50ä»¶ã«åˆ¶é™
            
        except Exception as e:
            logger.error(f"è­°æ¡ˆãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def is_valid_bill_link(self, href: str, text: str) -> bool:
        """æœ‰åŠ¹ãªè­°æ¡ˆãƒªãƒ³ã‚¯ã‹ã©ã†ã‹åˆ¤å®š"""
        # ç„¡åŠ¹ãªãƒªãƒ³ã‚¯ã‚’é™¤å¤–
        if not href or href.startswith('#') or href.strip() == '':
            return False
        
        # é™¤å¤–ã™ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            'menu', 'index', 'top', 'search', 'help', 'site',
            'javascript:', 'mailto:', 'pdf', 'doc'
        ]
        
        for pattern in exclude_patterns:
            if pattern in href.lower() or pattern in text.lower():
                return False
        
        # è­°æ¡ˆé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
        bill_keywords = [
            'æ³•æ¡ˆ', 'æ³•å¾‹æ¡ˆ', 'æ”¹æ­£æ¡ˆ', 'è¨­ç½®æ³•', 'å»ƒæ­¢æ³•',
            'äºˆç®—', 'æ±ºç®—', 'æ¡ç´„', 'æ‰¿èª', 'è­°æ±º',
            'ç¬¬.*å·', 'æ¡ˆ'
        ]
        
        # URLãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè­°æ¡ˆæœ¬æ–‡ã‚„çµŒéæƒ…å ±ï¼‰
        url_patterns = [
            'honbun/', 'keika/', 'gian', '.htm'
        ]
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯URLã«è­°æ¡ˆé–¢é€£è¦ç´ ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
        text_match = any(re.search(keyword, text) for keyword in bill_keywords)
        url_match = any(pattern in href.lower() for pattern in url_patterns)
        
        return text_match and url_match and len(text) > 5
    
    def extract_bill_number(self, text: str, href: str) -> str:
        """è­°æ¡ˆç•ªå·ã‚’æŠ½å‡º"""
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç•ªå·ã‚’æŠ½å‡º
        patterns = [
            r'ç¬¬(\d+)å·',
            r'(\d+)å·',
            r'ç¬¬(\d+)',
            r'(\d+)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        
        # URLã‹ã‚‰ç•ªå·ã‚’æŠ½å‡º
        url_match = re.search(r'(\d+)', href)
        if url_match:
            return url_match.group(1)
        
        return ""
    
    def extract_bill_detail(self, link_info: Dict[str, str], session_number: int) -> Optional[Dict[str, Any]]:
        """è­°æ¡ˆè©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            response = self.session.get(link_info['url'], timeout=30)
            response.raise_for_status()
            response.encoding = 'shift_jis'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è­°æ¡ˆæƒ…å ±ã‚’è§£æ
            title = self.extract_bill_title(soup, link_info['title'])
            bill_content = self.extract_bill_content(soup)
            submitter = self.extract_submitter(soup)
            submission_date = self.extract_submission_date(soup)
            status = self.extract_status(soup)
            committee = self.extract_committee(soup)
            
            # é–¢é€£ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            related_links = self.extract_related_links(soup, link_info['url'])
            
            bill_detail = {
                'title': title,
                'bill_number': link_info['bill_number'],
                'session_number': session_number,
                'url': link_info['url'],
                'submitter': submitter,
                'submission_date': submission_date,
                'status': status,
                'status_normalized': self.normalize_status(status),
                'committee': committee,
                'bill_content': bill_content,
                'related_links': related_links,
                'summary': self.generate_summary(bill_content, title),
                'category': self.classify_bill_category(title),
                'collected_at': datetime.now().isoformat(),
                'year': datetime.now().year
            }
            
            return bill_detail
            
        except Exception as e:
            logger.error(f"è­°æ¡ˆè©³ç´°æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({link_info['url']}): {str(e)}")
            return None
    
    def extract_bill_title(self, soup: BeautifulSoup, fallback_title: str) -> str:
        """è­°æ¡ˆã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        try:
            # titleã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
                if title and len(title) > 10 and 'è­°æ¡ˆ' not in title:
                    return title
            
            # h1, h2ã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
            for tag_name in ['h1', 'h2', 'h3']:
                header_tag = soup.find(tag_name)
                if header_tag:
                    title = header_tag.get_text(strip=True)
                    if title and len(title) > 10:
                        return title
            
            # ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ³•æ¡ˆåã‚’æŠ½å‡º
            page_text = soup.get_text()
            
            # æ³•æ¡ˆåãƒ‘ã‚¿ãƒ¼ãƒ³
            title_patterns = [
                r'(.+?æ³•æ¡ˆ)',
                r'(.+?æ³•å¾‹æ¡ˆ)',
                r'(.+?æ”¹æ­£æ¡ˆ)',
                r'(.+?è¨­ç½®æ³•)',
                r'(.+?å»ƒæ­¢æ³•)'
            ]
            
            for pattern in title_patterns:
                matches = re.findall(pattern, page_text)
                for match in matches:
                    if len(match) > 10 and len(match) < 100:
                        return match.strip()
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if fallback_title and len(fallback_title) > 10:
                return fallback_title
            
            return "æ³•æ¡ˆã‚¿ã‚¤ãƒˆãƒ«æœªå–å¾—"
            
        except Exception as e:
            logger.error(f"è­°æ¡ˆã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return fallback_title or "ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼"
    
    def extract_bill_content(self, soup: BeautifulSoup) -> str:
        """è­°æ¡ˆæœ¬æ–‡ã‚’æŠ½å‡º"""
        try:
            # ã‚ˆã‚Šåºƒç¯„å›²ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            full_text = soup.get_text(separator='\n', strip=True)
            
            # æ³•æ¡ˆæœ¬æ–‡éƒ¨åˆ†ã‚’ç‰¹å®š
            content_patterns = [
                r'ç¬¬ä¸€æ¡(.+?)é™„å‰‡',
                r'ï¼ˆç›®çš„ï¼‰(.+?)é™„å‰‡',
                r'ã“ã®æ³•å¾‹ã¯(.+?)ã€‚',
                r'(.{200,1000})'
            ]
            
            for pattern in content_patterns:
                match = re.search(pattern, full_text, re.DOTALL)
                if match:
                    content = match.group(1).strip()
                    if len(content) > 50:
                        return self.clean_text(content)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨
            if len(full_text) > 100:
                return self.clean_text(full_text[:500])
            
            return ""
            
        except Exception as e:
            logger.error(f"è­°æ¡ˆæœ¬æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_submitter(self, soup: BeautifulSoup) -> str:
        """æå‡ºè€…ã‚’æŠ½å‡º"""
        try:
            page_text = soup.get_text()
            
            patterns = [
                r'æå‡ºè€…[ï¼š:]\s*([^\n\r]+)',
                r'æå‡º[ï¼š:]\s*([^\n\r]+)',
                r'([^\n\r]+)æå‡º',
                r'å†…é–£æå‡º',
                r'è­°å“¡æå‡º'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, page_text)
                if match:
                    submitter = match.group(1) if len(match.groups()) > 0 else match.group(0)
                    return submitter.strip()
            
            return ""
            
        except Exception as e:
            logger.error(f"æå‡ºè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_submission_date(self, soup: BeautifulSoup) -> str:
        """æå‡ºæ—¥ã‚’æŠ½å‡º"""
        try:
            page_text = soup.get_text()
            
            patterns = [
                r'æå‡ºæ—¥[ï¼š:]\s*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥æå‡º',
                r'ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, page_text)
                if match:
                    groups = match.groups()
                    if len(groups) >= 3:
                        year, month, day = groups[:3]
                        # ä»¤å’Œå¹´å·ã®å‡¦ç†
                        if len(year) <= 2:
                            year = str(2018 + int(year))
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            return ""
            
        except Exception as e:
            logger.error(f"æå‡ºæ—¥æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_status(self, soup: BeautifulSoup) -> str:
        """è­°æ¡ˆçŠ¶æ³ã‚’æŠ½å‡º"""
        try:
            page_text = soup.get_text()
            
            status_keywords = ['å¯æ±º', 'å¦æ±º', 'å»ƒæ¡ˆ', 'ç¶™ç¶šå¯©è­°', 'æˆç«‹', 'å¯©è­°ä¸­', 'å§”å“¡ä¼šå¯©æŸ»ä¸­']
            
            for keyword in status_keywords:
                if keyword in page_text:
                    return keyword
            
            return "å¯©è­°ä¸­"
            
        except Exception as e:
            logger.error(f"è­°æ¡ˆçŠ¶æ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return "ä¸æ˜"
    
    def normalize_status(self, status: str) -> str:
        """è­°æ¡ˆçŠ¶æ³ã‚’æ­£è¦åŒ–"""
        status_mapping = {
            'å¯æ±º': 'å¯æ±º',
            'æˆç«‹': 'æˆç«‹',
            'å¦æ±º': 'å¦æ±º',
            'å»ƒæ¡ˆ': 'å»ƒæ¡ˆ',
            'ç¶™ç¶šå¯©è­°': 'ç¶™ç¶šå¯©è­°',
            'å¯©è­°ä¸­': 'å¯©è­°ä¸­',
            'å§”å“¡ä¼šå¯©æŸ»ä¸­': 'å¯©è­°ä¸­'
        }
        
        return status_mapping.get(status, 'ä¸æ˜')
    
    def extract_committee(self, soup: BeautifulSoup) -> str:
        """å§”å“¡ä¼šåã‚’æŠ½å‡º"""
        try:
            page_text = soup.get_text()
            
            committee_patterns = [
                r'([^å§”å“¡ä¼š]*å§”å“¡ä¼š)',
                r'([^èª¿æŸ»ä¼š]*èª¿æŸ»ä¼š)'
            ]
            
            for pattern in committee_patterns:
                match = re.search(pattern, page_text)
                if match:
                    return match.group(1)
            
            return ""
            
        except Exception as e:
            logger.error(f"å§”å“¡ä¼šåæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_related_links(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """é–¢é€£ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        try:
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # é–¢é€£æ–‡æ›¸ã®ãƒªãƒ³ã‚¯ã‚’åˆ¤å®š
                if any(ext in href.lower() for ext in ['.pdf', '.doc', '.html', '.htm']):
                    full_url = self.build_absolute_url(href, base_url)
                    if full_url and 'menu' not in href.lower():
                        links.append({
                            'url': full_url,
                            'title': text or 'é–¢é€£æ–‡æ›¸'
                        })
            
        except Exception as e:
            logger.error(f"é–¢é€£ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return links[:5]  # æœ€å¤§5ä»¶ã«åˆ¶é™
    
    def generate_summary(self, content: str, title: str) -> str:
        """è­°æ¡ˆã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        try:
            if content and len(content) > 100:
                summary = content[:200]
                if len(content) > 200:
                    summary += "..."
                return summary
            elif title:
                return f"{title}ã«é–¢ã™ã‚‹æ³•æ¡ˆ"
            else:
                return "è­°æ¡ˆã®æ¦‚è¦"
            
        except Exception as e:
            logger.error(f"ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def classify_bill_category(self, title: str) -> str:
        """è­°æ¡ˆã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é¡"""
        categories = {
            'å¤–äº¤ãƒ»å®‰å…¨ä¿éšœ': ['å¤–äº¤', 'æ¡ç´„', 'é˜²è¡›', 'è‡ªè¡›éšŠ', 'å®‰å…¨ä¿éšœ'],
            'çµŒæ¸ˆãƒ»è²¡æ”¿': ['çµŒæ¸ˆ', 'è²¡æ”¿', 'äºˆç®—', 'ç¨åˆ¶', 'é‡‘è', 'ç”£æ¥­'],
            'ç¤¾ä¼šä¿éšœ': ['å¹´é‡‘', 'åŒ»ç™‚', 'ä»‹è­·', 'ç¦ç¥‰', 'ç¤¾ä¼šä¿éšœ'],
            'æ•™è‚²ãƒ»æ–‡åŒ–': ['æ•™è‚²', 'å­¦æ ¡', 'å¤§å­¦', 'æ–‡åŒ–', 'æ–‡éƒ¨ç§‘å­¦'],
            'ç’°å¢ƒãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼': ['ç’°å¢ƒ', 'ã‚¨ãƒãƒ«ã‚®ãƒ¼', 'åŸå­åŠ›'],
            'åŠ´åƒãƒ»é›‡ç”¨': ['åŠ´åƒ', 'é›‡ç”¨', 'åƒãæ–¹'],
            'å¸æ³•ãƒ»è¡Œæ”¿': ['å¸æ³•', 'è¡Œæ”¿', 'å…¬å‹™å“¡', 'è£åˆ¤'],
            'åœ°æ–¹ãƒ»éƒ½å¸‚': ['åœ°æ–¹', 'è‡ªæ²»ä½“', 'éƒ½å¸‚'],
            'äº¤é€šãƒ»å›½åœŸ': ['äº¤é€š', 'é“è·¯', 'å›½åœŸ']
        }
        
        title_lower = title.lower()
        
        for category, keywords in categories.items():
            if any(keyword in title_lower for keyword in keywords):
                return category
        
        return 'ä¸€èˆ¬'
    
    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢"""
        if not text:
            return ""
        
        # æ”¹è¡Œãƒ»ã‚¹ãƒšãƒ¼ã‚¹ã®æ­£è¦åŒ–
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'[\u3000]+', ' ', text)
        
        return text.strip()
    
    def validate_bill_data(self, bill: Dict[str, Any]) -> bool:
        """è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        required_fields = ['title', 'session_number']
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
        for field in required_fields:
            if not bill.get(field):
                return False
        
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒæ„å‘³ã®ã‚ã‚‹ã‚‚ã®ã‹ãƒã‚§ãƒƒã‚¯
        title = bill.get('title', '')
        if title in ['æœ¬æ–‡', 'ãƒ¡ã‚¤ãƒ³', 'ã‚¨ãƒ©ãƒ¼', '', 'çµŒé', 'è­°æ¡ˆæƒ…å ±', 'ç«‹æ³•æƒ…å ±']:
            return False
        
        # URLã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        url = bill.get('url', '')
        if not url or 'menu.htm' in url or 'index.nsf' in url:
            return False
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é•·ã•ãƒã‚§ãƒƒã‚¯
        if len(title) < 10:
            return False
        
        return True
    
    def collect_all_bills(self) -> List[Dict[str, Any]]:
        """ã™ã¹ã¦ã®è­°æ¡ˆã‚’åé›†"""
        logger.info("æå‡ºæ³•æ¡ˆåé›†é–‹å§‹...")
        all_bills = []
        
        try:
            for session in self.target_sessions:
                if len(all_bills) >= self.max_bills:
                    logger.info(f"æœ€å¤§åé›†ä»¶æ•°({self.max_bills})ã«åˆ°é”ã—ã¾ã—ãŸ")
                    break
                
                session_bills = self.collect_bills_from_session(session)
                all_bills.extend(session_bills)
                logger.info(f"ç¬¬{session}å›å›½ä¼šã‹ã‚‰{len(session_bills)}ä»¶ã®è­°æ¡ˆã‚’åé›†")
            
            logger.info(f"æå‡ºæ³•æ¡ˆåé›†å®Œäº†: {len(all_bills)}ä»¶")
            return all_bills
            
        except Exception as e:
            logger.error(f"æå‡ºæ³•æ¡ˆåé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def save_bills_data(self, bills: List[Dict[str, Any]]):
        """è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if not bills:
            logger.warning("ä¿å­˜ã™ã‚‹è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        current_date = datetime.now()
        data_period = current_date.strftime('%Y%m%d')
        timestamp = current_date.strftime('%H%M%S')
        
        data_structure = {
            "metadata": {
                "data_type": "shugiin_bills_enhanced",
                "collection_method": "enhanced_scraping",
                "total_bills": len(bills),
                "generated_at": current_date.isoformat(),
                "source_site": "www.shugiin.go.jp",
                "target_sessions": self.target_sessions,
                "data_quality": "enhanced_collection",
                "version": "2.0"
            },
            "data": bills
        }
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜
        raw_filename = f"bills_{data_period}_{timestamp}.json"
        raw_filepath = self.bills_dir / raw_filename
        
        with open(raw_filepath, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        frontend_filename = f"bills_{data_period}_{timestamp}.json"
        frontend_filepath = self.frontend_bills_dir / frontend_filename
        
        with open(frontend_filepath, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
        latest_file = self.frontend_bills_dir / "bills_latest.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {latest_file}")
        
        logger.info(f"è­°æ¡ˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  - ç”Ÿãƒ‡ãƒ¼ã‚¿: {raw_filepath}")
        logger.info(f"  - ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: {frontend_filepath}")
        logger.info(f"  - ä»¶æ•°: {len(bills)}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æå‡ºæ³•æ¡ˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå¼·åŒ–ç‰ˆï¼‰')
    parser.add_argument('--max-bills', type=int, default=50, help='æœ€å¤§åé›†ä»¶æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ä»¶)')
    
    args = parser.parse_args()
    
    collector = BillsEnhancedCollector(max_bills=args.max_bills)
    
    try:
        # æå‡ºæ³•æ¡ˆåé›†
        bills = collector.collect_all_bills()
        
        if bills:
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            collector.save_bills_data(bills)
            logger.info(f"æ–°è¦æå‡ºæ³•æ¡ˆåé›†å®Œäº†: {len(bills)}ä»¶")
        else:
            logger.info("æ–°è¦æå‡ºæ³•æ¡ˆã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()