#!/usr/bin/env python3
"""
å…¨47éƒ½é“åºœçœŒ + æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›†
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from collect_go2senkyo_optimized import Go2senkyoOptimizedCollector

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def collect_all_japan():
    """å…¨æ—¥æœ¬ã®ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆ47éƒ½é“åºœçœŒ + æ¯”ä¾‹ä»£è¡¨ï¼‰"""
    logger.info("ğŸš€ å…¨47éƒ½é“åºœçœŒ + æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
    
    # å…¨éƒ½é“åºœçœŒï¼ˆGo2senkyoã®ã‚³ãƒ¼ãƒ‰é †ï¼‰
    all_prefectures = {
        "åŒ—æµ·é“": 1, "é’æ£®çœŒ": 2, "å²©æ‰‹çœŒ": 3, "å®®åŸçœŒ": 4, "ç§‹ç”°çœŒ": 5,
        "å±±å½¢çœŒ": 6, "ç¦å³¶çœŒ": 7, "èŒ¨åŸçœŒ": 8, "æ ƒæœ¨çœŒ": 9, "ç¾¤é¦¬çœŒ": 10,
        "åŸ¼ç‰çœŒ": 11, "åƒè‘‰çœŒ": 12, "æ±äº¬éƒ½": 13, "ç¥å¥ˆå·çœŒ": 14, "æ–°æ½ŸçœŒ": 15,
        "å¯Œå±±çœŒ": 16, "çŸ³å·çœŒ": 17, "ç¦äº•çœŒ": 18, "å±±æ¢¨çœŒ": 19, "é•·é‡çœŒ": 20,
        "å²é˜œçœŒ": 21, "é™å²¡çœŒ": 22, "æ„›çŸ¥çœŒ": 23, "ä¸‰é‡çœŒ": 24, "æ»‹è³€çœŒ": 25,
        "äº¬éƒ½åºœ": 26, "å¤§é˜ªåºœ": 27, "å…µåº«çœŒ": 28, "å¥ˆè‰¯çœŒ": 29, "å’Œæ­Œå±±çœŒ": 30,
        "é³¥å–çœŒ": 31, "å³¶æ ¹çœŒ": 32, "å²¡å±±çœŒ": 33, "åºƒå³¶çœŒ": 34, "å±±å£çœŒ": 35,
        "å¾³å³¶çœŒ": 36, "é¦™å·çœŒ": 37, "æ„›åª›çœŒ": 38, "é«˜çŸ¥çœŒ": 39, "ç¦å²¡çœŒ": 40,
        "ä½è³€çœŒ": 41, "é•·å´çœŒ": 42, "ç†Šæœ¬çœŒ": 43, "å¤§åˆ†çœŒ": 44, "å®®å´çœŒ": 45,
        "é¹¿å…å³¶çœŒ": 46, "æ²–ç¸„çœŒ": 47
    }
    
    collector = Go2senkyoOptimizedCollector()
    all_candidates = []
    success_count = 0
    error_count = 0
    
    try:
        # å„éƒ½é“åºœçœŒã‚’é †æ¬¡å‡¦ç†
        for prefecture, code in all_prefectures.items():
            try:
                logger.info(f"ğŸ“ {prefecture} (ã‚³ãƒ¼ãƒ‰: {code}) åé›†ä¸­...")
                
                # åŸºæœ¬æƒ…å ±åé›†ï¼ˆè©³ç´°ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãªã—ï¼‰
                url = f"{collector.base_url}/2025/prefecture/{code}"
                collector.random_delay(0.5, 1.5)  # çŸ­ã„é–“éš”
                
                response = collector.session.get(url, timeout=30)
                if response.status_code != 200:
                    logger.warning(f"{prefecture}: HTTP {response.status_code}")
                    error_count += 1
                    continue
                
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # å€™è£œè€…ãƒ–ãƒ­ãƒƒã‚¯å–å¾—
                candidate_blocks = soup.find_all('div', class_='p_senkyoku_list_block')
                
                if not candidate_blocks:
                    logger.warning(f"{prefecture}: å€™è£œè€…ãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    error_count += 1
                    continue
                
                prefecture_candidates = []
                for i, block in enumerate(candidate_blocks):
                    try:
                        candidate_info = collector.extract_candidate_from_block(block, prefecture, url, i)
                        if candidate_info:
                            prefecture_candidates.append(candidate_info)
                    except Exception as e:
                        logger.debug(f"{prefecture} ãƒ–ãƒ­ãƒƒã‚¯{i}ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                
                all_candidates.extend(prefecture_candidates)
                success_count += 1
                
                logger.info(f"âœ… {prefecture}: {len(prefecture_candidates)}å")
                
                # é€²æ—è¡¨ç¤º
                progress = success_count + error_count
                logger.info(f"é€²æ—: {progress}/47 ({progress/47*100:.1f}%)")
                
            except Exception as e:
                logger.error(f"âŒ {prefecture}ã‚¨ãƒ©ãƒ¼: {e}")
                error_count += 1
                continue
        
        # æ¯”ä¾‹ä»£è¡¨ã‚‚åé›†ï¼ˆURLç¢ºèªãŒå¿…è¦ï¼‰
        logger.info("ğŸ“ æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚’è©¦è¡Œ...")
        try:
            # æ¯”ä¾‹ä»£è¡¨ã®URLï¼ˆæ¨æ¸¬ï¼‰
            proportional_urls = [
                f"{collector.base_url}/2025/seitou",  # æ”¿å…šãƒšãƒ¼ã‚¸
                f"{collector.base_url}/2025/hirei",   # æ¯”ä¾‹ä»£è¡¨ãƒšãƒ¼ã‚¸ï¼ˆæ¨æ¸¬ï¼‰
            ]
            
            for i, prop_url in enumerate(proportional_urls):
                try:
                    response = collector.session.get(prop_url, timeout=30)
                    if response.status_code == 200:
                        logger.info(f"âœ… æ¯”ä¾‹ä»£è¡¨ãƒšãƒ¼ã‚¸ç™ºè¦‹: {prop_url}")
                        # æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¯ä»Šå¾Œå®Ÿè£…
                        break
                except:
                    continue
        except Exception as e:
            logger.debug(f"æ¯”ä¾‹ä»£è¡¨åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        logger.info(f"ğŸ¯ å…¨éƒ½é“åºœçœŒåé›†å®Œäº†:")
        logger.info(f"  æˆåŠŸ: {success_count}éƒ½é“åºœçœŒ")
        logger.info(f"  å¤±æ•—: {error_count}éƒ½é“åºœçœŒ")
        logger.info(f"  ç·å€™è£œè€…æ•°: {len(all_candidates)}å")
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        if all_candidates:
            save_complete_data(all_candidates, collector.output_dir)
            
            # çµ±è¨ˆè¡¨ç¤º
            show_statistics(all_candidates)
        
        return all_candidates
        
    except Exception as e:
        logger.error(f"âŒ å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def save_complete_data(candidates, output_dir):
    """å®Œå…¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    main_file = output_dir / f"go2senkyo_complete_{timestamp}.json"
    latest_file = output_dir / "go2senkyo_optimized_latest.json"
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    pref_stats = {}
    
    for candidate in candidates:
        party = candidate.get('party', 'ç„¡æ‰€å±')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        pref_stats[prefecture] = pref_stats.get(prefecture, 0) + 1
    
    save_data = {
        "metadata": {
            "data_type": "go2senkyo_complete_sangiin_2025",
            "collection_method": "complete_47_prefectures_scraping",
            "total_candidates": len(candidates),
            "generated_at": datetime.now().isoformat(),
            "source_site": "sangiin.go2senkyo.com",
            "coverage": {
                "prefectures": len(pref_stats),
                "parties": len(party_stats)
            },
            "collection_stats": {
                "total_candidates": len(candidates),
                "detailed_profiles": 0,
                "with_photos": 0,
                "with_policies": 0,
                "errors": 0
            },
            "quality_metrics": {
                "detail_coverage": "0%",
                "photo_coverage": "0%", 
                "policy_coverage": "0%"
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": pref_stats
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    with open(main_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ å®Œå…¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: {main_file}")

def show_statistics(candidates):
    """çµ±è¨ˆè¡¨ç¤º"""
    prefectures = {}
    parties = {}
    
    for candidate in candidates:
        pref = candidate.get('prefecture', 'æœªåˆ†é¡')
        party = candidate.get('party', 'ç„¡æ‰€å±')
        
        prefectures[pref] = prefectures.get(pref, 0) + 1
        parties[party] = parties.get(party, 0) + 1
    
    logger.info("ğŸ“Š éƒ½é“åºœçœŒåˆ¥çµ±è¨ˆï¼ˆä¸Šä½20ï¼‰:")
    top_prefs = sorted(prefectures.items(), key=lambda x: x[1], reverse=True)[:20]
    for pref, count in top_prefs:
        logger.info(f"  {pref}: {count}å")
    
    logger.info("ğŸ›ï¸ æ”¿å…šåˆ¥çµ±è¨ˆï¼ˆä¸Šä½15ï¼‰:")
    top_parties = sorted(parties.items(), key=lambda x: x[1], reverse=True)[:15]
    for party, count in top_parties:
        logger.info(f"  {party}: {count}å")
    
    # åœ°åŸŸåˆ¥çµ±è¨ˆ
    regions = {
        "é–¢æ±": ["æ±äº¬éƒ½", "ç¥å¥ˆå·çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ", "èŒ¨åŸçœŒ", "æ ƒæœ¨çœŒ", "ç¾¤é¦¬çœŒ"],
        "é–¢è¥¿": ["å¤§é˜ªåºœ", "å…µåº«çœŒ", "äº¬éƒ½åºœ", "å¥ˆè‰¯çœŒ", "å’Œæ­Œå±±çœŒ", "æ»‹è³€çœŒ"],
        "ä¸­éƒ¨": ["æ„›çŸ¥çœŒ", "é™å²¡çœŒ", "å²é˜œçœŒ", "ä¸‰é‡çœŒ", "æ–°æ½ŸçœŒ", "å¯Œå±±çœŒ", "çŸ³å·çœŒ", "ç¦äº•çœŒ", "å±±æ¢¨çœŒ", "é•·é‡çœŒ"],
        "ä¹å·": ["ç¦å²¡çœŒ", "ä½è³€çœŒ", "é•·å´çœŒ", "ç†Šæœ¬çœŒ", "å¤§åˆ†çœŒ", "å®®å´çœŒ", "é¹¿å…å³¶çœŒ", "æ²–ç¸„çœŒ"],
        "æ±åŒ—": ["é’æ£®çœŒ", "å²©æ‰‹çœŒ", "å®®åŸçœŒ", "ç§‹ç”°çœŒ", "å±±å½¢çœŒ", "ç¦å³¶çœŒ"],
        "ä¸­å›½": ["åºƒå³¶çœŒ", "å²¡å±±çœŒ", "é³¥å–çœŒ", "å³¶æ ¹çœŒ", "å±±å£çœŒ"],
        "å››å›½": ["å¾³å³¶çœŒ", "é¦™å·çœŒ", "æ„›åª›çœŒ", "é«˜çŸ¥çœŒ"],
        "åŒ—æµ·é“": ["åŒ—æµ·é“"]
    }
    
    logger.info("ğŸ—¾ åœ°åŸŸåˆ¥çµ±è¨ˆ:")
    for region, prefs in regions.items():
        region_count = sum(prefectures.get(pref, 0) for pref in prefs)
        logger.info(f"  {region}: {region_count}å")

if __name__ == "__main__":
    collect_all_japan()