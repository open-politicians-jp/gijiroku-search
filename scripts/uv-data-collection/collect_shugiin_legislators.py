#!/usr/bin/env python3
"""
è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Issue #16å¯¾å¿œ)

è¡†è­°é™¢è­°å“¡ãƒšãƒ¼ã‚¸ã‹ã‚‰è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€å‚è­°é™¢ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆå¯èƒ½ãªå½¢å¼ã§ä¿å­˜
https://www.shugiin.go.jp/internet/itdb_annai.nsf/html/statics/shiryo/kaiha_m.htm

æ©Ÿèƒ½:
- è¡†è­°é™¢è­°å“¡ä¸€è¦§ã®åé›†
- æ”¿å…šåˆ¥ãƒ»é¸æŒ™åŒºåˆ¥æƒ…å ±ã®æ•´ç†
- å‚è­°é™¢ãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆå¯¾å¿œ
- frontend/public/data/legislators/ ã«ä¿å­˜
"""

import json
import requests
import time
import re
import random
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ShugiinLegislatorsCollector:
    """è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹ (Issue #16å¯¾å¿œ)"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.legislators_dir = self.project_root / "frontend" / "public" / "data" / "legislators"
        self.raw_data_dir = self.project_root / "data" / "processed" / "shugiin_legislators"
        self.raw_data_dir.mkdir(parents=True, exist_ok=True)
        self.legislators_dir.mkdir(parents=True, exist_ok=True)
        
        # è¡†è­°é™¢è­°å“¡é–¢é€£URL
        self.base_url = "https://www.shugiin.go.jp"
        self.members_url = "https://www.shugiin.go.jp/internet/itdb_annai.nsf/html/statics/syu/1giin.htm"
        
        # æ”¿å…šåæ­£è¦åŒ–ãƒãƒƒãƒ”ãƒ³ã‚°
        self.party_mapping = {
            'è‡ªç”±æ°‘ä¸»å…š': 'è‡ªç”±æ°‘ä¸»å…š',
            'è‡ªæ°‘å…š': 'è‡ªç”±æ°‘ä¸»å…š',
            'ç«‹æ†²æ°‘ä¸»å…š': 'ç«‹æ†²æ°‘ä¸»å…š',
            'ç«‹æ°‘': 'ç«‹æ†²æ°‘ä¸»å…š',
            'æ—¥æœ¬ç¶­æ–°ã®ä¼š': 'æ—¥æœ¬ç¶­æ–°ã®ä¼š',
            'ç¶­æ–°': 'æ—¥æœ¬ç¶­æ–°ã®ä¼š',
            'å…¬æ˜å…š': 'å…¬æ˜å…š',
            'å…¬æ˜': 'å…¬æ˜å…š',
            'æ—¥æœ¬å…±ç”£å…š': 'æ—¥æœ¬å…±ç”£å…š',
            'å…±ç”£å…š': 'æ—¥æœ¬å…±ç”£å…š',
            'å…±ç”£': 'æ—¥æœ¬å…±ç”£å…š',
            'å›½æ°‘æ°‘ä¸»å…š': 'å›½æ°‘æ°‘ä¸»å…š',
            'å›½æ°‘': 'å›½æ°‘æ°‘ä¸»å…š',
            'ã‚Œã„ã‚æ–°é¸çµ„': 'ã‚Œã„ã‚æ–°é¸çµ„',
            'ã‚Œã„ã‚': 'ã‚Œã„ã‚æ–°é¸çµ„',
            'ç¤¾ä¼šæ°‘ä¸»å…š': 'ç¤¾ä¼šæ°‘ä¸»å…š',
            'ç¤¾æ°‘': 'ç¤¾ä¼šæ°‘ä¸»å…š',
            'NHKå…š': 'NHKå…š',
            'Nå›½': 'NHKå…š',
            'ç„¡æ‰€å±': 'ç„¡æ‰€å±'
        }
        
    def update_headers(self):
        """User-Agentæ›´æ–°ã¨IPå½è£…"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def random_delay(self, min_seconds=1, max_seconds=3):
        """ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
    
    def normalize_party_name(self, party_name: str) -> str:
        """æ”¿å…šåã‚’æ­£è¦åŒ–"""
        for key, normalized in self.party_mapping.items():
            if key in party_name:
                return normalized
        return party_name
    
    def extract_constituency_info(self, constituency_text: str) -> Dict[str, Any]:
        """é¸æŒ™åŒºæƒ…å ±ã‚’è§£æ"""
        if not constituency_text:
            return {"type": "unknown", "region": None, "district": None}
        
        # æ¯”ä¾‹ä»£è¡¨ã®å ´åˆ
        if "æ¯”ä¾‹" in constituency_text:
            # æ¯”ä¾‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º (ä¾‹: "æ¯”ä¾‹è¿‘ç•¿" â†’ "è¿‘ç•¿")
            region_match = re.search(r'æ¯”ä¾‹(.+)', constituency_text)
            region = region_match.group(1) if region_match else "å…¨å›½"
            return {"type": "proportional", "region": region, "district": None}
        
        # å°é¸æŒ™åŒºã®å ´åˆ (ä¾‹: "æ±äº¬1åŒº", "åŒ—æµ·é“12åŒº")
        district_match = re.search(r'(.+?)(\d+)åŒº', constituency_text)
        if district_match:
            prefecture = district_match.group(1)
            district_num = int(district_match.group(2))
            return {
                "type": "single_member", 
                "region": prefecture, 
                "district": district_num,
                "full_name": constituency_text
            }
        
        # ãã®ä»–ã®å ´åˆ
        return {"type": "other", "region": constituency_text, "district": None}
    
    def collect_members_data(self) -> List[Dict[str, Any]]:
        """è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        
        try:
            self.random_delay()
            response = self.session.get(self.members_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            logger.info(f"è¡†è­°é™¢è­°å“¡ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ: {self.members_url}")
            
            # è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            members = self.extract_members_from_tables(soup)
            
            logger.info(f"è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(members)}å")
            return members
            
        except Exception as e:
            logger.error(f"è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def extract_members_from_tables(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        members = []
        
        try:
            # è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            tables = soup.find_all('table')
            logger.info(f"ç™ºè¦‹ã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(tables)}")
            
            for table_idx, table in enumerate(tables):
                rows = table.find_all('tr')
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆè¡Œæ•°ãŒå¤šã„ï¼‰
                if len(rows) > 20:
                    logger.info(f"ãƒ†ãƒ¼ãƒ–ãƒ«{table_idx + 1}ã‚’è§£æä¸­: {len(rows)}è¡Œ")
                    
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦è­°å“¡ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ç¢ºèª
                    if rows:
                        header_cells = rows[0].find_all(['th', 'td'])
                        header_texts = [cell.get_text(strip=True) for cell in header_cells]
                        if any('æ°å' in text for text in header_texts):
                            logger.info(f"è­°å“¡ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç™ºè¦‹: {header_texts}")
                            table_members = self.parse_members_table(table)
                            members.extend(table_members)
                            logger.info(f"ãƒ†ãƒ¼ãƒ–ãƒ«{table_idx + 1}ã‹ã‚‰{len(table_members)}åæŠ½å‡º")
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ä»¥å¤–ã®æ§‹é€ ã‚‚è©¦è¡Œ
            if not members:
                logger.info("ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ãƒªã‚¹ãƒˆå½¢å¼ã‚’æ¤œç´¢")
                members = self.extract_members_from_lists(soup)
            
            return members
            
        except Exception as e:
            logger.error(f"è­°å“¡ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def parse_members_table(self, table: BeautifulSoup) -> List[Dict[str, Any]]:
        """è­°å“¡ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è§£æ"""
        members = []
        
        try:
            rows = table.find_all('tr')
            
            for row_idx, row in enumerate(rows[1:], 1):  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                cells = row.find_all(['td', 'th'])
                
                if len(cells) >= 3:  # æœ€ä½é™ã®åˆ—æ•°ãƒã‚§ãƒƒã‚¯
                    member_data = self.extract_member_from_row(cells, row_idx)
                    if member_data:
                        members.append(member_data)
            
        except Exception as e:
            logger.error(f"ãƒ†ãƒ¼ãƒ–ãƒ«è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return members
    
    def extract_member_from_row(self, cells: List, row_idx: int) -> Optional[Dict[str, Any]]:
        """è¡Œã‹ã‚‰è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆè¡†è­°é™¢å½¢å¼å¯¾å¿œï¼‰"""
        try:
            # è¡†è­°é™¢ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ : ['æ°å', 'ãµã‚ŠãŒãª', 'ä¼šæ´¾', 'é¸æŒ™åŒº', 'å½“é¸å›æ•°']
            if len(cells) < 5:
                return None
            
            # å„åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            name_raw = cells[0].get_text(strip=True)
            reading = cells[1].get_text(strip=True).replace('\n', '').replace('\u3000', ' ')
            party_raw = cells[2].get_text(strip=True)
            constituency = cells[3].get_text(strip=True)
            election_count_str = cells[4].get_text(strip=True)
            
            # åå‰ã®æ­£è¦åŒ–ï¼ˆã€Œå›ã€ã‚’é™¤å»ï¼‰
            name = name_raw.replace('å›', '').strip()
            if not name or name == 'æ°å':  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                return None
            
            # æ”¿å…šåæ­£è¦åŒ–
            party_normalized = self.normalize_party_name(party_raw)
            
            # å½“é¸å›æ•°
            try:
                election_count = int(election_count_str) if election_count_str.isdigit() else 0
            except ValueError:
                election_count = 0
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            profile_url = ""
            for cell in cells:
                links = cell.find_all('a', href=True)
                for link in links:
                    href = link.get('href')
                    if href and 'profile' in href.lower():
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        if href.startswith('../../../../'):
                            profile_url = self.base_url + '/' + href[5:]  # ../../../../ã‚’é™¤å»
                            profile_url = profile_url.replace('//', '/')  # é‡è¤‡ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’ä¿®æ­£
                            profile_url = profile_url.replace('http:/', 'http://')  # httpãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä¿®æ­£
                        elif href.startswith('/'):
                            profile_url = self.base_url + href
                        elif href.startswith('http'):
                            profile_url = href
                        break
                if profile_url:
                    break
            
            # é¸æŒ™åŒºæƒ…å ±ã®è©³ç´°è§£æ
            constituency_info = self.extract_constituency_info(constituency)
            
            member = {
                "id": f"shugiin_{row_idx:03d}",
                "name": name,
                "reading": reading,
                "house": "shugiin",
                "party": party_normalized,
                "party_original": party_raw,
                "constituency": constituency,
                "constituency_type": constituency_info["type"],
                "region": constituency_info["region"],
                "district": constituency_info.get("district"),
                "election_count": election_count,
                "status": "active",  # ç¾è·ã¨ä»®å®š
                "profile_url": profile_url,
                "collected_at": datetime.now().isoformat(),
                "source_url": self.members_url
            }
            
            return member
            
        except Exception as e:
            logger.debug(f"è¡Œ{row_idx}ã®è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def extract_members_from_lists(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """ãƒªã‚¹ãƒˆå½¢å¼ã‹ã‚‰è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        members = []
        
        try:
            # divã€ulã€olç­‰ã‹ã‚‰è­°å“¡åã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
            member_elements = soup.find_all(['div', 'li', 'p'], string=re.compile(r'[ä¸€-é¾¯]{2,4}[ã€€\s]+[ä¸€-é¾¯]{2,4}'))
            
            for idx, element in enumerate(member_elements):
                text = element.get_text(strip=True)
                
                # è­°å“¡åã‚‰ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                if 'ã€€' in text and len(text.split('ã€€')) == 2:
                    name_parts = text.split('ã€€')
                    if all(len(part) >= 1 for part in name_parts):
                        member = {
                            "id": f"shugiin_list_{idx:03d}",
                            "name": text,
                            "house": "shugiin",
                            "party": "æœªåˆ†é¡",
                            "party_original": "",
                            "constituency": "",
                            "constituency_type": "unknown",
                            "region": None,
                            "district": None,
                            "status": "active",
                            "profile_url": "",
                            "collected_at": datetime.now().isoformat(),
                            "source_url": self.members_url
                        }
                        members.append(member)
            
        except Exception as e:
            logger.error(f"ãƒªã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return members
    
    def enhance_with_profile_data(self, members: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—"""
        logger.info(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±åé›†é–‹å§‹: {len(members)}å")
        
        enhanced_members = []
        
        for idx, member in enumerate(members):
            try:
                enhanced = member.copy()
                
                profile_url = member.get('profile_url', '')
                if profile_url:
                    self.random_delay()
                    profile_data = self.fetch_profile_data(profile_url)
                    enhanced.update(profile_data)
                
                enhanced_members.append(enhanced)
                
                # é€²æ—è¡¨ç¤º
                if (idx + 1) % 50 == 0:
                    logger.info(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«åé›†é€²æ—: {idx + 1}/{len(members)}")
                
            except Exception as e:
                logger.error(f"è­°å“¡{member.get('name', 'unknown')}ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                enhanced_members.append(member)
                continue
        
        logger.info(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±åé›†å®Œäº†: {len(enhanced_members)}å")
        return enhanced_members
    
    def fetch_profile_data(self, profile_url: str) -> Dict[str, Any]:
        """å€‹åˆ¥è­°å“¡ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        profile_data = {}
        
        try:
            response = self.session.get(profile_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å†™çœŸURLæŠ½å‡º
            photo_img = soup.find('img', src=re.compile(r'photo|picture'))
            if photo_img:
                photo_src = photo_img.get('src', '')
                if photo_src.startswith('/'):
                    profile_data['photo_url'] = self.base_url + photo_src
                elif photo_src.startswith('http'):
                    profile_data['photo_url'] = photo_src
            
            # çµŒæ­´æƒ…å ±æŠ½å‡º
            career_elements = soup.find_all(string=re.compile(r'ç”Ÿã¾ã‚Œ|å’æ¥­|çµŒæ­´'))
            if career_elements:
                career_text = ' '.join([elem.strip() for elem in career_elements[:3]])
                profile_data['career'] = career_text[:500]  # é•·ã™ãã‚‹å ´åˆã¯åˆ¶é™
            
            # ãã®ä»–ã®è©³ç´°æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
            
        except Exception as e:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼ ({profile_url}): {e}")
        
        return profile_data
    
    def save_members_data(self, members: List[Dict[str, Any]]):
        """è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if not members:
            logger.warning("ä¿å­˜ã™ã‚‹è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã‚’åŸºæº–ã¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«å
        current_date = datetime.now()
        data_period = current_date.strftime('%Y%m01')
        timestamp = current_date.strftime('%H%M%S')
        
        # è¡†è­°é™¢å°‚ç”¨ãƒ•ã‚¡ã‚¤ãƒ«å
        shugiin_filename = f"shugiin_legislators_{data_period}_{timestamp}.json"
        shugiin_filepath = self.raw_data_dir / shugiin_filename
        
        # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆè¡†å‚çµ±åˆç”¨ï¼‰
        unified_filename = f"legislators_{data_period}_{timestamp}.json"
        
        # æ—¢å­˜ã®å‚è­°é™¢ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        existing_sangiin = []
        existing_files = list(self.legislators_dir.glob("legislators_*.json"))
        if existing_files:
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            latest_file = sorted(existing_files)[-1]
            try:
                with open(latest_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    if isinstance(existing_data, dict) and 'data' in existing_data:
                        # å‚è­°é™¢ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                        existing_sangiin = [leg for leg in existing_data['data'] if leg.get('house') == 'sangiin']
                    elif isinstance(existing_data, list):
                        existing_sangiin = [leg for leg in existing_data if leg.get('house') == 'sangiin']
            except Exception as e:
                logger.warning(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # è¡†å‚çµ±åˆãƒ‡ãƒ¼ã‚¿
        unified_members = existing_sangiin + members
        
        # è¡†è­°é™¢å°‚ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        shugiin_data = {
            "metadata": {
                "house": "shugiin",
                "data_type": "legislators",
                "total_count": len(members),
                "generated_at": current_date.isoformat(),
                "source_url": self.members_url,
                "source_attribution": "House of Representatives - Japan",
                "data_quality": "official_shugiin_data"
            },
            "data": members
        }
        
        with open(shugiin_filepath, 'w', encoding='utf-8') as f:
            json.dump(shugiin_data, f, ensure_ascii=False, indent=2)
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ï¼‰
        unified_filepath = self.legislators_dir / unified_filename
        unified_data = {
            "metadata": {
                "data_type": "legislators_unified",
                "houses": ["shugiin", "sangiin"],
                "total_count": len(unified_members),
                "shugiin_count": len(members),
                "sangiin_count": len(existing_sangiin),
                "generated_at": current_date.isoformat(),
                "source": "unified_legislators_collection"
            },
            "data": unified_members
        }
        
        with open(unified_filepath, 'w', encoding='utf-8') as f:
            json.dump(unified_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  - è¡†è­°é™¢å°‚ç”¨: {shugiin_filepath}")
        logger.info(f"  - è¡†å‚çµ±åˆ: {unified_filepath}")
        logger.info(f"  - è¡†è­°é™¢ä»¶æ•°: {len(members)}å")
        logger.info(f"  - å‚è­°é™¢ä»¶æ•°: {len(existing_sangiin)}å")
        logger.info(f"  - çµ±åˆä»¶æ•°: {len(unified_members)}å")
        
        # çµ±è¨ˆè¡¨ç¤º
        self.display_collection_stats(members)
    
    def display_collection_stats(self, members: List[Dict[str, Any]]):
        """åé›†çµ±è¨ˆã‚’è¡¨ç¤º"""
        logger.info("\nğŸ“Š è¡†è­°é™¢è­°å“¡åé›†çµ±è¨ˆ:")
        
        # æ”¿å…šåˆ¥é›†è¨ˆ
        party_counts = {}
        for member in members:
            party = member['party']
            party_counts[party] = party_counts.get(party, 0) + 1
        
        logger.info("æ”¿å…šåˆ¥è­°å“¡æ•°:")
        for party, count in sorted(party_counts.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  {party}: {count}å")
        
        # é¸æŒ™åŒºã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
        constituency_types = {}
        for member in members:
            const_type = member['constituency_type']
            constituency_types[const_type] = constituency_types.get(const_type, 0) + 1
        
        logger.info("\né¸æŒ™åŒºã‚¿ã‚¤ãƒ—åˆ¥:")
        for const_type, count in constituency_types.items():
            logger.info(f"  {const_type}: {count}å")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹ (Issue #16)")
    
    collector = ShugiinLegislatorsCollector()
    
    try:
        # è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿åé›†
        members = collector.collect_members_data()
        
        if not members:
            logger.error("è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã®å¼·åŒ–
        enhanced_members = collector.enhance_with_profile_data(members)
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        collector.save_members_data(enhanced_members)
        
        logger.info("âœ¨ è¡†è­°é™¢è­°å“¡ãƒ‡ãƒ¼ã‚¿åé›†å‡¦ç†å®Œäº† (Issue #16)")
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()