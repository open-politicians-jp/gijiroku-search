#!/usr/bin/env python3
"""
Go2senkyo.comå…¨47éƒ½é“åºœçœŒå®Œå…¨åé›†
æ§‹é€ åŒ–æŠ½å‡ºã§é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import json
import logging
import requests
import time
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import random

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CompleteGo2senkyoCollector:
    def __init__(self):
        self.session = requests.Session()
        ua = UserAgent()
        desktop_ua = ua.random
        
        self.session.headers.update({
            'User-Agent': desktop_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        # å…¨éƒ½é“åºœçœŒãƒãƒƒãƒ”ãƒ³ã‚°
        self.prefectures = {
            1: "åŒ—æµ·é“", 2: "é’æ£®çœŒ", 3: "å²©æ‰‹çœŒ", 4: "å®®åŸçœŒ", 5: "ç§‹ç”°çœŒ", 6: "å±±å½¢çœŒ", 7: "ç¦å³¶çœŒ",
            8: "èŒ¨åŸçœŒ", 9: "æ ƒæœ¨çœŒ", 10: "ç¾¤é¦¬çœŒ", 11: "åŸ¼ç‰çœŒ", 12: "åƒè‘‰çœŒ", 13: "æ±äº¬éƒ½", 14: "ç¥å¥ˆå·çœŒ",
            15: "æ–°æ½ŸçœŒ", 16: "å¯Œå±±çœŒ", 17: "çŸ³å·çœŒ", 18: "ç¦äº•çœŒ", 19: "å±±æ¢¨çœŒ", 20: "é•·é‡çœŒ", 21: "å²é˜œçœŒ",
            22: "é™å²¡çœŒ", 23: "æ„›çŸ¥çœŒ", 24: "ä¸‰é‡çœŒ", 25: "æ»‹è³€çœŒ", 26: "äº¬éƒ½åºœ", 27: "å¤§é˜ªåºœ", 28: "å…µåº«çœŒ",
            29: "å¥ˆè‰¯çœŒ", 30: "å’Œæ­Œå±±çœŒ", 31: "é³¥å–çœŒ", 32: "å³¶æ ¹çœŒ", 33: "å²¡å±±çœŒ", 34: "åºƒå³¶çœŒ", 35: "å±±å£çœŒ",
            36: "å¾³å³¶çœŒ", 37: "é¦™å·çœŒ", 38: "æ„›åª›çœŒ", 39: "é«˜çŸ¥çœŒ", 40: "ç¦å²¡çœŒ", 41: "ä½è³€çœŒ", 42: "é•·å´çœŒ",
            43: "ç†Šæœ¬çœŒ", 44: "å¤§åˆ†çœŒ", 45: "å®®å´çœŒ", 46: "é¹¿å…å³¶çœŒ", 47: "æ²–ç¸„çœŒ"
        }
        
        # æ”¿å…šãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
        self.parties = [
            "è‡ªç”±æ°‘ä¸»å…š", "ç«‹æ†²æ°‘ä¸»å…š", "æ—¥æœ¬ç¶­æ–°ã®ä¼š", "å…¬æ˜å…š", "æ—¥æœ¬å…±ç”£å…š",
            "å›½æ°‘æ°‘ä¸»å…š", "ã‚Œã„ã‚æ–°é¸çµ„", "å‚æ”¿å…š", "ç¤¾ä¼šæ°‘ä¸»å…š", "NHKå…š",
            "æ—¥æœ¬ä¿å®ˆå…š", "æ—¥æœ¬æ”¹é©å…š", "ç„¡æ‰€å±", "ç„¡æ‰€å±é€£åˆ", "æ—¥æœ¬èª çœŸä¼š",
            "æ—¥æœ¬ã®å®¶åº­ã‚’å®ˆã‚‹ä¼š", "å†ç”Ÿã®é“", "å·®åˆ¥æ’²æ»…å…š", "æ ¸èåˆå…š", "ãƒãƒ¼ãƒ ã¿ã‚‰ã„",
            "å¤šå¤«å¤šå¦»å…š", "å›½æ”¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ä¼š", "æ–°å…šã‚„ã¾ã¨", "æœªåˆ†é¡"
        ]

    def collect_all_prefectures_complete(self):
        """å…¨47éƒ½é“åºœçœŒã®å®Œå…¨åé›†"""
        logger.info("ğŸš€ Go2senkyo.comå…¨47éƒ½é“åºœçœŒå®Œå…¨åé›†é–‹å§‹...")
        
        all_results = []
        failed_prefectures = []
        success_count = 0
        
        # åŠ¹ç‡åŒ–ã®ãŸã‚ã€10çœŒãšã¤ãƒãƒƒãƒå‡¦ç†
        batch_size = 10
        prefecture_items = list(self.prefectures.items())
        
        for batch_start in range(0, len(prefecture_items), batch_size):
            batch_end = min(batch_start + batch_size, len(prefecture_items))
            current_batch = prefecture_items[batch_start:batch_end]
            
            logger.info(f"\nğŸ”„ ãƒãƒƒãƒ {batch_start//batch_size + 1}/{(len(prefecture_items)-1)//batch_size + 1}: {batch_start+1}-{batch_end}çœŒ")
            
            for pref_code, pref_name in current_batch:
                try:
                    logger.info(f"ğŸ“ [{pref_code}/47] {pref_name} åé›†ä¸­...")
                    
                    candidates = self.collect_prefecture_structured(pref_code)
                    
                    if candidates:
                        all_results.extend(candidates)
                        success_count += 1
                        logger.info(f"âœ… {pref_name}: {len(candidates)}å")
                    else:
                        logger.warning(f"âš ï¸ {pref_name}: å€™è£œè€…ãªã—")
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆæœ€å¾Œã®çœŒä»¥å¤–ï¼‰
                    if pref_code < 47:
                        time.sleep(1.5)
                    
                except Exception as e:
                    logger.error(f"âŒ {pref_name} åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                    failed_prefectures.append((pref_code, pref_name))
                    continue
            
            # ãƒãƒƒãƒé–“ã®ä¼‘æ†©
            if batch_end < len(prefecture_items):
                logger.info(f"â¸ï¸ ãƒãƒƒãƒé–“ä¼‘æ†©ï¼ˆ3ç§’ï¼‰...")
                time.sleep(3)
        
        logger.info(f"\nğŸ¯ å…¨éƒ½é“åºœçœŒåé›†å®Œäº†:")
        logger.info(f"  æˆåŠŸ: {success_count}/47çœŒ")
        logger.info(f"  ç·å€™è£œè€…: {len(all_results)}å")
        
        if failed_prefectures:
            logger.warning(f"âš ï¸ åé›†å¤±æ•—çœŒ: {len(failed_prefectures)}çœŒ")
            for pref_code, pref_name in failed_prefectures:
                logger.warning(f"  - {pref_name}")
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        self.check_duplicates_structured(all_results)
        
        # çµæœä¿å­˜
        saved_file = self.save_complete_results(all_results, success_count, failed_prefectures)
        
        return saved_file

    def collect_prefecture_structured(self, pref_code: int):
        """æ§‹é€ åŒ–ã•ã‚ŒãŸéƒ½é“åºœçœŒãƒ‡ãƒ¼ã‚¿åé›†"""
        pref_name = self.prefectures.get(pref_code, f"æœªçŸ¥_{pref_code}")
        url = f"https://sangiin.go2senkyo.com/2025/prefecture/{pref_code}"
        
        try:
            # åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                logger.debug(f"âŒ {pref_name} ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            # é…å»¶ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ
            time.sleep(2)  # çŸ­ç¸®ç‰ˆ
            
            # å†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ•ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
            response2 = self.session.get(url, timeout=30)
            soup = BeautifulSoup(response2.text, 'html.parser')
            
            # æ§‹é€ åŒ–æŠ½å‡ºã®å®Ÿè¡Œ
            candidates = self.extract_candidates_structured(soup, pref_name, url)
            
            return candidates
            
        except Exception as e:
            logger.debug(f"âŒ {pref_name} ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def extract_candidates_structured(self, soup: BeautifulSoup, prefecture: str, page_url: str):
        """æ§‹é€ åŒ–ã•ã‚ŒãŸå€™è£œè€…æŠ½å‡º"""
        candidates = []
        
        try:
            # å€™è£œè€…ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ç´¢
            candidate_blocks = soup.find_all('div', class_='p_senkyoku_list_block')
            
            for i, block in enumerate(candidate_blocks):
                try:
                    candidate = self.extract_candidate_from_structured_block(block, prefecture, page_url, i)
                    if candidate:
                        candidates.append(candidate)
                    
                except Exception as e:
                    logger.debug(f"ãƒ–ãƒ­ãƒƒã‚¯ {i} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            logger.debug(f"{prefecture} æ§‹é€ åŒ–æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def extract_candidate_from_structured_block(self, block, prefecture: str, page_url: str, index: int):
        """æ§‹é€ åŒ–ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": "",
                "name": "",
                "name_kana": "",
                "prefecture": prefecture,
                "constituency": prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', ''),
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "profile_url": "",
                "source_page": page_url,
                "source": "go2senkyo_structured_complete",
                "collected_at": datetime.now().isoformat()
            }
            
            # 1. åå‰ã¨èª­ã¿ã®æŠ½å‡º
            name_info = self.extract_name_and_kana_structured(block)
            if name_info:
                candidate["name"] = name_info["name"]
                candidate["name_kana"] = name_info["kana"]
            
            # 2. æ”¿å…šã®æŠ½å‡º
            party = self.extract_party_from_block(block)
            if party:
                candidate["party"] = party
                candidate["party_normalized"] = party
            
            # 3. ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLã®æŠ½å‡º
            profile_url = self.extract_profile_url_from_block(block)
            if profile_url:
                candidate["profile_url"] = profile_url
                # candidate_idã‚’URLã‹ã‚‰æŠ½å‡º
                match = re.search(r'/seijika/(\\d+)', profile_url)
                if match:
                    candidate["candidate_id"] = f"go2s_{match.group(1)}"
            
            # åå‰ãŒå–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not candidate["name"]:
                return None
            
            return candidate
            
        except Exception as e:
            logger.debug(f"æ§‹é€ åŒ–ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def extract_name_and_kana_structured(self, block):
        """æ§‹é€ åŒ–ã•ã‚ŒãŸåå‰ãƒ»èª­ã¿æŠ½å‡º"""
        try:
            name_container = block.find('div', class_='p_senkyoku_list_block_text_name')
            if not name_container:
                return None
            
            name = ""
            kana = ""
            
            # åå‰ã®æŠ½å‡º (class="text")
            text_elem = name_container.find('p', class_='text')
            if text_elem:
                spans = text_elem.find_all('span')
                name = ''.join(span.get_text() for span in spans).strip()
            
            # èª­ã¿ã®æŠ½å‡º (class="kana")
            kana_elem = name_container.find('p', class_='kana')
            if kana_elem:
                spans = kana_elem.find_all('span')
                kana = ''.join(span.get_text() for span in spans).strip()
            
            if name:
                return {"name": name, "kana": kana}
            
        except Exception as e:
            logger.debug(f"æ§‹é€ åŒ–åå‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return None

    def extract_party_from_block(self, block):
        """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰æ”¿å…šæŠ½å‡º"""
        try:
            party_elem = block.find('div', class_='p_senkyoku_list_block_text_party')
            if party_elem:
                party_text = party_elem.get_text(strip=True)
                
                # æ”¿å…šãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆã¨ãƒãƒƒãƒãƒ³ã‚°
                for party in self.parties:
                    if party in party_text:
                        return party
                
                # ãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆã«ãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
                if party_text:
                    return party_text
            
        except Exception as e:
            logger.debug(f"æ”¿å…šæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return "æœªåˆ†é¡"

    def extract_profile_url_from_block(self, block):
        """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLæŠ½å‡º"""
        try:
            links = block.find_all('a', href=True)
            for link in links:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                if '/seijika/' in href and 'è©³ç´°ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«' in link_text:
                    return href
            
        except Exception as e:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return ""

    def check_duplicates_structured(self, candidates):
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
        logger.info("ğŸ” é‡è¤‡ãƒã‚§ãƒƒã‚¯:")
        
        # candidate_idãƒ™ãƒ¼ã‚¹ã®é‡è¤‡
        id_counts = {}
        for candidate in candidates:
            candidate_id = candidate.get('candidate_id', '')
            if candidate_id:
                id_counts[candidate_id] = id_counts.get(candidate_id, 0) + 1
        
        id_duplicates = {k: v for k, v in id_counts.items() if v > 1}
        if id_duplicates:
            logger.warning("âš ï¸ IDé‡è¤‡ç™ºè¦‹:")
            for cid, count in id_duplicates.items():
                logger.warning(f"  {cid}: {count}å›")
        else:
            logger.info("âœ… IDé‡è¤‡ãªã—")
        
        # åå‰+éƒ½é“åºœçœŒãƒ™ãƒ¼ã‚¹ã®é‡è¤‡
        name_counts = {}
        for candidate in candidates:
            name = candidate.get('name', '')
            prefecture = candidate.get('prefecture', '')
            key = f"{name}_{prefecture}"
            name_counts[key] = name_counts.get(key, 0) + 1
        
        name_duplicates = {k: v for k, v in name_counts.items() if v > 1}
        if name_duplicates:
            logger.warning("âš ï¸ åå‰é‡è¤‡ç™ºè¦‹:")
            for name_pref, count in name_duplicates.items():
                logger.warning(f"  {name_pref}: {count}å›")
        else:
            logger.info("âœ… åå‰é‡è¤‡ãªã—")

    def save_complete_results(self, candidates, success_count, failed_prefectures):
        """å®Œå…¨åé›†çµæœã®ä¿å­˜"""
        logger.info("ğŸ’¾ å®Œå…¨åé›†çµæœã®ä¿å­˜...")
        
        # çµ±è¨ˆè¨ˆç®—
        party_stats = {}
        prefecture_stats = {}
        with_kana_count = 0
        
        for candidate in candidates:
            party = candidate.get('party', 'æœªåˆ†é¡')
            prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
            
            party_stats[party] = party_stats.get(party, 0) + 1
            prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
            
            if candidate.get('name_kana'):
                with_kana_count += 1
        
        # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        data = {
            "metadata": {
                "data_type": "go2senkyo_complete_structured_sangiin_2025",
                "collection_method": "structured_html_extraction_all_47_prefectures",
                "total_candidates": len(candidates),
                "candidates_with_kana": with_kana_count,
                "successful_prefectures": success_count,
                "failed_prefectures": len(failed_prefectures),
                "generated_at": datetime.now().isoformat(),
                "source_site": "sangiin.go2senkyo.com",
                "coverage": {
                    "constituency_types": 1,
                    "parties": len(party_stats),
                    "prefectures": len(prefecture_stats)
                }
            },
            "statistics": {
                "by_party": party_stats,
                "by_prefecture": prefecture_stats,
                "by_constituency_type": {"single_member": len(candidates)}
            },
            "data": candidates
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        complete_file = data_dir / f"go2senkyo_complete_structured_{timestamp}.json"
        latest_file = data_dir / "go2senkyo_optimized_latest.json"
        
        with open(complete_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # åˆç†çš„ãªå€™è£œè€…æ•°ã®å ´åˆã®ã¿æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        if len(candidates) >= 100 and len(prefecture_stats) >= 30:
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {latest_file}")
        
        logger.info(f"ğŸ“ å®Œå…¨åé›†çµæœä¿å­˜: {complete_file}")
        
        # çµ±è¨ˆè¡¨ç¤º
        logger.info("\nğŸ“Š Go2senkyoå®Œå…¨åé›†çµ±è¨ˆ:")
        logger.info(f"  ç·å€™è£œè€…: {len(candidates)}å")
        logger.info(f"  èª­ã¿ä»˜ã: {with_kana_count}å ({with_kana_count/len(candidates)*100:.1f}%)")
        logger.info(f"  æˆåŠŸçœŒ: {success_count}/47çœŒ ({success_count/47*100:.1f}%)")
        logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
        logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_stats)}éƒ½é“åºœçœŒ")
        
        # ä¸»è¦æ”¿å…šã®ç¢ºèª
        major_parties = dict(sorted(party_stats.items(), key=lambda x: x[1], reverse=True)[:10])
        logger.info("\nğŸ“ˆ ä¸»è¦æ”¿å…šåˆ¥å€™è£œè€…æ•°:")
        for party, count in major_parties.items():
            logger.info(f"  {party}: {count}å")
        
        # ä¸»è¦éƒ½é“åºœçœŒã®ç¢ºèª
        major_prefs = dict(sorted(prefecture_stats.items(), key=lambda x: x[1], reverse=True)[:15])
        logger.info("\nğŸ“ ä¸»è¦éƒ½é“åºœçœŒåˆ¥å€™è£œè€…æ•°:")
        for pref, count in major_prefs.items():
            logger.info(f"  {pref}: {count}å")
        
        return complete_file

    def check_if_update_needed(self) -> bool:
        """7æ—¥é–“éš”ã§ã®æ›´æ–°å¿…è¦æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
            latest_file = data_dir / "go2senkyo_optimized_latest.json"
            
            if not latest_file.exists():
                logger.info("ğŸ“… æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„ãŸã‚æ›´æ–°å®Ÿè¡Œ")
                return True
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚æ›´æ–°æ—¥ã‚’å–å¾—
            last_modified = datetime.fromtimestamp(latest_file.stat().st_mtime)
            now = datetime.now()
            days_since_update = (now - last_modified).days
            
            if days_since_update >= 7:
                logger.info(f"ğŸ“… å‰å›æ›´æ–°ã‹ã‚‰{days_since_update}æ—¥çµŒé: æ›´æ–°å®Ÿè¡Œ")
                return True
            else:
                logger.info(f"â­ï¸ å‰å›æ›´æ–°ã‹ã‚‰{days_since_update}æ—¥: æ›´æ–°ã‚¹ã‚­ãƒƒãƒ—")
                return False
                
        except Exception as e:
            logger.warning(f"æ›´æ–°ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ›´æ–°å®Ÿè¡Œ")
            return True

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Go2senkyo.comå…¨47éƒ½é“åºœçœŒå®Œå…¨åé›†')
    parser.add_argument('--force-update', action='store_true', help='7æ—¥é–“éš”ã‚’ç„¡è¦–ã—ã¦å¼·åˆ¶æ›´æ–°')
    parser.add_argument('--test-mode', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆæ±äº¬ãƒ»å¤§é˜ªãƒ»ç¥å¥ˆå·ã®ã¿ï¼‰')
    parser.add_argument('--max-candidates', type=int, default=1000, help='æœ€å¤§å€™è£œè€…æ•°åˆ¶é™')
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ Go2senkyo.comå‚é™¢é¸å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
    
    collector = CompleteGo2senkyoCollector()
    
    # å¼·åˆ¶æ›´æ–°ãƒ•ãƒ©ã‚°ãŒãªã„å ´åˆã¯7æ—¥é–“éš”ãƒã‚§ãƒƒã‚¯
    if not args.force_update and not collector.check_if_update_needed():
        logger.info("ğŸ“ 7æ—¥é–“éš”ãƒã‚§ãƒƒã‚¯ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—")
        return
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯å¯¾è±¡éƒ½é“åºœçœŒã‚’é™å®š
    if args.test_mode:
        logger.info("ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: æ±äº¬ãƒ»å¤§é˜ªãƒ»ç¥å¥ˆå·ã®ã¿åé›†")
        original_prefectures = collector.prefectures.copy()
        collector.prefectures = {13: "æ±äº¬éƒ½", 27: "å¤§é˜ªåºœ", 14: "ç¥å¥ˆå·çœŒ"}
    
    result_file = collector.collect_all_prefectures_complete()
    
    if result_file:
        logger.info(f"âœ… å…¨47éƒ½é“åºœçœŒåé›†å®Œäº†: {result_file}")
    else:
        logger.error("âŒ åé›†å¤±æ•—")

if __name__ == "__main__":
    main()