#!/usr/bin/env python3
"""
å®Ÿéš›ã®æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä¸»è¦æ”¿å…šã®å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åé›†
"""

import json
import requests
import time
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ManifestoCollector:
    """æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.frontend_manifestos_dir = self.project_root / "frontend" / "public" / "data" / "manifestos"
        self.frontend_manifestos_dir.mkdir(parents=True, exist_ok=True)
        
        # æ”¿å…šæƒ…å ±
        self.parties = {
            "è‡ªç”±æ°‘ä¸»å…š": {
                "url": "https://www.jimin.jp/policy/pamphlet/",
                "backup_url": "https://www.jimin.jp/policy/manifest/",
                "aliases": ["è‡ªæ°‘å…š", "è‡ªæ°‘", "LDP"]
            },
            "ç«‹æ†²æ°‘ä¸»å…š": {
                "url": "https://cdp-japan.jp/policies/5pillars",
                "backup_url": "https://cdp-japan.jp/",
                "aliases": ["ç«‹æ°‘", "ç«‹æ†²"]
            },
            "æ—¥æœ¬ç¶­æ–°ã®ä¼š": {
                "url": "https://o-ishin.jp/policy/",
                "backup_url": "https://o-ishin.jp/",
                "aliases": ["ç¶­æ–°", "ç¶­æ–°ã®ä¼š", "å¤§é˜ªç¶­æ–°"]
            },
            "å…¬æ˜å…š": {
                "url": "https://www.komei.or.jp/policy/",
                "backup_url": "https://www.komei.or.jp/",
                "aliases": ["å…¬æ˜"]
            },
            "æ—¥æœ¬å…±ç”£å…š": {
                "url": "https://www.jcp.or.jp/web_policy/",
                "backup_url": "https://www.jcp.or.jp/",
                "aliases": ["å…±ç”£å…š", "å…±ç”£", "JCP"]
            },
            "å›½æ°‘æ°‘ä¸»å…š": {
                "url": "https://new-kokumin.jp/policies",
                "backup_url": "https://new-kokumin.jp/",
                "aliases": ["å›½æ°‘"]
            },
            "ã‚Œã„ã‚æ–°é¸çµ„": {
                "url": "https://reiwa-shinsengumi.com/policy/",
                "backup_url": "https://reiwa-shinsengumi.com/",
                "aliases": ["ã‚Œã„ã‚"]
            },
            "å‚æ”¿å…š": {
                "url": "https://www.sanseito.jp/policy/",
                "backup_url": "https://www.sanseito.jp/",
                "aliases": ["å‚æ”¿"]
            }
        }
        
    def update_headers(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°"""
        headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache'
        }
        self.session.headers.update(headers)
        
    def fetch_page_content(self, url: str) -> Optional[str]:
        """ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
        try:
            time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡ºãƒ»ä¿®æ­£
            response.encoding = response.apparent_encoding or 'utf-8'
            
            return response.text
        except Exception as e:
            logger.error(f"âŒ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
            
    def extract_policy_content(self, html: str, party_name: str) -> List[Dict[str, Any]]:
        """HTMLã‹ã‚‰æ”¿ç­–å†…å®¹ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'html.parser')
        policies = []
        
        # å…±é€šçš„ãªæ”¿ç­–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        policy_selectors = [
            'div.policy', 'div.manifesto', 'div.content',
            'section.policy', 'section.manifesto',
            'article', 'main', '.policy-content',
            'h2, h3, h4',  # è¦‹å‡ºã—ãƒ™ãƒ¼ã‚¹
            'p'  # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹
        ]
        
        content_text = ""
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
        title_elem = soup.find('title')
        page_title = title_elem.text.strip() if title_elem else f"{party_name}æ”¿ç­–"
        
        # ãƒ¡ã‚¤ãƒ³å†…å®¹ã‚’æŠ½å‡º
        for selector in policy_selectors:
            elements = soup.select(selector)
            if elements:
                for elem in elements[:10]:  # æœ€åˆã®10è¦ç´ ã¾ã§
                    text = elem.get_text(strip=True)
                    if text and len(text) > 20:  # çŸ­ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
                        content_text += text + "\n\n"
                        
        # å†…å®¹ãŒå°‘ãªã„å ´åˆã¯bodyå…¨ä½“ã‹ã‚‰æŠ½å‡º
        if len(content_text) < 200:
            body = soup.find('body')
            if body:
                content_text = body.get_text(separator='\n', strip=True)
                
        # ä¸è¦ãªéƒ¨åˆ†ã‚’é™¤å»
        content_text = self.clean_policy_text(content_text)
        
        if content_text:
            policies.append({
                "party": party_name,
                "title": page_title,
                "year": datetime.now().year,
                "category": "æ”¿ç­–ç¶±é ˜",
                "content": content_text[:2000],  # 2000æ–‡å­—ã¾ã§
                "url": "",  # å¾Œã§è¨­å®š
                "collected_at": datetime.now().isoformat()
            })
            
        return policies
        
    def clean_policy_text(self, text: str) -> str:
        """æ”¿ç­–ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not text:
            return text
            
        # æ–‡å­—åŒ–ã‘ä¿®å¾©ï¼ˆä¸»ã«UTF-8é–¢é€£ï¼‰
        try:
            # ã‚ˆãã‚ã‚‹æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
            text = text.encode('latin1').decode('utf-8')
        except (UnicodeDecodeError, UnicodeEncodeError):
            # æ—¢ã«æ­£ã—ã„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å ´åˆã¯ãã®ã¾ã¾
            pass
            
        # æ”¹è¡Œã‚’æ•´ç†
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ä¸è¦ãªæ–‡å­—åˆ—ã‚’é™¤å»
        remove_patterns = [
            r'Copyright.*?All Rights Reserved',
            r'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼',
            r'ã‚µã‚¤ãƒˆãƒãƒƒãƒ—',
            r'ãŠå•ã„åˆã‚ã›',
            r'ãƒ¡ãƒ‹ãƒ¥ãƒ¼',
            r'ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³',
            r'ãƒ•ãƒƒã‚¿ãƒ¼',
            r'ãƒ˜ãƒƒãƒ€ãƒ¼',
            r'æ¤œç´¢',
            r'SNS.*?follow',
            r'JavaScript.*?æœ‰åŠ¹',
        ]
        
        for pattern in remove_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
            
        # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’æ•´ç†
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
        
    def collect_party_manifesto(self, party_name: str, party_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å€‹åˆ¥æ”¿å…šã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åé›†"""
        logger.info(f"ğŸ“„ {party_name}ã®æ”¿ç­–ã‚’åé›†ä¸­...")
        
        manifestos = []
        
        # ãƒ¡ã‚¤ãƒ³URLã‚’è©¦è¡Œ
        html = self.fetch_page_content(party_info["url"])
        if not html:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—URLã‚’è©¦è¡Œ
            logger.info(f"âš ï¸ ãƒ¡ã‚¤ãƒ³URLå¤±æ•—ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—URLã‚’è©¦è¡Œ: {party_name}")
            html = self.fetch_page_content(party_info["backup_url"])
            
        if html:
            policies = self.extract_policy_content(html, party_name)
            for policy in policies:
                policy["url"] = party_info["url"]
                policy["party_aliases"] = party_info["aliases"]
                manifestos.append(policy)
                
        if manifestos:
            logger.info(f"âœ… {party_name}: {len(manifestos)}ä»¶ã®æ”¿ç­–ã‚’åé›†")
        else:
            logger.warning(f"âš ï¸ {party_name}: æ”¿ç­–ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
        return manifestos
        
    def collect_all_manifestos(self) -> List[Dict[str, Any]]:
        """å…¨æ”¿å…šã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åé›†"""
        logger.info("ğŸš€ æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†é–‹å§‹...")
        
        all_manifestos = []
        
        for party_name, party_info in self.parties.items():
            try:
                manifestos = self.collect_party_manifesto(party_name, party_info)
                all_manifestos.extend(manifestos)
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                time.sleep(3)
                
            except Exception as e:
                logger.error(f"âŒ {party_name}ã®åé›†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                
        logger.info(f"ğŸ‰ åé›†å®Œäº†: åˆè¨ˆ {len(all_manifestos)}ä»¶")
        return all_manifestos
        
    def save_manifestos(self, manifestos: List[Dict[str, Any]]):
        """ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’ä¿å­˜"""
        if not manifestos:
            logger.warning("âš ï¸ ä¿å­˜ã™ã‚‹ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
            
        # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã‚’åŸºæº–ã¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆç¾åœ¨ã®å¹´æœˆ + æ™‚åˆ»ï¼‰
        current_date = datetime.now()
        data_period = current_date.strftime('%Y%m01')  # å½“æœˆã®ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
        timestamp = current_date.strftime('%H%M%S')
        filename = f"manifestos_{data_period}_{timestamp}.json"
        filepath = self.frontend_manifestos_dir / filename
        
        data = {
            "metadata": {
                "data_type": "manifestos",
                "total_count": len(manifestos),
                "generated_at": datetime.now().isoformat(),
                "source": "å„æ”¿å…šå…¬å¼ã‚µã‚¤ãƒˆ",
                "collection_method": "real_party_scraping"
            },
            "data": manifestos
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        file_size = filepath.stat().st_size / 1024
        logger.info(f"ğŸ’¾ ä¿å­˜å®Œäº†: {filename} ({file_size:.1f} KB)")
        
        # å¤ã„ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        self.remove_sample_files()
        
    def remove_sample_files(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        # å¤ã„ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä»Šæ—¥ä½œæˆã—ãŸæ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯é™¤å¤–ï¼‰
        current_date = datetime.now().strftime("%Y%m%d")
        
        all_files = list(self.frontend_manifestos_dir.glob("manifestos_*.json"))
        sample_files = []
        
        for file in all_files:
            filename = file.name
            # ä»Šæ—¥ã‚ˆã‚Šå‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã€ã¾ãŸã¯sampleã¨ã„ã†æ–‡å­—åˆ—ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤å¯¾è±¡ã¨ã™ã‚‹
            if ("sample" in filename.lower() or 
                (filename.startswith("manifestos_202506") and current_date not in filename)):
                sample_files.append(file)
        
        for sample_file in sample_files:
            try:
                sample_file.unlink()
                logger.info(f"ğŸ—‘ï¸ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {sample_file.name}")
            except Exception as e:
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    collector = ManifestoCollector()
    
    # å…¨æ”¿å…šã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åé›†
    manifestos = collector.collect_all_manifestos()
    
    # ä¿å­˜
    collector.save_manifestos(manifestos)
    
    logger.info("âœ¨ æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†å®Œäº†!")

if __name__ == "__main__":
    main()