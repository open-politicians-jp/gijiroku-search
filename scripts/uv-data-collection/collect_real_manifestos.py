#!/usr/bin/env python3
"""
å®Ÿéš›ã®æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä¸»è¦æ”¿å…šã®å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åé›†
é€±æ¬¡å®Ÿè¡Œå¯¾å¿œãƒ»éå»ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ©Ÿèƒ½ä»˜ã
"""

import json
import requests
import time
import re
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ManifestoCollector:
    """æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, weekly_mode=False, archive_mode=False):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        self.weekly_mode = weekly_mode
        self.archive_mode = archive_mode
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.frontend_manifestos_dir = self.project_root / "frontend" / "public" / "data" / "manifestos"
        self.frontend_manifestos_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        if self.archive_mode:
            self.archive_dir = self.frontend_manifestos_dir / "archive"
            self.archive_dir.mkdir(parents=True, exist_ok=True)
        
        # æ”¿å…šæƒ…å ±
        self.parties = {
            "è‡ªç”±æ°‘ä¸»å…š": {
                "url": "https://www.jimin.jp/policy/pamphlet/",
                "backup_url": "https://www.jimin.jp/policy/manifest/",
                "aliases": ["è‡ªæ°‘å…š", "è‡ªæ°‘", "LDP"]
            },
            "ç«‹æ†²æ°‘ä¸»å…š": {
                "url": "https://cdp-japan.jp/policies/5pillars",
                "backup_url": "https://cdp-japan.jp/",
                "aliases": ["ç«‹æ°‘", "ç«‹æ†²"]
            },
            "æ—¥æœ¬ç¶­æ–°ã®ä¼š": {
                "url": "https://o-ishin.jp/policy/",
                "backup_url": "https://o-ishin.jp/",
                "aliases": ["ç¶­æ–°", "ç¶­æ–°ã®ä¼š", "å¤§é˜ªç¶­æ–°"]
            },
            "å…¬æ˜å…š": {
                "url": "https://www.komei.or.jp/policy/",
                "backup_url": "https://www.komei.or.jp/",
                "aliases": ["å…¬æ˜"]
            },
            "æ—¥æœ¬å…±ç”£å…š": {
                "url": "https://www.jcp.or.jp/web_policy/",
                "backup_url": "https://www.jcp.or.jp/",
                "aliases": ["å…±ç”£å…š", "å…±ç”£", "JCP"]
            },
            "å›½æ°‘æ°‘ä¸»å…š": {
                "url": "https://new-kokumin.jp/policies",
                "backup_url": "https://new-kokumin.jp/",
                "aliases": ["å›½æ°‘"]
            },
            "ã‚Œã„ã‚æ–°é¸çµ„": {
                "url": "https://reiwa-shinsengumi.com/policy/",
                "backup_url": "https://reiwa-shinsengumi.com/",
                "aliases": ["ã‚Œã„ã‚"]
            },
            "å‚æ”¿å…š": {
                "url": "https://www.sanseito.jp/policy/",
                "backup_url": "https://www.sanseito.jp/",
                "aliases": ["å‚æ”¿"]
            }
        }
        
    def update_headers(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°"""
        headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache'
        }
        self.session.headers.update(headers)
        
    def fetch_page_content(self, url: str) -> Optional[str]:
        """ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
        try:
            time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡ºãƒ»ä¿®æ­£
            response.encoding = response.apparent_encoding or 'utf-8'
            
            return response.text
        except Exception as e:
            logger.error(f"âŒ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
            
    def extract_policy_content(self, html: str, party_name: str) -> List[Dict[str, Any]]:
        """HTMLã‹ã‚‰æ”¿ç­–å†…å®¹ã‚’æŠ½å‡º"""
        soup = BeautifulSoup(html, 'html.parser')
        policies = []
        
        # å…±é€šçš„ãªæ”¿ç­–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¢ã™
        policy_selectors = [
            'div.policy', 'div.manifesto', 'div.content',
            'section.policy', 'section.manifesto',
            'article', 'main', '.policy-content',
            'h2, h3, h4',  # è¦‹å‡ºã—ãƒ™ãƒ¼ã‚¹
            'p'  # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹
        ]
        
        content_text = ""
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
        title_elem = soup.find('title')
        page_title = title_elem.text.strip() if title_elem else f"{party_name}æ”¿ç­–"
        
        # ãƒ¡ã‚¤ãƒ³å†…å®¹ã‚’æŠ½å‡º
        for selector in policy_selectors:
            elements = soup.select(selector)
            if elements:
                for elem in elements[:10]:  # æœ€åˆã®10è¦ç´ ã¾ã§
                    text = elem.get_text(strip=True)
                    if text and len(text) > 20:  # çŸ­ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
                        content_text += text + "\n\n"
                        
        # å†…å®¹ãŒå°‘ãªã„å ´åˆã¯bodyå…¨ä½“ã‹ã‚‰æŠ½å‡º
        if len(content_text) < 200:
            body = soup.find('body')
            if body:
                content_text = body.get_text(separator='\n', strip=True)
                
        # ä¸è¦ãªéƒ¨åˆ†ã‚’é™¤å»
        content_text = self.clean_policy_text(content_text)
        
        if content_text:
            policies.append({
                "party": party_name,
                "title": page_title,
                "year": datetime.now().year,
                "category": "æ”¿ç­–ç¶±é ˜",
                "content": content_text[:2000],  # 2000æ–‡å­—ã¾ã§
                "url": "",  # å¾Œã§è¨­å®š
                "collected_at": datetime.now().isoformat()
            })
            
        return policies
        
    def clean_policy_text(self, text: str) -> str:
        """æ”¿ç­–ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not text:
            return text
            
        # æ–‡å­—åŒ–ã‘ä¿®å¾©ï¼ˆä¸»ã«UTF-8é–¢é€£ï¼‰
        try:
            # ã‚ˆãã‚ã‚‹æ–‡å­—åŒ–ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£
            text = text.encode('latin1').decode('utf-8')
        except (UnicodeDecodeError, UnicodeEncodeError):
            # æ—¢ã«æ­£ã—ã„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å ´åˆã¯ãã®ã¾ã¾
            pass
            
        # æ”¹è¡Œã‚’æ•´ç†
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ä¸è¦ãªæ–‡å­—åˆ—ã‚’é™¤å»
        remove_patterns = [
            r'Copyright.*?All Rights Reserved',
            r'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼',
            r'ã‚µã‚¤ãƒˆãƒãƒƒãƒ—',
            r'ãŠå•ã„åˆã‚ã›',
            r'ãƒ¡ãƒ‹ãƒ¥ãƒ¼',
            r'ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³',
            r'ãƒ•ãƒƒã‚¿ãƒ¼',
            r'ãƒ˜ãƒƒãƒ€ãƒ¼',
            r'æ¤œç´¢',
            r'SNS.*?follow',
            r'JavaScript.*?æœ‰åŠ¹',
        ]
        
        for pattern in remove_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
            
        # é€£ç¶šã™ã‚‹ç©ºç™½ã‚’æ•´ç†
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
        
    def collect_party_manifesto(self, party_name: str, party_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å€‹åˆ¥æ”¿å…šã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åé›†"""
        logger.info(f"ğŸ“„ {party_name}ã®æ”¿ç­–ã‚’åé›†ä¸­...")
        
        manifestos = []
        
        # ãƒ¡ã‚¤ãƒ³URLã‚’è©¦è¡Œ
        html = self.fetch_page_content(party_info["url"])
        if not html:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—URLã‚’è©¦è¡Œ
            logger.info(f"âš ï¸ ãƒ¡ã‚¤ãƒ³URLå¤±æ•—ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—URLã‚’è©¦è¡Œ: {party_name}")
            html = self.fetch_page_content(party_info["backup_url"])
            
        if html:
            policies = self.extract_policy_content(html, party_name)
            for policy in policies:
                policy["url"] = party_info["url"]
                policy["party_aliases"] = party_info["aliases"]
                manifestos.append(policy)
                
        if manifestos:
            logger.info(f"âœ… {party_name}: {len(manifestos)}ä»¶ã®æ”¿ç­–ã‚’åé›†")
        else:
            logger.warning(f"âš ï¸ {party_name}: æ”¿ç­–ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
        return manifestos
        
    def collect_all_manifestos(self) -> List[Dict[str, Any]]:
        """å…¨æ”¿å…šã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åé›†"""
        logger.info("ğŸš€ æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†é–‹å§‹...")
        
        all_manifestos = []
        
        for party_name, party_info in self.parties.items():
            try:
                manifestos = self.collect_party_manifesto(party_name, party_info)
                all_manifestos.extend(manifestos)
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                time.sleep(3)
                
            except Exception as e:
                logger.error(f"âŒ {party_name}ã®åé›†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                
        logger.info(f"ğŸ‰ åé›†å®Œäº†: åˆè¨ˆ {len(all_manifestos)}ä»¶")
        return all_manifestos
        
    def save_manifestos(self, manifestos: List[Dict[str, Any]]):
        """ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’ä¿å­˜"""
        if not manifestos:
            logger.warning("âš ï¸ ä¿å­˜ã™ã‚‹ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        current_date = datetime.now()
        
        # é€±æ¬¡ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯é€±ç•ªå·ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«å
        if self.weekly_mode:
            year = current_date.year
            week = current_date.isocalendar()[1]
            weekly_filename = f"manifestos_{year}_w{week:02d}.json"
            weekly_filepath = self.frontend_manifestos_dir / weekly_filename
        
        # é€šå¸¸ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å
        filename = f"manifestos_{timestamp}.json"
        filepath = self.frontend_manifestos_dir / filename
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æº–å‚™
        metadata = {
            "data_type": "manifestos",
            "total_count": len(manifestos),
            "generated_at": current_date.isoformat(),
            "source": "å„æ”¿å…šå…¬å¼ã‚µã‚¤ãƒˆ",
            "collection_method": "weekly_automated_collection" if self.weekly_mode else "real_party_scraping"
        }
        
        if self.weekly_mode:
            metadata.update({
                "collection_week": f"{current_date.year}-W{current_date.isocalendar()[1]:02d}",
                "archive_enabled": self.archive_mode
            })
        
        data = {
            "metadata": metadata,
            "data": manifestos
        }
        
        # é€šå¸¸ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        file_size = filepath.stat().st_size / 1024
        logger.info(f"ğŸ’¾ ä¿å­˜å®Œäº†: {filename} ({file_size:.1f} KB)")
        
        # é€±æ¬¡ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯é€±æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜
        if self.weekly_mode and weekly_filepath != filepath:
            with open(weekly_filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“… é€±æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {weekly_filename}")
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯éå»ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒ
        if self.archive_mode:
            self.archive_old_files()
            logger.info("ğŸ“š ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ©Ÿèƒ½æœ‰åŠ¹: éå»ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ")
        
        # latest ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
        latest_filepath = self.frontend_manifestos_dir / "manifestos_latest.json"
        with open(latest_filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info("ğŸ”„ latest ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°å®Œäº†")
        
    def archive_old_files(self):
        """å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•"""
        if not self.archive_mode:
            return
            
        # 2æ—¥ä»¥ä¸Šå¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ç§»å‹•
        cutoff_date = datetime.now() - timedelta(days=2)
        archived_count = 0
        
        all_files = list(self.frontend_manifestos_dir.glob("manifestos_*.json"))
        
        for file in all_files:
            # latest ãƒ•ã‚¡ã‚¤ãƒ«ã¨é€±æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ã¯é™¤å¤–
            if file.name in ["manifestos_latest.json"] or "_w" in file.name:
                continue
                
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆæ—¥æ™‚ã‚’ãƒã‚§ãƒƒã‚¯
            try:
                file_mtime = datetime.fromtimestamp(file.stat().st_mtime)
                if file_mtime < cutoff_date:
                    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
                    archive_path = self.archive_dir / file.name
                    file.rename(archive_path)
                    logger.info(f"ğŸ“š ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç§»å‹•: {file.name}")
                    archived_count += 1
            except Exception as e:
                logger.error(f"âŒ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚¨ãƒ©ãƒ¼ {file.name}: {e}")
        
        if archived_count > 0:
            logger.info(f"ğŸ“š {archived_count}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°è§£æ
    parser = argparse.ArgumentParser(description='æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--weekly', action='store_true', help='é€±æ¬¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆé€±ç•ªå·ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼‰')
    parser.add_argument('--archive', action='store_true', help='ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ¢ãƒ¼ãƒ‰ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ä¿æŒï¼‰')
    parser.add_argument('--no-cleanup', action='store_true', help='ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚’ç„¡åŠ¹åŒ–ï¼ˆéæ¨å¥¨ï¼‰')
    
    args = parser.parse_args()
    
    # åé›†å™¨ã‚’åˆæœŸåŒ–
    collector = ManifestoCollector(
        weekly_mode=args.weekly,
        archive_mode=args.archive
    )
    
    logger.info(f"ğŸ“„ ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†é–‹å§‹...")
    logger.info(f"ğŸ“… é€±æ¬¡ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if args.weekly else 'ç„¡åŠ¹'}")
    logger.info(f"ğŸ“š ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if args.archive else 'ç„¡åŠ¹'}")
    
    # å…¨æ”¿å…šã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã‚’åé›†
    manifestos = collector.collect_all_manifestos()
    
    # ä¿å­˜
    collector.save_manifestos(manifestos)
    
    logger.info("âœ¨ æ”¿å…šãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†å®Œäº†!")

if __name__ == "__main__":
    main()