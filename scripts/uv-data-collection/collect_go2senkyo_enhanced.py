#!/usr/bin/env python3
"""
Go2senkyo.comå‚è­°é™¢é¸2025ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆé¸æŒ™åŒºåˆ¥å¼·åŒ–ç‰ˆï¼‰

https://sangiin.go2senkyo.com/2025/prefecture/{prefecture_code} 
å„éƒ½é“åºœçœŒé¸æŒ™åŒºã®è©³ç´°ãƒ‡ãƒ¼ã‚¿åé›†ã¨å€™è£œè€…å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

æ©Ÿèƒ½:
- éƒ½é“åºœçœŒåˆ¥é¸æŒ™åŒºãƒ‡ãƒ¼ã‚¿åé›†
- å€™è£œè€…è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ã®åŒ…æ‹¬çš„æƒ…å ±å–å¾—
- HPã€SNSã€æ”¿ç­–ãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆåé›†
- ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å†™çœŸãƒ»çµŒæ­´æƒ…å ±
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é¸æŒ™æƒ…å‹¢ãƒ‡ãƒ¼ã‚¿
"""

import json
import requests
import time
import re
import random
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin, urlparse, parse_qs

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Go2senkyoEnhancedCollector:
    """Go2senkyo.com å‚é™¢é¸2025å¼·åŒ–ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.output_dir = self.project_root / "frontend" / "public" / "data" / "sangiin_candidates"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Go2senkyoåŸºæœ¬URLè¨­å®š
        self.base_url = "https://sangiin.go2senkyo.com"
        self.sangiin_2025_url = f"{self.base_url}/2025"
        
        # éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆGo2senkyoç”¨ï¼‰
        self.prefecture_codes = {
            "åŒ—æµ·é“": 1, "é’æ£®çœŒ": 2, "å²©æ‰‹çœŒ": 3, "å®®åŸçœŒ": 4, "ç§‹ç”°çœŒ": 5,
            "å±±å½¢çœŒ": 6, "ç¦å³¶çœŒ": 7, "èŒ¨åŸçœŒ": 8, "æ ƒæœ¨çœŒ": 9, "ç¾¤é¦¬çœŒ": 10,
            "åŸ¼ç‰çœŒ": 11, "åƒè‘‰çœŒ": 12, "æ±äº¬éƒ½": 13, "ç¥å¥ˆå·çœŒ": 14, "æ–°æ½ŸçœŒ": 15,
            "å¯Œå±±çœŒ": 16, "çŸ³å·çœŒ": 17, "ç¦äº•çœŒ": 18, "å±±æ¢¨çœŒ": 19, "é•·é‡çœŒ": 20,
            "å²é˜œçœŒ": 21, "é™å²¡çœŒ": 22, "æ„›çŸ¥çœŒ": 23, "ä¸‰é‡çœŒ": 24, "æ»‹è³€çœŒ": 25,
            "äº¬éƒ½åºœ": 26, "å¤§é˜ªåºœ": 27, "å…µåº«çœŒ": 28, "å¥ˆè‰¯çœŒ": 29, "å’Œæ­Œå±±çœŒ": 30,
            "é³¥å–çœŒ": 31, "å³¶æ ¹çœŒ": 32, "å²¡å±±çœŒ": 33, "åºƒå³¶çœŒ": 34, "å±±å£çœŒ": 35,
            "å¾³å³¶çœŒ": 36, "é¦™å·çœŒ": 37, "æ„›åª›çœŒ": 38, "é«˜çŸ¥çœŒ": 39, "ç¦å²¡çœŒ": 40,
            "ä½è³€çœŒ": 41, "é•·å´çœŒ": 42, "ç†Šæœ¬çœŒ": 43, "å¤§åˆ†çœŒ": 44, "å®®å´çœŒ": 45,
            "é¹¿å…å³¶çœŒ": 46, "æ²–ç¸„çœŒ": 47
        }
        
        # æ”¹é¸è­°å¸­æ•°ï¼ˆ2025å¹´å‚é™¢é¸ï¼‰
        self.sangiin_seats = {
            "åŒ—æµ·é“": 1, "é’æ£®çœŒ": 1, "å²©æ‰‹çœŒ": 1, "å®®åŸçœŒ": 1, "ç§‹ç”°çœŒ": 1,
            "å±±å½¢çœŒ": 1, "ç¦å³¶çœŒ": 1, "èŒ¨åŸçœŒ": 2, "æ ƒæœ¨çœŒ": 1, "ç¾¤é¦¬çœŒ": 1,
            "åŸ¼ç‰çœŒ": 3, "åƒè‘‰çœŒ": 3, "æ±äº¬éƒ½": 6, "ç¥å¥ˆå·çœŒ": 4, "æ–°æ½ŸçœŒ": 1,
            "å¯Œå±±çœŒ": 1, "çŸ³å·çœŒ": 1, "ç¦äº•çœŒ": 1, "å±±æ¢¨çœŒ": 1, "é•·é‡çœŒ": 1,
            "å²é˜œçœŒ": 1, "é™å²¡çœŒ": 2, "æ„›çŸ¥çœŒ": 4, "ä¸‰é‡çœŒ": 1, "æ»‹è³€çœŒ": 1,
            "äº¬éƒ½åºœ": 2, "å¤§é˜ªåºœ": 4, "å…µåº«çœŒ": 3, "å¥ˆè‰¯çœŒ": 1, "å’Œæ­Œå±±çœŒ": 1,
            "é³¥å–çœŒ": 1, "å³¶æ ¹çœŒ": 1, "å²¡å±±çœŒ": 1, "åºƒå³¶çœŒ": 2, "å±±å£çœŒ": 1,
            "å¾³å³¶çœŒ": 1, "é¦™å·çœŒ": 1, "æ„›åª›çœŒ": 1, "é«˜çŸ¥çœŒ": 1, "ç¦å²¡çœŒ": 3,
            "ä½è³€çœŒ": 1, "é•·å´çœŒ": 1, "ç†Šæœ¬çœŒ": 1, "å¤§åˆ†çœŒ": 1, "å®®å´çœŒ": 1,
            "é¹¿å…å³¶çœŒ": 1, "æ²–ç¸„çœŒ": 1
        }
        
        # åé›†çµ±è¨ˆ
        self.stats = {
            "total_candidates": 0,
            "with_detailed_info": 0,
            "with_photos": 0,
            "with_policies": 0,
            "with_sns": 0,
            "prefectures_processed": 0,
            "errors": 0
        }
    
    def update_headers(self):
        """User-Agentæ›´æ–°ã¨ãƒ–ãƒ©ã‚¦ã‚¶å½è£…"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Cache-Control': 'no-cache',
            'Referer': self.base_url
        })
    
    def random_delay(self, min_seconds=1, max_seconds=3):
        """ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ï¼ˆGo2senkyoè² è·è»½æ¸›ï¼‰"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
    
    def collect_all_prefectures(self) -> List[Dict[str, Any]]:
        """å…¨éƒ½é“åºœçœŒã®å‚é™¢é¸å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info("ğŸš€ Go2senkyo å…¨éƒ½é“åºœçœŒå‚é™¢é¸2025ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        
        all_candidates = []
        
        # ä¸»è¦éƒ½é“åºœçœŒã‚’å„ªå…ˆçš„ã«å‡¦ç†
        priority_prefectures = ["æ±äº¬éƒ½", "å¤§é˜ªåºœ", "ç¥å¥ˆå·çœŒ", "æ„›çŸ¥çœŒ", "åŸ¼ç‰çœŒ", "åƒè‘‰çœŒ", "å…µåº«çœŒ", "ç¦å²¡çœŒ"]
        other_prefectures = [pref for pref in self.prefecture_codes.keys() if pref not in priority_prefectures]
        
        processing_order = priority_prefectures + other_prefectures
        
        for prefecture in processing_order:
            try:
                logger.info(f"ğŸ“ {prefecture} ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
                pref_candidates = self.collect_prefecture_candidates(prefecture)
                
                if pref_candidates:
                    all_candidates.extend(pref_candidates)
                    self.stats["prefectures_processed"] += 1
                    logger.info(f"âœ… {prefecture}: {len(pref_candidates)}ååé›†")
                else:
                    logger.warning(f"âš ï¸ {prefecture}: ãƒ‡ãƒ¼ã‚¿åé›†å¤±æ•—")
                
                # éƒ½é“åºœçœŒé–“ã®é–“éš”
                self.random_delay(2, 4)
                
            except Exception as e:
                logger.error(f"âŒ {prefecture}ã‚¨ãƒ©ãƒ¼: {str(e)}")
                self.stats["errors"] += 1
                continue
        
        logger.info(f"ğŸ¯ å…¨éƒ½é“åºœçœŒåé›†å®Œäº†: {len(all_candidates)}å")
        self.stats["total_candidates"] = len(all_candidates)
        
        return all_candidates
    
    def collect_prefecture_candidates(self, prefecture: str) -> List[Dict[str, Any]]:
        """éƒ½é“åºœçœŒåˆ¥å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†"""
        pref_code = self.prefecture_codes.get(prefecture)
        if not pref_code:
            logger.warning(f"éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prefecture}")
            return []
        
        prefecture_url = f"{self.sangiin_2025_url}/prefecture/{pref_code}"
        
        try:
            self.random_delay()
            response = self.session.get(prefecture_url, timeout=30)
            
            if response.status_code != 200:
                logger.warning(f"{prefecture}ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            candidates = self.parse_prefecture_page(soup, prefecture, prefecture_url)
            
            # å„å€™è£œè€…ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
            enhanced_candidates = []
            for candidate in candidates:
                try:
                    enhanced = self.enhance_candidate_details(candidate)
                    enhanced_candidates.append(enhanced)
                    self.random_delay(1, 2)  # å€™è£œè€…è©³ç´°å–å¾—é–“éš”
                    
                except Exception as e:
                    logger.debug(f"å€™è£œè€…{candidate.get('name', 'unknown')}è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    enhanced_candidates.append(candidate)
                    continue
            
            return enhanced_candidates
            
        except Exception as e:
            logger.error(f"{prefecture}ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return []
    
    def parse_prefecture_page(self, soup: BeautifulSoup, prefecture: str, page_url: str) -> List[Dict[str, Any]]:
        """éƒ½é“åºœçœŒãƒšãƒ¼ã‚¸ã‹ã‚‰å€™è£œè€…åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º"""
        candidates = []
        
        try:
            # Go2senkyoã®å€™è£œè€…ãƒªã‚¹ãƒˆè¦ç´ ã‚’æ¢ã™
            candidate_selectors = [
                '.candidate-item',
                '.candidate-card', 
                '.person-item',
                '[class*="candidate"]',
                '[class*="person"]'
            ]
            
            candidate_elements = []
            for selector in candidate_selectors:
                elements = soup.select(selector)
                if elements:
                    candidate_elements = elements
                    logger.debug(f"å€™è£œè€…è¦ç´ ç™ºè¦‹: {selector} ({len(elements)}ä»¶)")
                    break
            
            # ã‚ˆã‚Šæ±ç”¨çš„ãªæ¢ç´¢
            if not candidate_elements:
                # åå‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®æ¢ç´¢
                name_pattern = re.compile(r'[ä¸€-é¾¯]{2,4}[\sã€€]+[ä¸€-é¾¯]{2,8}')
                potential_candidates = soup.find_all(['div', 'li', 'article'], string=name_pattern)
                
                if not potential_candidates:
                    # ãƒªãƒ³ã‚¯ã‹ã‚‰å€™è£œè€…ãƒšãƒ¼ã‚¸ã‚’æ¢ã™
                    candidate_links = soup.find_all('a', href=re.compile(r'candidate|person|profile'))
                    for link in candidate_links:
                        name_text = link.get_text(strip=True)
                        if name_pattern.search(name_text):
                            potential_candidates.append(link)
                
                candidate_elements = potential_candidates[:20]  # æœ€å¤§20åã¾ã§
            
            for idx, element in enumerate(candidate_elements):
                try:
                    candidate_data = self.extract_candidate_basic_info(element, prefecture, page_url, idx)
                    if candidate_data:
                        candidates.append(candidate_data)
                        
                except Exception as e:
                    logger.debug(f"å€™è£œè€…è¦ç´ {idx}è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            logger.info(f"{prefecture}: {len(candidates)}åã®åŸºæœ¬æƒ…å ±å–å¾—")
            
        except Exception as e:
            logger.error(f"{prefecture}ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return candidates
    
    def extract_candidate_basic_info(self, element, prefecture: str, page_url: str, idx: int) -> Optional[Dict[str, Any]]:
        """å€™è£œè€…è¦ç´ ã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            # å€™è£œè€…åã®æŠ½å‡º
            name = ""
            name_selectors = ['h2', 'h3', 'h4', '.name', '.candidate-name', 'a']
            
            for selector in name_selectors:
                name_elem = element.select_one(selector) if hasattr(element, 'select_one') else element.find(selector)
                if name_elem:
                    text = name_elem.get_text(strip=True)
                    name_match = re.search(r'([ä¸€-é¾¯]{2,4}[\sã€€]+[ä¸€-é¾¯]{2,8})', text)
                    if name_match:
                        name = name_match.group(1).strip().replace('\u3000', ' ')
                        break
            
            if not name:
                # è¦ç´ å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åå‰ã‚’æ¢ã™
                full_text = element.get_text(strip=True) if hasattr(element, 'get_text') else str(element)
                name_match = re.search(r'([ä¸€-é¾¯]{2,4}[\sã€€]+[ä¸€-é¾¯]{2,8})', full_text)
                if name_match:
                    name = name_match.group(1).strip().replace('\u3000', ' ')
            
            if not name:
                return None
            
            # æ”¿å…šæƒ…å ±ã®æŠ½å‡º
            party = ""
            party_keywords = ['è‡ªæ°‘', 'ç«‹æ†²', 'ç¶­æ–°', 'å…¬æ˜', 'å…±ç”£', 'å›½æ°‘', 'ã‚Œã„ã‚', 'ç¤¾æ°‘', 'Nå›½', 'NHK', 'ç„¡æ‰€å±']
            
            for keyword in party_keywords:
                if keyword in element.get_text() if hasattr(element, 'get_text') else str(element):
                    party_mapping = {
                        'è‡ªæ°‘': 'è‡ªç”±æ°‘ä¸»å…š', 'ç«‹æ†²': 'ç«‹æ†²æ°‘ä¸»å…š', 'ç¶­æ–°': 'æ—¥æœ¬ç¶­æ–°ã®ä¼š',
                        'å…¬æ˜': 'å…¬æ˜å…š', 'å…±ç”£': 'æ—¥æœ¬å…±ç”£å…š', 'å›½æ°‘': 'å›½æ°‘æ°‘ä¸»å…š',
                        'ã‚Œã„ã‚': 'ã‚Œã„ã‚æ–°é¸çµ„', 'ç¤¾æ°‘': 'ç¤¾ä¼šæ°‘ä¸»å…š', 
                        'Nå›½': 'NHKå…š', 'NHK': 'NHKå…š', 'ç„¡æ‰€å±': 'ç„¡æ‰€å±'
                    }
                    party = party_mapping.get(keyword, keyword)
                    break
            
            if not party:
                party = "æœªåˆ†é¡"
            
            # è©³ç´°ãƒšãƒ¼ã‚¸URLã®æŠ½å‡º
            detail_url = ""
            if hasattr(element, 'find'):
                link_elem = element.find('a', href=True)
                if link_elem:
                    href = link_elem['href']
                    if href.startswith('/'):
                        detail_url = urljoin(self.base_url, href)
                    elif href.startswith('http'):
                        detail_url = href
            elif hasattr(element, 'get') and element.get('href'):
                href = element.get('href')
                if href.startswith('/'):
                    detail_url = urljoin(self.base_url, href)
                elif href.startswith('http'):
                    detail_url = href
            
            # å€™è£œè€…IDç”Ÿæˆ
            candidate_id = f"go2senkyo_{prefecture.replace('éƒ½', '').replace('åºœ', '').replace('çœŒ', '')}_{idx+1:03d}"
            
            candidate_data = {
                "candidate_id": candidate_id,
                "name": name,
                "prefecture": prefecture,
                "constituency": prefecture.replace('éƒ½', '').replace('åºœ', '').replace('çœŒ', ''),
                "constituency_type": "single_member",
                "party": party,
                "party_normalized": party,
                "detail_url": detail_url,
                "source_page": page_url,
                "source": "go2senkyo_enhanced",
                "collected_at": datetime.now().isoformat(),
                "seats_contested": self.sangiin_seats.get(prefecture, 1)
            }
            
            return candidate_data
            
        except Exception as e:
            logger.debug(f"å€™è£œè€…åŸºæœ¬æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def enhance_candidate_details(self, candidate: Dict[str, Any]) -> Dict[str, Any]:
        """å€™è£œè€…è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰åŒ…æ‹¬çš„æƒ…å ±ã‚’å–å¾—"""
        enhanced = candidate.copy()
        detail_url = candidate.get('detail_url', '')
        
        if not detail_url:
            return enhanced
        
        try:
            logger.debug(f"å€™è£œè€…è©³ç´°å–å¾—: {candidate.get('name', 'unknown')} - {detail_url}")
            
            self.random_delay(1, 2)
            response = self.session.get(detail_url, timeout=20)
            
            if response.status_code != 200:
                logger.debug(f"è©³ç´°ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {detail_url}")
                return enhanced
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã®æŠ½å‡º
            profile_data = self.extract_profile_details(soup, detail_url)
            enhanced.update(profile_data)
            
            # æ”¿ç­–ãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæƒ…å ±
            policy_data = self.extract_policy_information(soup)
            enhanced.update(policy_data)
            
            # SNSãƒ»Webã‚µã‚¤ãƒˆæƒ…å ±
            social_data = self.extract_social_links(soup)
            enhanced.update(social_data)
            
            # å†™çœŸãƒ»ç”»åƒ
            photo_data = self.extract_candidate_photos(soup, detail_url)
            enhanced.update(photo_data)
            
            # çµ±è¨ˆæ›´æ–°
            if profile_data or policy_data or social_data or photo_data:
                self.stats["with_detailed_info"] += 1
            if photo_data.get('photo_url'):
                self.stats["with_photos"] += 1
            if policy_data.get('manifesto_summary'):
                self.stats["with_policies"] += 1
            if social_data.get('sns_accounts'):
                self.stats["with_sns"] += 1
            
        except Exception as e:
            logger.debug(f"å€™è£œè€…è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ ({candidate.get('name', 'unknown')}): {e}")
        
        return enhanced
    
    def extract_profile_details(self, soup: BeautifulSoup, page_url: str) -> Dict[str, Any]:
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«è©³ç´°æƒ…å ±ã®æŠ½å‡º"""
        profile = {}
        
        try:
            # å¹´é½¢ãƒ»ç”Ÿå¹´æœˆæ—¥
            age_patterns = [
                re.compile(r'(\d{1,2})æ­³'),
                re.compile(r'([æ˜­å¹³ä»¤]\w+\d+å¹´)'),
                re.compile(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)')
            ]
            
            page_text = soup.get_text()
            for pattern in age_patterns:
                match = pattern.search(page_text)
                if match:
                    profile['age_info'] = match.group(1)
                    break
            
            # çµŒæ­´ãƒ»å­¦æ­´
            career_keywords = ['çµŒæ­´', 'å­¦æ­´', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'ç•¥æ­´', 'å‡ºèº«']
            for keyword in career_keywords:
                career_section = soup.find(string=re.compile(keyword))
                if career_section and career_section.parent:
                    career_text = career_section.parent.get_text(strip=True)
                    if len(career_text) > 50:
                        profile['career'] = career_text[:800]
                        break
            
            # è·æ¥­ãƒ»è‚©æ›¸ã
            title_keywords = ['è·æ¥­', 'ç¾è·', 'è‚©æ›¸', 'å½¹è·']
            for keyword in title_keywords:
                title_section = soup.find(string=re.compile(keyword))
                if title_section and title_section.parent:
                    title_text = title_section.parent.get_text(strip=True)
                    profile['occupation'] = title_text[:200]
                    break
            
            # å‡ºèº«åœ°
            origin_patterns = [
                re.compile(r'([éƒ½é“åºœçœŒ][å¸‚åŒºç”ºæ‘]+)å‡ºèº«'),
                re.compile(r'å‡ºèº«[ï¼š:]\s*([éƒ½é“åºœçœŒ][å¸‚åŒºç”ºæ‘]+)'),
            ]
            
            for pattern in origin_patterns:
                match = pattern.search(page_text)
                if match:
                    profile['birthplace'] = match.group(1)
                    break
        
        except Exception as e:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return profile
    
    def extract_policy_information(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """æ”¿ç­–ãƒ»ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆæƒ…å ±ã®æŠ½å‡º"""
        policy = {}
        
        try:
            # æ”¿ç­–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
            policy_keywords = ['æ”¿ç­–', 'ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ', 'å…¬ç´„', 'ä¸»å¼µ', 'æ”¿æ²»ä¿¡æ¡', 'å–ã‚Šçµ„ã¿', 'é‡ç‚¹æ”¿ç­–']
            policy_texts = []
            
            for keyword in policy_keywords:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
                policy_elements = soup.find_all(string=re.compile(keyword))
                
                for element in policy_elements:
                    if element.parent:
                        parent_text = element.parent.get_text(strip=True)
                        if len(parent_text) > 30:
                            policy_texts.append(parent_text)
                
                # ååˆ†ãªæƒ…å ±ãŒé›†ã¾ã£ãŸã‚‰åœæ­¢
                if len(' '.join(policy_texts)) > 500:
                    break
            
            if policy_texts:
                combined_policy = ' '.join(policy_texts)
                # é‡è¤‡é™¤å»ã¨æ•´ç†
                unique_sentences = []
                for sentence in combined_policy.split('ã€‚'):
                    if sentence.strip() and sentence not in unique_sentences:
                        unique_sentences.append(sentence.strip())
                
                policy['manifesto_summary'] = 'ã€‚'.join(unique_sentences[:10])[:1000]
            
            # é‡ç‚¹åˆ†é‡ã®æŠ½å‡º
            policy_areas = ['çµŒæ¸ˆ', 'å¤–äº¤', 'å®‰å…¨ä¿éšœ', 'æ•™è‚²', 'ç’°å¢ƒ', 'ç¤¾ä¼šä¿éšœ', 'åŒ»ç™‚', 'è¾²æ¥­']
            mentioned_areas = []
            
            page_text = soup.get_text().lower()
            for area in policy_areas:
                if area in page_text:
                    mentioned_areas.append(area)
            
            if mentioned_areas:
                policy['policy_areas'] = mentioned_areas
        
        except Exception as e:
            logger.debug(f"æ”¿ç­–æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return policy
    
    def extract_social_links(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """SNSãƒ»Webã‚µã‚¤ãƒˆãƒªãƒ³ã‚¯ã®æŠ½å‡º"""
        social = {"sns_accounts": {}, "websites": []}
        
        try:
            # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href'].lower()
                
                # Twitter/X
                if 'twitter.com' in href or 'x.com' in href:
                    social["sns_accounts"]["twitter"] = link['href']
                
                # Facebook
                elif 'facebook.com' in href:
                    social["sns_accounts"]["facebook"] = link['href']
                
                # Instagram
                elif 'instagram.com' in href:
                    social["sns_accounts"]["instagram"] = link['href']
                
                # YouTube
                elif 'youtube.com' in href or 'youtu.be' in href:
                    social["sns_accounts"]["youtube"] = link['href']
                
                # TikTok
                elif 'tiktok.com' in href:
                    social["sns_accounts"]["tiktok"] = link['href']
                
                # å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ
                elif any(domain in href for domain in ['.com', '.jp', '.org', '.net']) and \
                     not any(exclude in href for exclude in ['twitter', 'facebook', 'instagram', 'youtube', 'go2senkyo']):
                    if len(social["websites"]) < 3:  # æœ€å¤§3ã¤ã¾ã§
                        social["websites"].append({
                            "url": link['href'],
                            "title": link.get_text(strip=True)[:50]
                        })
            
            # ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæƒ…å ±ã‚’ãƒ¡ã‚¤ãƒ³ã«ç§»å‹•
            if social["websites"]:
                social["official_website"] = social["websites"][0]["url"]
        
        except Exception as e:
            logger.debug(f"SNSãƒ»ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return social
    
    def extract_candidate_photos(self, soup: BeautifulSoup, page_url: str) -> Dict[str, Any]:
        """å€™è£œè€…å†™çœŸãƒ»ç”»åƒã®æŠ½å‡º"""
        photos = {}
        
        try:
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å†™çœŸã®å€™è£œ
            img_selectors = [
                'img[alt*="ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«"]',
                'img[alt*="å€™è£œè€…"]',
                'img[alt*="å†™çœŸ"]',
                '.profile-image img',
                '.candidate-photo img',
                '.person-image img'
            ]
            
            photo_candidates = []
            
            for selector in img_selectors:
                imgs = soup.select(selector)
                photo_candidates.extend(imgs)
            
            # ã‚ˆã‚Šæ±ç”¨çš„ãªç”»åƒæ¤œç´¢
            if not photo_candidates:
                all_imgs = soup.find_all('img')
                for img in all_imgs:
                    alt = img.get('alt', '').lower()
                    src = img.get('src', '').lower()
                    
                    # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»åƒã®å¯èƒ½æ€§ãŒé«˜ã„ã‚‚ã®
                    if any(keyword in alt for keyword in ['profile', 'candidate', 'å€™è£œ', 'person']) or \
                       any(keyword in src for keyword in ['profile', 'candidate', 'å€™è£œ', 'person']):
                        photo_candidates.append(img)
            
            # æœ€é©ãªå†™çœŸã‚’é¸æŠ
            if photo_candidates:
                best_photo = photo_candidates[0]
                src = best_photo.get('src', '')
                
                if src:
                    if src.startswith('/'):
                        photo_url = urljoin(page_url, src)
                    elif src.startswith('http'):
                        photo_url = src
                    else:
                        photo_url = urljoin(page_url, '/' + src)
                    
                    photos['photo_url'] = photo_url
                    photos['photo_alt'] = best_photo.get('alt', '')
        
        except Exception as e:
            logger.debug(f"å†™çœŸæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return photos
    
    def save_enhanced_data(self, candidates: List[Dict[str, Any]]):
        """å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        if not candidates:
            logger.warning("ä¿å­˜ã™ã‚‹å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
        main_file = self.output_dir / f"go2senkyo_enhanced_{timestamp}.json"
        latest_file = self.output_dir / "go2senkyo_enhanced_latest.json"
        
        # çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        party_stats = {}
        prefecture_stats = {}
        
        for candidate in candidates:
            party = candidate.get('party', 'ç„¡æ‰€å±')
            prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
            
            party_stats[party] = party_stats.get(party, 0) + 1
            prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
        
        # ä¿å­˜ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        save_data = {
            "metadata": {
                "data_type": "go2senkyo_enhanced_sangiin_2025",
                "collection_method": "prefecture_by_prefecture_scraping",
                "total_candidates": len(candidates),
                "generated_at": datetime.now().isoformat(),
                "source_site": "sangiin.go2senkyo.com",
                "collection_stats": self.stats,
                "data_quality": {
                    "detailed_info_rate": f"{(self.stats['with_detailed_info']/max(len(candidates), 1)*100):.1f}%",
                    "photo_rate": f"{(self.stats['with_photos']/max(len(candidates), 1)*100):.1f}%",
                    "policy_rate": f"{(self.stats['with_policies']/max(len(candidates), 1)*100):.1f}%",
                    "sns_rate": f"{(self.stats['with_sns']/max(len(candidates), 1)*100):.1f}%"
                }
            },
            "statistics": {
                "by_party": party_stats,
                "by_prefecture": prefecture_stats
            },
            "data": candidates
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(main_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ Go2senkyoå¼·åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  - ãƒ•ã‚¡ã‚¤ãƒ«: {main_file}")
        logger.info(f"  - å€™è£œè€…æ•°: {len(candidates)}å")
        logger.info(f"  - å‡¦ç†éƒ½é“åºœçœŒ: {self.stats['prefectures_processed']}")
        
        # è©³ç´°çµ±è¨ˆè¡¨ç¤º
        self.display_final_stats(candidates, party_stats, prefecture_stats)
    
    def display_final_stats(self, candidates: List[Dict[str, Any]], 
                           party_stats: Dict[str, int], 
                           prefecture_stats: Dict[str, int]):
        """æœ€çµ‚çµ±è¨ˆã®è¡¨ç¤º"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š Go2senkyo å‚é™¢é¸2025 å¼·åŒ–ãƒ‡ãƒ¼ã‚¿åé›†çµæœ")
        logger.info("="*60)
        
        logger.info(f"ğŸ¯ ç·åé›†æ•°: {len(candidates)}å")
        logger.info(f"ğŸ“ å‡¦ç†éƒ½é“åºœçœŒ: {self.stats['prefectures_processed']}")
        logger.info(f"ğŸ“‹ è©³ç´°æƒ…å ±ä»˜ã: {self.stats['with_detailed_info']}å")
        logger.info(f"ğŸ“· å†™çœŸä»˜ã: {self.stats['with_photos']}å")
        logger.info(f"ğŸ“œ æ”¿ç­–æƒ…å ±ä»˜ã: {self.stats['with_policies']}å")
        logger.info(f"ğŸ”— SNSæƒ…å ±ä»˜ã: {self.stats['with_sns']}å")
        logger.info(f"âŒ ã‚¨ãƒ©ãƒ¼æ•°: {self.stats['errors']}")
        
        # æ”¿å…šåˆ¥ãƒˆãƒƒãƒ—10
        logger.info("\nğŸ›ï¸ æ”¿å…šåˆ¥å€™è£œè€…æ•° (ãƒˆãƒƒãƒ—10):")
        top_parties = sorted(party_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        for party, count in top_parties:
            logger.info(f"  {party}: {count}å")
        
        # éƒ½é“åºœçœŒåˆ¥ãƒˆãƒƒãƒ—10
        logger.info("\nğŸ—¾ éƒ½é“åºœçœŒåˆ¥å€™è£œè€…æ•° (ãƒˆãƒƒãƒ—10):")
        top_prefectures = sorted(prefecture_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        for prefecture, count in top_prefectures:
            logger.info(f"  {prefecture}: {count}å")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("ğŸš€ Go2senkyoå‚é™¢é¸2025å¼·åŒ–ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
    
    collector = Go2senkyoEnhancedCollector()
    
    try:
        # å…¨éƒ½é“åºœçœŒã®ãƒ‡ãƒ¼ã‚¿åé›†
        all_candidates = collector.collect_all_prefectures()
        
        if not all_candidates:
            logger.error("âŒ å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        collector.save_enhanced_data(all_candidates)
        
        logger.info("âœ¨ Go2senkyoå¼·åŒ–ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
        
    except KeyboardInterrupt:
        logger.info("âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()