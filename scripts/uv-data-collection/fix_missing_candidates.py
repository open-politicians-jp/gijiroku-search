#!/usr/bin/env python3
"""
ä¸è¶³ã—ã¦ã„ã‚‹å€™è£œè€…ã®ä¿®æ­£åé›†
"""

import json
import logging
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MissingCandidatesFixer:
    def __init__(self):
        self.session = requests.Session()
        ua = UserAgent()
        desktop_ua = ua.random
        
        self.session.headers.update({
            'User-Agent': desktop_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def get_all_candidates_for_prefecture(self, pref_code: int, pref_name: str):
        """éƒ½é“åºœçœŒã®å…¨å€™è£œè€…ã‚’å–å¾—"""
        url = f"https://sangiin.go2senkyo.com/2025/prefecture/{pref_code}"
        logger.info(f"ğŸ” {pref_name} (ã‚³ãƒ¼ãƒ‰: {pref_code}) ãƒ‡ãƒ¼ã‚¿å–å¾—: {url}")
        
        try:
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                logger.error(f"âŒ {pref_name} ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            logger.info(f"ğŸ“Š {pref_name} HTMLå–å¾—æˆåŠŸ: {len(response.text):,} æ–‡å­—")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å…¨ã¦ã®å€™è£œè€…ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            profile_links = soup.find_all('a', href=re.compile(r'/seijika/\d+'))
            logger.info(f"ğŸ”— {pref_name} ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ç™ºè¦‹: {len(profile_links)}å€‹")
            
            candidates = []
            seen_ids = set()
            
            for i, link in enumerate(profile_links):
                try:
                    href = link.get('href', '')
                    if not href:
                        continue
                    
                    # å€™è£œè€…IDæŠ½å‡º
                    match = re.search(r'/seijika/(\d+)', href)
                    if not match:
                        continue
                    
                    candidate_id = match.group(1)
                    if candidate_id in seen_ids:
                        continue
                    seen_ids.add(candidate_id)
                    
                    # å€™è£œè€…æƒ…å ±æŠ½å‡º
                    candidate_info = self.extract_candidate_info(link, pref_name, candidate_id, url)
                    if candidate_info:
                        candidates.append(candidate_info)
                        logger.info(f"  âœ… {i+1}: {candidate_info['name']} ({candidate_info['party']})")
                    
                except Exception as e:
                    logger.debug(f"ãƒªãƒ³ã‚¯ {i} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            logger.info(f"ğŸ¯ {pref_name} å–å¾—å®Œäº†: {len(candidates)}å")
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def extract_candidate_info(self, link, prefecture: str, candidate_id: str, page_url: str):
        """å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            # åŸºæœ¬æƒ…å ±
            candidate = {
                "candidate_id": f"go2s_{candidate_id}",
                "name": "",
                "prefecture": prefecture,
                "constituency": prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', ''),
                "constituency_type": "single_member",
                "party": "",
                "party_normalized": "",
                "profile_url": f"https://go2senkyo.com{link.get('href')}",
                "source_page": page_url,
                "source": "go2senkyo_fixed",
                "collected_at": datetime.now().isoformat()
            }
            
            # åå‰ã®æŠ½å‡ºï¼ˆè¤‡æ•°æ–¹æ³•è©¦è¡Œï¼‰
            name = self.extract_name_from_context(link)
            if name:
                candidate["name"] = name
            
            # æ”¿å…šã®æŠ½å‡º
            party = self.extract_party_from_context(link)
            if party:
                candidate["party"] = party
                candidate["party_normalized"] = party
            
            # åå‰ãŒå–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not candidate["name"]:
                return None
            
            return candidate
            
        except Exception as e:
            logger.debug(f"å€™è£œè€…æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_name_from_context(self, link):
        """ãƒªãƒ³ã‚¯å‘¨è¾ºã‹ã‚‰åå‰ã‚’æŠ½å‡º"""
        name = ""
        
        try:
            # 1. ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆç¢ºèª
            link_text = link.get_text(strip=True)
            if link_text and link_text not in ['è©³ç´°ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'è©³ç´°']:
                if re.match(r'[ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,}', link_text):
                    name = link_text
                    return name
            
            # 2. è¦ªè¦ç´ ã‹ã‚‰æ¤œç´¢
            parent = link.parent
            search_depth = 0
            while parent and search_depth < 5:
                # ã‚¯ãƒ©ã‚¹åã§ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¤œç´¢
                name_elements = parent.find_all(class_=re.compile(r'name|title|candidate'))
                for elem in name_elements:
                    text = elem.get_text(strip=True)
                    if text and text not in ['è©³ç´°ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«']:
                        # æ—¥æœ¬äººåãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ
                        name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,8})', text)
                        if name_match:
                            potential_name = name_match.group(1)
                            if len(potential_name) >= 2:
                                name = potential_name
                                return name
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„å…¨ä½“ã‹ã‚‰æ¤œç´¢
                parent_text = parent.get_text(strip=True)
                lines = parent_text.split('\n')
                for line in lines:
                    line = line.strip()
                    if line and len(line) < 30:  # çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆè¡Œ
                        name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,8})', line)
                        if name_match:
                            potential_name = name_match.group(1)
                            if len(potential_name) >= 2 and potential_name not in ['è©³ç´°', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'é¸æŒ™', 'å€™è£œ']:
                                name = potential_name
                                return name
                
                parent = parent.parent
                search_depth += 1
            
            # 3. å…„å¼Ÿè¦ç´ ã‹ã‚‰æ¤œç´¢
            if link.parent:
                siblings = link.parent.find_all(text=True)
                for sibling in siblings:
                    text = sibling.strip()
                    if text:
                        name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]{2,8})', text)
                        if name_match:
                            potential_name = name_match.group(1)
                            if len(potential_name) >= 2:
                                name = potential_name
                                return name
        
        except Exception as e:
            logger.debug(f"åå‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return name
    
    def extract_party_from_context(self, link):
        """ãƒªãƒ³ã‚¯å‘¨è¾ºã‹ã‚‰æ”¿å…šã‚’æŠ½å‡º"""
        party = ""
        
        try:
            # æ”¿å…šãƒªã‚¹ãƒˆ
            parties = [
                "è‡ªç”±æ°‘ä¸»å…š", "ç«‹æ†²æ°‘ä¸»å…š", "æ—¥æœ¬ç¶­æ–°ã®ä¼š", "å…¬æ˜å…š", "æ—¥æœ¬å…±ç”£å…š",
                "å›½æ°‘æ°‘ä¸»å…š", "ã‚Œã„ã‚æ–°é¸çµ„", "å‚æ”¿å…š", "ç¤¾ä¼šæ°‘ä¸»å…š", "NHKå…š",
                "æ—¥æœ¬ä¿å®ˆå…š", "æ—¥æœ¬æ”¹é©å…š", "ç„¡æ‰€å±", "ç„¡æ‰€å±é€£åˆ", "æ—¥æœ¬èª çœŸä¼š",
                "æ—¥æœ¬ã®å®¶åº­ã‚’å®ˆã‚‹ä¼š", "å†ç”Ÿã®é“", "å·®åˆ¥æ’²æ»…å…š", "æ ¸èåˆå…š", "ãƒãƒ¼ãƒ ã¿ã‚‰ã„",
                "å¤šå¤«å¤šå¦»å…š", "å›½æ”¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ä¼š"
            ]
            
            # è¦ªè¦ç´ ã‹ã‚‰æ¤œç´¢
            parent = link.parent
            search_depth = 0
            while parent and search_depth < 5:
                parent_text = parent.get_text()
                
                for p in parties:
                    if p in parent_text:
                        party = p
                        return party
                
                parent = parent.parent
                search_depth += 1
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            party = "æœªåˆ†é¡"
        
        except Exception as e:
            logger.debug(f"æ”¿å…šæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            party = "æœªåˆ†é¡"
        
        return party

def fix_missing_candidates():
    """ä¸è¶³å€™è£œè€…ã®ä¿®æ­£"""
    logger.info("ğŸ”§ ä¸è¶³å€™è£œè€…ã®ä¿®æ­£é–‹å§‹...")
    
    # å•é¡Œã®ã‚ã‚‹éƒ½é“åºœçœŒã‚’ãƒã‚§ãƒƒã‚¯
    problem_prefectures = [
        (24, "ä¸‰é‡çœŒ"),  # å®Ÿéš›4å â†’ ç¾åœ¨1å
        (26, "äº¬éƒ½åºœ"),  # å®Ÿéš›9å â†’ ç¾åœ¨6å
    ]
    
    fixer = MissingCandidatesFixer()
    
    # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    current_candidates = data.get('data', [])
    logger.info(f"ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿: {len(current_candidates)}å")
    
    # å„éƒ½é“åºœçœŒã®ä¿®æ­£
    new_candidates = []
    
    for pref_code, pref_name in problem_prefectures:
        logger.info(f"\n=== {pref_name} ä¿®æ­£å‡¦ç† ===")
        
        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»
        existing_candidates = [c for c in current_candidates if c['prefecture'] != pref_name]
        logger.info(f"ğŸ—‘ï¸ {pref_name} æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤")
        
        # æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
        fresh_candidates = fixer.get_all_candidates_for_prefecture(pref_code, pref_name)
        if fresh_candidates:
            new_candidates.extend(fresh_candidates)
            logger.info(f"âœ… {pref_name} æ–°ãƒ‡ãƒ¼ã‚¿è¿½åŠ : {len(fresh_candidates)}å")
        
        # é€²è¡ŒçŠ¶æ³ã®ä¼‘æ†©
        import time
        time.sleep(2)
    
    # ä»–ã®éƒ½é“åºœçœŒãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆ
    other_prefectures = [c for c in current_candidates 
                        if c['prefecture'] not in ['ä¸‰é‡çœŒ', 'äº¬éƒ½åºœ']]
    
    final_candidates = other_prefectures + new_candidates
    logger.info(f"ğŸ“Š æœ€çµ‚çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(final_candidates)}å")
    
    # ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    data['data'] = final_candidates
    data['metadata']['total_candidates'] = len(final_candidates)
    data['metadata']['generated_at'] = datetime.now().isoformat()
    data['metadata']['data_type'] = "go2senkyo_missing_fixed_sangiin_2025"
    
    # çµ±è¨ˆå†è¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    
    for candidate in final_candidates:
        party = candidate.get('party', 'ç„¡æ‰€å±')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
    
    data['statistics'] = {
        "by_party": party_stats,
        "by_prefecture": prefecture_stats,
        "by_constituency_type": {"single_member": len(final_candidates)}
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    fixed_file = data_dir / f"go2senkyo_missing_fixed_{timestamp}.json"
    
    with open(fixed_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ä¿å­˜å®Œäº†: {fixed_file}")
    
    # æœ€çµ‚çµ±è¨ˆ
    logger.info("\nğŸ“Š ä¿®æ­£å¾Œçµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(final_candidates)}å")
    logger.info(f"  ä¸‰é‡çœŒ: {prefecture_stats.get('ä¸‰é‡çœŒ', 0)}å")
    logger.info(f"  äº¬éƒ½åºœ: {prefecture_stats.get('äº¬éƒ½åºœ', 0)}å")

if __name__ == "__main__":
    fix_missing_candidates()