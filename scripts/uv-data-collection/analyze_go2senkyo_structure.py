#!/usr/bin/env python3
"""
Go2senkyo HTMLæ§‹é€ åˆ†æãƒ„ãƒ¼ãƒ«

æ±äº¬éƒ½ãƒšãƒ¼ã‚¸ã®HTMLã‚’è©³ç´°åˆ†æã—ã¦å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’æŠŠæ¡ã™ã‚‹
"""

import requests
import re
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_html_structure():
    """HTMLêµ¬ì¡° ìƒì„¸ ë¶„ì„"""
    logger.info("ğŸ” Go2senkyo HTMLêµ¬ì¡° ë¶„ì„ ì‹œì‘...")
    
    ua = UserAgent()
    session = requests.Session()
    session.headers.update({
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
    })
    
    tokyo_url = "https://sangiin.go2senkyo.com/2025/prefecture/13"
    
    try:
        response = session.get(tokyo_url, timeout=30)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. ëª¨ë“  í´ë˜ìŠ¤ëª… ë¶„ì„
        logger.info("ğŸ“‹ ëª¨ë“  í´ë˜ìŠ¤ëª… ë¶„ì„...")
        all_classes = set()
        for element in soup.find_all(class_=True):
            if isinstance(element['class'], list):
                all_classes.update(element['class'])
            else:
                all_classes.add(element['class'])
        
        relevant_classes = [cls for cls in all_classes if any(keyword in cls.lower() for keyword in 
                          ['candidate', 'person', 'member', 'card', 'item', 'list', 'å€™è£œ'])]
        
        logger.info(f"ğŸ¯ é–¢é€£ã‚¯ãƒ©ã‚¹æ•°: {len(relevant_classes)}")
        for cls in sorted(relevant_classes)[:20]:
            logger.info(f"  - {cls}")
        
        # 2. ë¦¬ìŠ¤íŠ¸/ì¹´ë“œ êµ¬ì¡° ë¶„ì„
        logger.info("\nğŸ“¦ ãƒªã‚¹ãƒˆ/ã‚«ãƒ¼ãƒ‰æ§‹ì¡° ë¶„æ...")
        list_containers = soup.find_all(['ul', 'ol', 'div'], class_=re.compile(r'list|grid|container|wrapper'))
        
        for i, container in enumerate(list_containers[:10]):
            class_name = ' '.join(container.get('class', []))
            children_count = len(container.find_all(['li', 'div', 'article']))
            logger.info(f"  {i+1}: {container.name}.{class_name} - {children_count}ê°œ ìì‹ìš”ì†Œ")
        
        # 3. ì´ë¦„ì´ í¬í•¨ëœ ìš”ì†Œë“¤ ìƒì„¸ ë¶„ì„
        logger.info("\nğŸ‘¤ ì´ë¦„ í¬í•¨ ìš”ì†Œ ë¶„ì„...")
        name_pattern = re.compile(r'[ä¸€-é¾¯]{2,4}[\sã€€]+[ä¸€-é¾¯]{2,8}')
        
        name_elements = []
        for element in soup.find_all(string=name_pattern):
            parent = element.parent if element.parent else None
            if parent:
                name_elements.append({
                    'text': element.strip(),
                    'parent_tag': parent.name,
                    'parent_class': ' '.join(parent.get('class', [])),
                    'parent_id': parent.get('id', ''),
                    'siblings_count': len(parent.find_all())
                })
        
        # ì´ë¦„ ìš”ì†Œì˜ ë¶€ëª¨ íŒ¨í„´ ë¶„ì„
        parent_patterns = {}
        for elem in name_elements[:15]:  # ì²˜ìŒ 15ê°œë§Œ ë¶„ì„
            pattern = f"{elem['parent_tag']}.{elem['parent_class']}"
            if pattern not in parent_patterns:
                parent_patterns[pattern] = []
            parent_patterns[pattern].append(elem['text'])
        
        logger.info("ğŸ—ï¸ ì´ë¦„ ìš”ì†Œì˜ ë¶€ëª¨ íŒ¨í„´:")
        for pattern, names in parent_patterns.items():
            logger.info(f"  {pattern}: {len(names)}ê°œ - {', '.join(names[:3])}...")
        
        # 4. ë§í¬ êµ¬ì¡° ë¶„ì„
        logger.info("\nğŸ”— ë§í¬ êµ¬ì¡° ë¶„ì„...")
        all_links = soup.find_all('a', href=True)
        
        # í›„ë³´ì ê´€ë ¨ ë§í¬ í•„í„°ë§
        candidate_like_links = []
        for link in all_links:
            href = link['href'].lower()
            text = link.get_text(strip=True)
            
            # í›„ë³´ìëª…ì´ë‚˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë§í¬
            if (name_pattern.search(text) or 
                any(keyword in href for keyword in ['candidate', 'person', 'member', 'profile']) or
                any(keyword in text for keyword in ['ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«', 'å€™è£œè€…', 'è©³ç´°'])):
                
                candidate_like_links.append({
                    'text': text,
                    'href': href,
                    'parent_class': ' '.join(link.parent.get('class', [])) if link.parent else ''
                })
        
        logger.info(f"ğŸ¯ å€™è£œè€…é–¢é€£ãƒªãƒ³ã‚¯: {len(candidate_like_links)}å€‹")
        for i, link in enumerate(candidate_like_links[:10]):
            logger.info(f"  {i+1}: {link['text'][:30]} -> {link['href'][:50]}")
        
        # 5. ì´ë¯¸ì§€ êµ¬ì¡° ë¶„ì„
        logger.info("\nğŸ–¼ï¸ ì´ë¯¸ì§€ êµ¬ì¡° ë¶„ì„...")
        images = soup.find_all('img')
        profile_images = []
        
        for img in images:
            alt = img.get('alt', '').lower()
            src = img.get('src', '').lower()
            
            if (any(keyword in alt for keyword in ['profile', 'candidate', 'å€™è£œ', 'å†™çœŸ']) or
                any(keyword in src for keyword in ['profile', 'candidate', 'å€™è£œ', 'photo'])):
                profile_images.append({
                    'alt': img.get('alt', ''),
                    'src': img.get('src', ''),
                    'parent_class': ' '.join(img.parent.get('class', [])) if img.parent else ''
                })
        
        logger.info(f"ğŸ‘¤ í”„ë¡œí•„ ì´ë¯¸ì§€ í›„ë³´: {len(profile_images)}ê°œ")
        for i, img in enumerate(profile_images[:5]):
            logger.info(f"  {i+1}: {img['alt'][:30]} - {img['src'][:50]}")
        
        # 6. JavaScript/AJAX ë¡œë”© ì²´í¬
        logger.info("\nâš¡ JavaScript/AJAX ë¶„ì„...")
        scripts = soup.find_all('script')
        js_patterns = ['ajax', 'fetch', 'candidate', 'person', 'load']
        
        relevant_scripts = []
        for script in scripts:
            if script.string:
                for pattern in js_patterns:
                    if pattern.lower() in script.string.lower():
                        relevant_scripts.append(pattern)
                        break
        
        logger.info(f"ğŸ”§ ê´€ë ¨ JavaScript íŒ¨í„´: {set(relevant_scripts)}")
        
        # 7. ìµœì  ì„ íƒì ì¶”ì²œ
        logger.info("\nğŸ¯ ì¶”ì²œ ì„ íƒì:")
        
        # ê°€ì¥ ë§ì´ ë“±ì¥í•˜ëŠ” ë¶€ëª¨ íŒ¨í„´
        if parent_patterns:
            most_common_pattern = max(parent_patterns.items(), key=lambda x: len(x[1]))
            logger.info(f"  ìµœë‹¤ ì´ë¦„ íŒ¨í„´: {most_common_pattern[0]} ({len(most_common_pattern[1])}ê°œ)")
        
        # ë§í¬ ê¸°ë°˜ ì¶”ì²œ
        if candidate_like_links:
            link_parents = [link['parent_class'] for link in candidate_like_links if link['parent_class']]
            if link_parents:
                from collections import Counter
                common_link_parent = Counter(link_parents).most_common(1)[0]
                logger.info(f"  ë§í¬ ë¶€ëª¨ í´ë˜ìŠ¤: .{common_link_parent[0]} ({common_link_parent[1]}ê°œ)")
        
        # DOM êµ¬ì¡° ê¸°ë°˜ ì¶”ì²œ
        logger.info("  ì¶”ì²œ ì„ íƒì ì¡°í•©:")
        logger.info("    1. ì´ë¦„ íŒ¨í„´ + ë§í¬: a[href*='candidate'], a[href*='person']")
        logger.info("    2. í´ë˜ìŠ¤ ê¸°ë°˜: .candidate-item, .person-card, .member-item")
        logger.info("    3. êµ¬ì¡° ê¸°ë°˜: ul > li, .grid > div, .list > .item")
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì—ëŸ¬: {e}")

if __name__ == "__main__":
    analyze_html_structure()