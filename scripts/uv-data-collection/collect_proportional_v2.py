#!/usr/bin/env python3
"""
æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›† v2 - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¯¾å¿œ
"""

import json
import logging
import re
import requests
from datetime import datetime
from pathlib import Path
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
from collect_go2senkyo_optimized import Go2senkyoOptimizedCollector

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def collect_proportional_v2():
    """æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›† v2 - ã‚ˆã‚Šã‚¿ãƒ¼ã‚²ãƒƒãƒˆåŒ–ã•ã‚ŒãŸæ‰‹æ³•"""
    logger.info("ğŸš€ æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›† v2 é–‹å§‹...")
    
    collector = Go2senkyoOptimizedCollector()
    all_proportional_candidates = []
    
    # å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªæ”¿å…šIDã‚’å–å¾—
    proportional_parties = get_available_parties(collector)
    
    if not proportional_parties:
        logger.warning("åˆ©ç”¨å¯èƒ½ãªæ”¿å…šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return []
    
    try:
        # å„æ”¿å…šã®æ¯”ä¾‹ä»£è¡¨å€™è£œè€…ã‚’åé›†
        for party_info in proportional_parties:
            try:
                party_name = party_info['name']
                party_id = party_info['id']
                
                logger.info(f"ğŸ“ {party_name} (ID:{party_id}) æ¯”ä¾‹ä»£è¡¨åé›†ä¸­...")
                
                # æ”¿å…šåˆ¥ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                url = f"{collector.base_url}/2025/hirei/{party_id}"
                collector.random_delay(1, 2)
                
                response = collector.session.get(url, timeout=30)
                
                if response.status_code != 200:
                    logger.warning(f"{party_name}: HTTP {response.status_code}")
                    continue
                
                # HTMLã‹ã‚‰JavaScriptã®ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã‚’æŠ½å‡º
                candidates = extract_candidates_from_html(response.text, party_name, url, collector)
                
                if candidates:
                    all_proportional_candidates.extend(candidates)
                    logger.info(f"âœ… {party_name}: {len(candidates)}å")
                    
                    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
                    if candidates:
                        sample = candidates[0]
                        logger.info(f"  ã‚µãƒ³ãƒ—ãƒ«: {sample.get('name', 'N/A')} ({sample.get('party', 'N/A')})")
                else:
                    logger.warning(f"{party_name}: å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
            except Exception as e:
                logger.error(f"âŒ {party_name}ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        logger.info(f"ğŸ¯ æ¯”ä¾‹ä»£è¡¨åé›†å®Œäº†: {len(all_proportional_candidates)}å")
        return all_proportional_candidates
        
    except Exception as e:
        logger.error(f"âŒ æ¯”ä¾‹ä»£è¡¨åé›†ã‚¨ãƒ©ãƒ¼: {e}") 
        raise

def get_available_parties(collector):
    """åˆ©ç”¨å¯èƒ½ãªæ”¿å…šä¸€è¦§ã‚’å–å¾—"""
    logger.info("ğŸ” åˆ©ç”¨å¯èƒ½ãªæ”¿å…šä¸€è¦§ã‚’å–å¾—ä¸­...")
    
    try:
        # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰æ”¿å…šæƒ…å ±ã‚’å–å¾—
        url = f"{collector.base_url}/2025"
        response = collector.session.get(url, timeout=30)
        
        if response.status_code != 200:
            logger.error(f"ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ¯”ä¾‹ä»£è¡¨æ”¿å…šãƒªãƒ³ã‚¯ã‚’æ¢ã™
        parties = []
        hirei_links = soup.find_all('a', href=re.compile(r'/2025/hirei/\d+'))
        
        for link in hirei_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # IDã‚’æŠ½å‡º
            match = re.search(r'/hirei/(\d+)', href)
            if match and text:
                party_id = match.group(1)
                parties.append({
                    'name': text,
                    'id': party_id,
                    'url': href
                })
        
        # é‡è¤‡ã‚’å‰Šé™¤
        unique_parties = []
        seen_ids = set()
        for party in parties:
            if party['id'] not in seen_ids:
                unique_parties.append(party)
                seen_ids.add(party['id'])
        
        logger.info(f"ğŸ“‹ ç™ºè¦‹ã•ã‚ŒãŸæ”¿å…š: {len(unique_parties)}å€‹")
        for party in unique_parties[:10]:  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
            logger.info(f"  - {party['name']} (ID: {party['id']})")
        
        return unique_parties
        
    except Exception as e:
        logger.error(f"æ”¿å…šä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def extract_candidates_from_html(html_content, party_name, page_url, collector):
    """HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º"""
    candidates = []
    
    try:
        # ã¾ãšé€šå¸¸ã®HTMLè§£æã‚’è©¦ã™
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Next.jsã®ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and 'hireiList' in script.string:
                # JSON ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
                json_matches = re.findall(r'\{[^{}]*"hireiList"[^{}]*\}', script.string)
                for match in json_matches:
                    try:
                        # JSON ã‚’å®‰å…¨ã«è§£æ
                        data = json.loads(match)
                        if 'hireiList' in data:
                            logger.info(f"hireiList ãƒ‡ãƒ¼ã‚¿ç™ºè¦‹: {len(data['hireiList'])}æ”¿å…š")
                            # å¾Œã§å®Ÿè£…
                            break
                    except:
                        continue
        
        # æ¨™æº–çš„ãªHTMLã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚‚è©¦ã™
        candidate_blocks = soup.find_all(['div', 'li', 'article'], class_=re.compile(r'candidate|person|member|senkyoku|hirei'))
        
        if candidate_blocks:
            logger.info(f"{party_name}: {len(candidate_blocks)}å€‹ã®ãƒ–ãƒ­ãƒƒã‚¯ç™ºè¦‹")
            
            for i, block in enumerate(candidate_blocks):
                candidate_info = extract_candidate_info_from_block(block, party_name, page_url, i, collector)
                if candidate_info:
                    candidates.append(candidate_info)
        
        # å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ä»–ã®æ–¹æ³•ã‚’è©¦ã™
        if not candidates:
            # ãƒ†ã‚­ã‚¹ãƒˆè§£æã«ã‚ˆã‚‹å€™è£œè€…åæŠ½å‡º
            text_candidates = extract_candidates_from_text(html_content, party_name, page_url, collector)
            candidates.extend(text_candidates)
        
    except Exception as e:
        logger.debug(f"HTMLè§£æã‚¨ãƒ©ãƒ¼ ({party_name}): {e}")
    
    return candidates

def extract_candidate_info_from_block(block, party_name, page_url, idx, collector):
    """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º"""
    try:
        # æ­£ç¢ºãªã‚¯ãƒ©ã‚¹åã‚’ä½¿ã£ã¦åå‰ã‚’æŠ½å‡º
        name_ttl = block.select_one('.ttl, [class*="ttl"], .title, [class*="title"]')
        name_subttl = block.select_one('.subttl, [class*="subttl"], .subtitle, [class*="subtitle"]')
        
        # åå‰ï¼ˆæ¼¢å­—ï¼‰ã®å–å¾—
        if name_ttl:
            name = name_ttl.get_text(strip=True)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä»–ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
            name_elem = block.select_one('h2, h3, h4, [class*="name"]')
            if name_elem:
                name = name_elem.get_text(strip=True)
            else:
                return None
        
        # èª­ã¿åï¼ˆã‚«ã‚¿ã‚«ãƒŠï¼‰ã®å–å¾—
        name_kana = ""
        if name_subttl:
            name_kana = name_subttl.get_text(strip=True)
        
        # ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        invalid_names = [
            'ä¼šå“¡ç™»éŒ²', 'æ¯”ä¾‹ä»£è¡¨äºˆæƒ³', 'ãƒ­ã‚°ã‚¤ãƒ³', 'ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—', 
            'MYé¸æŒ™', 'é¸æŒ™åŒº', 'ã“ã®', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', 'äºˆæƒ³ã•ã‚Œã‚‹é¡”ã¶ã‚Œ',
            'ã«ã¤ã„ã¦', 'ã‚·ã‚§ã‚¢', 'ãƒšãƒ¼ã‚¸', 'æ”¿å…š', 'å…¨æ”¿å…š',
            # æ”¿å…šåã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            'è‡ªç”±æ°‘ä¸»å…š', 'ç«‹æ†²æ°‘ä¸»å…š', 'å…¬æ˜å…š', 'æ—¥æœ¬ç¶­æ–°ã®ä¼š', 'æ—¥æœ¬å…±ç”£å…š',
            'å›½æ°‘æ°‘ä¸»å…š', 'ã‚Œã„ã‚æ–°é¸çµ„', 'å‚æ”¿å…š', 'ç¤¾ä¼šæ°‘ä¸»å…š', 
            'æ—¥æœ¬ä¿å®ˆå…š', 'ã¿ã‚“ãªã§ã¤ãã‚‹å…š', 'NHKå…š', 'ãƒãƒ¼ãƒ ã¿ã‚‰ã„',
            'å†ç”Ÿã®é“', 'æ—¥æœ¬æ”¹é©å…š', 'ç„¡æ‰€å±é€£åˆ', 'æ—¥æœ¬èª çœŸä¼š',
            'æ¯”ä¾‹ä»£è¡¨', 'å…¨å›½æ¯”ä¾‹', 'ä»£è¡¨è€…', 'ç™¾ç”°å°šæ¨¹', 'çŸ³æ¿±å“²ä¿¡'
        ]
        
        if any(invalid in name for invalid in invalid_names):
            logger.debug(f"ç„¡åŠ¹ãªåå‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {name}")
            return None
        
        # æ—¥æœ¬äººã®åå‰ã¨ã—ã¦å¦¥å½“ã‹ãƒã‚§ãƒƒã‚¯
        if len(name) < 2 or len(name) > 10:
            return None
        
        # æ¼¢å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not re.search(r'[ä¸€-é¾¯]', name):
            return None
        
        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        profile_link = block.find('a', href=re.compile(r'/seijika/\\d+'))
        profile_url = ""
        candidate_id = f"hirei_{party_name}_{idx}"
        detailed_info = {}
        
        if profile_link:
            href = profile_link.get('href', '')
            if href.startswith('/'):
                profile_url = f"https://go2senkyo.com{href}"
            else:
                profile_url = href
            
            # å€™è£œè€…IDæŠ½å‡º
            match = re.search(r'/seijika/(\\d+)', href)
            if match:
                candidate_id = f"hirei_{match.group(1)}"
                
                # å€‹åˆ¥ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—
                detailed_info = get_detailed_profile_info(profile_url, collector)
        
        candidate_data = {
            "candidate_id": candidate_id,
            "name": name,
            "prefecture": "æ¯”ä¾‹ä»£è¡¨",
            "constituency": "å…¨å›½æ¯”ä¾‹",
            "constituency_type": "proportional",
            "party": party_name,
            "party_normalized": collector.normalize_party_name(party_name),
            "profile_url": profile_url,
            "source_page": page_url,
            "source": "go2senkyo_proportional_v2",
            "collected_at": datetime.now().isoformat()
        }
        
        # ã‚«ã‚¿ã‚«ãƒŠåå‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
        if name_kana:
            candidate_data["name_kana"] = name_kana
        
        # è©³ç´°æƒ…å ±ã‚’ãƒãƒ¼ã‚¸
        candidate_data.update(detailed_info)
        
        return candidate_data
        
    except Exception as e:
        logger.debug(f"å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({party_name}): {e}")
        return None

def get_detailed_profile_info(profile_url, collector):
    """å€‹åˆ¥ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    detailed_info = {}
    
    try:
        if not profile_url:
            return detailed_info
        
        logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«è©³ç´°å–å¾—: {profile_url}")
        collector.random_delay(0.5, 1.0)  # çŸ­ã„é–“éš”
        
        response = collector.session.get(profile_url, timeout=15)
        if response.status_code != 200:
            return detailed_info
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # p_seijika_profle_data_sitelist ã‚¯ãƒ©ã‚¹ã‹ã‚‰ã‚µã‚¤ãƒˆæƒ…å ±ã‚’å–å¾—
        sitelist_elem = soup.find(class_='p_seijika_profle_data_sitelist')
        if sitelist_elem:
            site_links = sitelist_elem.find_all('a', href=True)
            websites = []
            for link in site_links:
                url = link.get('href', '')
                title = link.get_text(strip=True)
                if url and title:
                    websites.append({"url": url, "title": title})
            
            if websites:
                detailed_info["websites"] = websites
                # å…¬å¼ã‚µã‚¤ãƒˆï¼ˆæœ€åˆã®ãƒªãƒ³ã‚¯ï¼‰ã‚’åˆ¥é€”ä¿å­˜
                detailed_info["official_website"] = websites[0]["url"]
        
        # å¹´é½¢æƒ…å ±
        age_elem = soup.find(string=re.compile(r'(\d+)æ­³'))
        if age_elem:
            age_match = re.search(r'(\d+)æ­³', age_elem)
            if age_match:
                detailed_info["age_info"] = age_match.group(1)
        
        # è·æ¥­ãƒ»è‚©æ›¸ã
        occupation_selectors = [
            '.occupation', '.job', '.title', 
            '[class*="occupation"]', '[class*="job"]'
        ]
        for selector in occupation_selectors:
            elem = soup.select_one(selector)
            if elem:
                occupation = elem.get_text(strip=True)
                if occupation and len(occupation) < 50:
                    detailed_info["occupation"] = occupation
                break
        
        # çµŒæ­´ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«
        profile_selectors = [
            '.profile', '.career', '.history', '.bio',
            '[class*="profile"]', '[class*="career"]', '[class*="history"]'
        ]
        for selector in profile_selectors:
            elem = soup.select_one(selector)
            if elem:
                career = elem.get_text(strip=True)
                if career and len(career) > 20:  # æ„å‘³ã®ã‚ã‚‹çµŒæ­´æƒ…å ±ã®ã¿
                    detailed_info["career"] = career[:500]  # 500æ–‡å­—ã¾ã§
                break
        
        # å‡ºèº«åœ°
        birthplace_elem = soup.find(string=re.compile(r'å‡ºèº«.*[éƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘]'))
        if birthplace_elem:
            birthplace_match = re.search(r'å‡ºèº«[ï¼š:]*([éƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘\w]+)', birthplace_elem)
            if birthplace_match:
                detailed_info["birthplace"] = birthplace_match.group(1)
        
        logger.debug(f"è©³ç´°æƒ…å ±å–å¾—å®Œäº†: {len(detailed_info)}é …ç›®")
        
    except Exception as e:
        logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    return detailed_info

def extract_candidates_from_text(html_content, party_name, page_url, collector):
    """HTMLãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å€™è£œè€…åã‚’æŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    candidates = []
    
    try:
        # HTMLã‚¿ã‚°ã‚’é™¤å»
        soup = BeautifulSoup(html_content, 'html.parser')
        text = soup.get_text()
        
        # ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        invalid_names = [
            'ä¼šå“¡ç™»éŒ²', 'æ¯”ä¾‹ä»£è¡¨äºˆæƒ³', 'ãƒ­ã‚°ã‚¤ãƒ³', 'ã‚µã‚¤ãƒ³ã‚¢ãƒƒãƒ—', 
            'MYé¸æŒ™', 'é¸æŒ™åŒº', 'ã“ã®', 'ã‚³ãƒ³ãƒ†ãƒ³ãƒ„', 'äºˆæƒ³ã•ã‚Œã‚‹é¡”ã¶ã‚Œ',
            'ã«ã¤ã„ã¦', 'ã‚·ã‚§ã‚¢', 'ãƒšãƒ¼ã‚¸', 'æ”¿å…š', 'å…¨æ”¿å…š', 'ã«ã¤ã„ã¦',
            'ã“ã®ãƒšãƒ¼ã‚¸', 'ã‚’ã‚·ã‚§ã‚¢', 'ã™ã‚‹'
        ]
        
        # æ—¥æœ¬äººã®åå‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
        name_patterns = [
            r'([ä¸€-é¾¯]{2,4}[\\sã€€]*[ä¸€-é¾¯]{2,8})',  # æ¼¢å­—ã®ã¿
            r'([ä¸€-é¾¯]{2,4}[\\sã€€]*[ä¸€-é¾¯]{2,8}[ã‚¡-ãƒ¶]+)',  # æ¼¢å­— + ã‚«ã‚¿ã‚«ãƒŠ
        ]
        
        found_names = set()
        for pattern in name_patterns:
            matches = re.finditer(pattern, text)
            for i, match in enumerate(matches):
                full_name = match.group(1).strip()
                
                # åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if len(full_name) < 2 or len(full_name) > 10:
                    continue
                
                # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if any(invalid in full_name for invalid in invalid_names):
                    continue
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if full_name in found_names:
                    continue
                
                # æ¼¢å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if not re.search(r'[ä¸€-é¾¯]', full_name):
                    continue
                
                found_names.add(full_name)
                
                # åå‰ã¨ã‚«ã‚¿ã‚«ãƒŠã‚’åˆ†é›¢
                name, name_kana = collector.separate_name_and_kana(full_name)
                
                candidate_data = {
                    "candidate_id": f"hirei_text_{party_name}_{i}",
                    "name": name,
                    "prefecture": "æ¯”ä¾‹ä»£è¡¨",
                    "constituency": "å…¨å›½æ¯”ä¾‹", 
                    "constituency_type": "proportional",
                    "party": party_name,
                    "party_normalized": collector.normalize_party_name(party_name),
                    "profile_url": "",
                    "source_page": page_url,
                    "source": "go2senkyo_proportional_text_extraction",
                    "collected_at": datetime.now().isoformat()
                }
                
                if name_kana:
                    candidate_data["name_kana"] = name_kana
                
                candidates.append(candidate_data)
        
        # å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿ãƒ­ã‚°å‡ºåŠ›
        if candidates:
            logger.info(f"{party_name} ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º: {len(candidates)}å")
        
    except Exception as e:
        logger.debug(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({party_name}): {e}")
    
    return candidates

def merge_with_constituency_data_v2():
    """é¸æŒ™åŒºãƒ‡ãƒ¼ã‚¿ã¨æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ v2"""
    logger.info("ğŸ”— é¸æŒ™åŒºãƒ‡ãƒ¼ã‚¿ã¨æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ v2...")
    
    try:
        # æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿åé›†
        proportional_candidates = collect_proportional_v2()
        
        # æ—¢å­˜ã®é¸æŒ™åŒºãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
        latest_file = data_dir / "go2senkyo_optimized_latest.json"
        
        if latest_file.exists():
            with open(latest_file, 'r', encoding='utf-8') as f:
                constituency_data = json.load(f)
                constituency_candidates = constituency_data.get('data', [])
        else:
            constituency_candidates = []
        
        # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        all_candidates = constituency_candidates + proportional_candidates
        
        logger.info(f"ğŸ“Š çµ±åˆçµæœ v2:")
        logger.info(f"  é¸æŒ™åŒºå€™è£œè€…: {len(constituency_candidates)}å")
        logger.info(f"  æ¯”ä¾‹ä»£è¡¨å€™è£œè€…: {len(proportional_candidates)}å")
        logger.info(f"  ç·å€™è£œè€…æ•°: {len(all_candidates)}å")
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
        if proportional_candidates:  # æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿ä¿å­˜
            save_merged_data_v2(all_candidates, data_dir)
        else:
            logger.warning("æ¯”ä¾‹ä»£è¡¨ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã§ããªã‹ã£ãŸãŸã‚ã€ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        return all_candidates
        
    except Exception as e:
        logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚¨ãƒ©ãƒ¼ v2: {e}")
        raise

def save_merged_data_v2(candidates, output_dir):
    """çµ±åˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ v2"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    main_file = output_dir / f"go2senkyo_merged_v2_{timestamp}.json"
    latest_file = output_dir / "go2senkyo_optimized_latest.json"
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    constituency_stats = {}
    
    for candidate in candidates:
        party = candidate.get('party', 'ç„¡æ‰€å±')
        constituency_type = candidate.get('constituency_type', 'unknown')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        constituency_stats[constituency_type] = constituency_stats.get(constituency_type, 0) + 1
    
    save_data = {
        "metadata": {
            "data_type": "go2senkyo_merged_sangiin_2025_v2",
            "collection_method": "constituency_and_proportional_scraping_v2",
            "total_candidates": len(candidates),
            "generated_at": datetime.now().isoformat(),
            "source_site": "sangiin.go2senkyo.com",
            "coverage": {
                "constituency_types": len(constituency_stats),
                "parties": len(party_stats)
            },
            "collection_stats": {
                "total_candidates": len(candidates),
                "detailed_profiles": 0,
                "with_photos": 0,
                "with_policies": 0,
                "errors": 0
            },
            "quality_metrics": {
                "detail_coverage": "0%",
                "photo_coverage": "0%",
                "policy_coverage": "0%"
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_constituency_type": constituency_stats
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    with open(main_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº† v2: {main_file}")

if __name__ == "__main__":
    merge_with_constituency_data_v2()