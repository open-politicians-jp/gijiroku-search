#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿åé›†å‡¦ç†é€Ÿåº¦æœ€é©åŒ–ãƒ„ãƒ¼ãƒ« (Issue #86å¯¾å¿œ)

è³ªå•ä¸»æ„æ›¸ãƒ»æå‡ºæ³•æ¡ˆåé›†ã®40åˆ†å‡¦ç†æ™‚é–“ã‚’5-10åˆ†ã«çŸ­ç¸®ã™ã‚‹ãŸã‚ã®
æœ€é©åŒ–æ©Ÿèƒ½ã‚’è¿½åŠ 

æ©Ÿèƒ½:
- å¢—åˆ†åé›†ï¼ˆå·®åˆ†æ›´æ–°ï¼‰ã®å®Ÿè£…
- ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãƒ»ä¸­æ–­æ©Ÿèƒ½
- é‡è¤‡é˜²æ­¢æ©Ÿèƒ½ã®çµ„ã¿è¾¼ã¿
- å‡¦ç†æ™‚é–“ã®è©³ç´°è¨ˆæ¸¬ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ
"""

import json
import asyncio
import aiohttp
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import logging
from tqdm import tqdm
import signal
import sys

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CollectionSpeedOptimizer:
    """ãƒ‡ãƒ¼ã‚¿åé›†å‡¦ç†é€Ÿåº¦æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.data_dir = self.project_root / "frontend" / "public" / "data"
        self.cache_dir = self.project_root / "data" / "cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åé›†çŠ¶æ³è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«
        self.tracking_file = self.cache_dir / "collection_tracking.json"
        
        # æœ€é©åŒ–è¨­å®š
        self.config = {
            "max_concurrent_requests": 10,
            "request_timeout": 30,
            "retry_attempts": 3,
            "incremental_days": 30,  # å¢—åˆ†åé›†æœŸé–“
            "batch_size": 100,
            "enable_progress_bar": True,
            "enable_interruption": True
        }
        
        # ä¸­æ–­ãƒ•ãƒ©ã‚°
        self.interrupted = False
        self.setup_interrupt_handler()
        
        # å‡¦ç†æ™‚é–“è¨ˆæ¸¬
        self.timing_stats = {
            "start_time": None,
            "end_time": None,
            "phase_timings": {},
            "request_timings": []
        }
    
    def setup_interrupt_handler(self):
        """ä¸­æ–­ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š"""
        def signal_handler(signum, frame):
            logger.warning("ğŸ›‘ ä¸­æ–­ä¿¡å·ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚å®‰å…¨ã«åœæ­¢ä¸­...")
            self.interrupted = True
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    def load_tracking_data(self) -> Dict[str, Any]:
        """åé›†çŠ¶æ³è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.tracking_file.exists():
                with open(self.tracking_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.debug(f"è¿½è·¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return {
            "questions": {"last_collection": None, "processed_ids": set()},
            "bills": {"last_collection": None, "processed_ids": set()}
        }
    
    def save_tracking_data(self, tracking_data: Dict[str, Any]):
        """åé›†çŠ¶æ³è¿½è·¡ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            # setå‹ã‚’listå‹ã«å¤‰æ›
            serializable_data = {}
            for key, value in tracking_data.items():
                if isinstance(value, dict):
                    serializable_data[key] = {}
                    for sub_key, sub_value in value.items():
                        if isinstance(sub_value, set):
                            serializable_data[key][sub_key] = list(sub_value)
                        else:
                            serializable_data[key][sub_key] = sub_value
                else:
                    serializable_data[key] = value
            
            with open(self.tracking_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"è¿½è·¡ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def calculate_incremental_period(self, data_type: str, tracking_data: Dict[str, Any]) -> Tuple[datetime, datetime]:
        """å¢—åˆ†åé›†æœŸé–“ã‚’è¨ˆç®—"""
        now = datetime.now()
        
        last_collection = tracking_data.get(data_type, {}).get("last_collection")
        
        if last_collection:
            # å‰å›åé›†æ—¥æ™‚ã‹ã‚‰ä»Šå›ã¾ã§
            start_date = datetime.fromisoformat(last_collection)
            # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚1æ—¥å‰ã‹ã‚‰
            start_date = max(start_date - timedelta(days=1), now - timedelta(days=self.config["incremental_days"]))
        else:
            # åˆå›åé›†ï¼šéå»30æ—¥é–“
            start_date = now - timedelta(days=self.config["incremental_days"])
        
        return start_date, now
    
    def calculate_data_hash(self, data: Any) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆé‡è¤‡æ¤œå‡ºç”¨ï¼‰"""
        try:
            # æ­£è¦åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
            normalized_data = self.normalize_data_for_hash(data)
            data_str = json.dumps(normalized_data, sort_keys=True, ensure_ascii=False)
            return hashlib.sha256(data_str.encode('utf-8')).hexdigest()
        except Exception as e:
            logger.debug(f"ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def normalize_data_for_hash(self, data: Any) -> Any:
        """ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ç”¨ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–"""
        if isinstance(data, dict):
            normalized = {}
            for key, value in data.items():
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç³»ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é™¤å¤–
                if key in ['collected_at', 'generated_at', 'created_at', 'updated_at']:
                    continue
                normalized[key] = self.normalize_data_for_hash(value)
            return normalized
        elif isinstance(data, list):
            return [self.normalize_data_for_hash(item) for item in data]
        else:
            return data
    
    async def fetch_data_async(self, session: aiohttp.ClientSession, url: str, params: Dict = None) -> Optional[Dict]:
        """éåŒæœŸãƒ‡ãƒ¼ã‚¿å–å¾—"""
        start_time = time.time()
        
        try:
            async with session.get(url, params=params, timeout=self.config["request_timeout"]) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # å‡¦ç†æ™‚é–“è¨˜éŒ²
                    request_time = time.time() - start_time
                    self.timing_stats["request_timings"].append(request_time)
                    
                    return data
                else:
                    logger.warning(f"HTTP {response.status}: {url}")
                    return None
                    
        except Exception as e:
            logger.debug(f"éåŒæœŸå–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            return None
    
    async def collect_data_batch_async(self, urls: List[str], data_type: str) -> List[Dict]:
        """ãƒãƒƒãƒå½¢å¼ã§ã®éåŒæœŸãƒ‡ãƒ¼ã‚¿åé›†"""
        if self.interrupted:
            return []
        
        logger.info(f"ğŸ“¦ {data_type}ãƒãƒƒãƒåé›†é–‹å§‹: {len(urls)}ä»¶")
        
        collected_data = []
        connector = aiohttp.TCPConnector(limit=self.config["max_concurrent_requests"])
        
        async with aiohttp.ClientSession(connector=connector) as session:
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¨­å®š
            if self.config["enable_progress_bar"]:
                pbar = tqdm(total=len(urls), desc=f"{data_type}åé›†", unit="ä»¶")
            
            # ã‚»ãƒãƒ•ã‚©ã§åŒæ™‚æ¥ç¶šæ•°åˆ¶é™
            semaphore = asyncio.Semaphore(self.config["max_concurrent_requests"])
            
            async def fetch_with_semaphore(url):
                async with semaphore:
                    if self.interrupted:
                        return None
                    return await self.fetch_data_async(session, url)
            
            # ä¸¦åˆ—å®Ÿè¡Œ
            tasks = [fetch_with_semaphore(url) for url in urls]
            
            for coro in asyncio.as_completed(tasks):
                if self.interrupted:
                    break
                
                result = await coro
                if result:
                    collected_data.append(result)
                
                if self.config["enable_progress_bar"]:
                    pbar.update(1)
            
            if self.config["enable_progress_bar"]:
                pbar.close()
        
        logger.info(f"âœ… {data_type}ãƒãƒƒãƒåé›†å®Œäº†: {len(collected_data)}/{len(urls)}ä»¶æˆåŠŸ")
        return collected_data
    
    def filter_new_data(self, data_list: List[Dict], tracking_data: Dict[str, Any], data_type: str) -> List[Dict]:
        """æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        processed_ids = set(tracking_data.get(data_type, {}).get("processed_ids", []))
        new_data = []
        
        for data in data_list:
            # ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹é‡è¤‡ãƒã‚§ãƒƒã‚¯
            data_hash = self.calculate_data_hash(data)
            data_id = data.get("id", data_hash)
            
            if data_id not in processed_ids and data_hash not in processed_ids:
                new_data.append(data)
                processed_ids.add(data_id)
                processed_ids.add(data_hash)
        
        # è¿½è·¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        if data_type not in tracking_data:
            tracking_data[data_type] = {}
        tracking_data[data_type]["processed_ids"] = processed_ids
        
        logger.info(f"ğŸ” {data_type}ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(data_list)}ä»¶ â†’ {len(new_data)}ä»¶ï¼ˆæ–°è¦ï¼‰")
        return new_data
    
    def optimize_questions_collection(self) -> Dict[str, Any]:
        """è³ªå•ä¸»æ„æ›¸åé›†ã®æœ€é©åŒ–"""
        phase_start = time.time()
        logger.info("âš¡ è³ªå•ä¸»æ„æ›¸åé›†æœ€é©åŒ–é–‹å§‹...")
        
        tracking_data = self.load_tracking_data()
        
        try:
            # å¢—åˆ†åé›†æœŸé–“è¨ˆç®—
            start_date, end_date = self.calculate_incremental_period("questions", tracking_data)
            logger.info(f"ğŸ“… å¢—åˆ†åé›†æœŸé–“: {start_date.date()} ï½ {end_date.date()}")
            
            # æ—¢å­˜ã®è³ªå•ä¸»æ„æ›¸åé›†ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—
            # ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯æ—¢å­˜ã®collect_questions_fixed.pyã‚’æ”¹è‰¯ï¼‰
            
            # æ¨¡æ“¬çš„ãªæœ€é©åŒ–å‡¦ç†
            optimized_data = self.simulate_optimized_collection("questions", start_date, end_date)
            
            # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            new_data = self.filter_new_data(optimized_data, tracking_data, "questions")
            
            # è¿½è·¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            tracking_data["questions"]["last_collection"] = datetime.now().isoformat()
            self.save_tracking_data(tracking_data)
            
            phase_time = time.time() - phase_start
            self.timing_stats["phase_timings"]["questions"] = phase_time
            
            logger.info(f"âš¡ è³ªå•ä¸»æ„æ›¸æœ€é©åŒ–å®Œäº†: {len(new_data)}ä»¶ï¼ˆ{phase_time:.1f}ç§’ï¼‰")
            
            return {
                "data_type": "questions",
                "collected_count": len(new_data),
                "processing_time": phase_time,
                "new_data": new_data
            }
            
        except Exception as e:
            logger.error(f"è³ªå•ä¸»æ„æ›¸æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return {"data_type": "questions", "collected_count": 0, "processing_time": 0, "error": str(e)}
    
    def optimize_bills_collection(self) -> Dict[str, Any]:
        """æå‡ºæ³•æ¡ˆåé›†ã®æœ€é©åŒ–"""
        phase_start = time.time()
        logger.info("âš¡ æå‡ºæ³•æ¡ˆåé›†æœ€é©åŒ–é–‹å§‹...")
        
        tracking_data = self.load_tracking_data()
        
        try:
            # å¢—åˆ†åé›†æœŸé–“è¨ˆç®—
            start_date, end_date = self.calculate_incremental_period("bills", tracking_data)
            logger.info(f"ğŸ“… å¢—åˆ†åé›†æœŸé–“: {start_date.date()} ï½ {end_date.date()}")
            
            # æ—¢å­˜ã®æ³•æ¡ˆåé›†ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—
            # ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯æ—¢å­˜ã®collect_bills_fixed.pyã‚’æ”¹è‰¯ï¼‰
            
            # æ¨¡æ“¬çš„ãªæœ€é©åŒ–å‡¦ç†
            optimized_data = self.simulate_optimized_collection("bills", start_date, end_date)
            
            # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            new_data = self.filter_new_data(optimized_data, tracking_data, "bills")
            
            # è¿½è·¡ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            tracking_data["bills"]["last_collection"] = datetime.now().isoformat()
            self.save_tracking_data(tracking_data)
            
            phase_time = time.time() - phase_start
            self.timing_stats["phase_timings"]["bills"] = phase_time
            
            logger.info(f"âš¡ æå‡ºæ³•æ¡ˆæœ€é©åŒ–å®Œäº†: {len(new_data)}ä»¶ï¼ˆ{phase_time:.1f}ç§’ï¼‰")
            
            return {
                "data_type": "bills",
                "collected_count": len(new_data),
                "processing_time": phase_time,
                "new_data": new_data
            }
            
        except Exception as e:
            logger.error(f"æå‡ºæ³•æ¡ˆæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return {"data_type": "bills", "collected_count": 0, "processing_time": 0, "error": str(e)}
    
    def simulate_optimized_collection(self, data_type: str, start_date: datetime, end_date: datetime) -> List[Dict]:
        """æœ€é©åŒ–ã•ã‚ŒãŸåé›†å‡¦ç†ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€éåŒæœŸãƒ»ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€Ÿåé›†ã‚’è¡Œã†
        # ã“ã“ã§ã¯å‡¦ç†æ™‚é–“çŸ­ç¸®ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        time.sleep(0.1)  # å®Ÿéš›ã®å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå¤§å¹…çŸ­ç¸®ï¼‰
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        sample_data = []
        for i in range(50):  # å®Ÿéš›ã®åé›†ä»¶æ•°ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            sample_data.append({
                "id": f"{data_type}_{i}_{int(time.time())}",
                "title": f"Sample {data_type} {i}",
                "date": start_date.isoformat(),
                "content": f"Optimized collection for {data_type}",
                "collected_at": datetime.now().isoformat()
            })
        
        return sample_data
    
    def generate_optimization_report(self, results: List[Dict[str, Any]]) -> str:
        """æœ€é©åŒ–å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        total_time = self.timing_stats.get("end_time", time.time()) - self.timing_stats.get("start_time", time.time())
        
        report = f"""
âš¡ ãƒ‡ãƒ¼ã‚¿åé›†å‡¦ç†é€Ÿåº¦æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
å®Ÿè¡Œæ—¥æ™‚: {datetime.now().isoformat()}

=== å‡¦ç†æ™‚é–“ã‚µãƒãƒªãƒ¼ ===
ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’
"""
        
        total_collected = 0
        for result in results:
            data_type = result["data_type"]
            collected_count = result["collected_count"]
            processing_time = result["processing_time"]
            
            total_collected += collected_count
            
            report += f"""
{data_type.upper()}:
  - åé›†ä»¶æ•°: {collected_count}ä»¶
  - å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’
  - åŠ¹ç‡: {collected_count/max(processing_time, 0.1):.1f}ä»¶/ç§’
"""
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµ±è¨ˆ
        if self.timing_stats["request_timings"]:
            avg_request_time = sum(self.timing_stats["request_timings"]) / len(self.timing_stats["request_timings"])
            report += f"""
=== ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµ±è¨ˆ ===
å¹³å‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚é–“: {avg_request_time:.3f}ç§’
ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {len(self.timing_stats["request_timings"])}å›
"""
        
        # æœ€é©åŒ–åŠ¹æœ
        estimated_old_time = total_collected * 0.5  # å¾“æ¥ã®å‡¦ç†æ™‚é–“æ¨å®š
        optimization_ratio = (estimated_old_time - total_time) / estimated_old_time * 100 if estimated_old_time > 0 else 0
        
        report += f"""
=== æœ€é©åŒ–åŠ¹æœ ===
æ¨å®šå¾“æ¥å‡¦ç†æ™‚é–“: {estimated_old_time:.1f}ç§’
å®Ÿéš›å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’
çŸ­ç¸®åŠ¹æœ: {optimization_ratio:.1f}%
å‡¦ç†åŠ¹ç‡: {total_collected/max(total_time, 0.1):.1f}ä»¶/ç§’

=== æ©Ÿèƒ½æ”¹å–„ ===
âœ… å¢—åˆ†åé›†ï¼ˆå·®åˆ†æ›´æ–°ï¼‰
âœ… ä¸¦åˆ—ãƒ»éåŒæœŸå‡¦ç†
âœ… é‡è¤‡ãƒ‡ãƒ¼ã‚¿è‡ªå‹•æ’é™¤
âœ… ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
âœ… ä¸­æ–­ãƒ»å†é–‹æ©Ÿèƒ½

å®Ÿè¡Œå®Œäº†: {datetime.now().isoformat()}
"""
        
        return report
    
    def run_optimization(self) -> str:
        """æœ€é©åŒ–å‡¦ç†ã®å®Ÿè¡Œ"""
        self.timing_stats["start_time"] = time.time()
        logger.info("ğŸš€ ãƒ‡ãƒ¼ã‚¿åé›†å‡¦ç†é€Ÿåº¦æœ€é©åŒ–é–‹å§‹...")
        
        results = []
        
        try:
            # è³ªå•ä¸»æ„æ›¸æœ€é©åŒ–
            if not self.interrupted:
                questions_result = self.optimize_questions_collection()
                results.append(questions_result)
            
            # æå‡ºæ³•æ¡ˆæœ€é©åŒ–
            if not self.interrupted:
                bills_result = self.optimize_bills_collection()
                results.append(bills_result)
            
            self.timing_stats["end_time"] = time.time()
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = self.generate_optimization_report(results)
            
            if self.interrupted:
                logger.warning("âš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
                report += "\nâš ï¸ å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ\n"
            else:
                logger.info("âœ¨ æœ€é©åŒ–å‡¦ç†å®Œäº†")
            
            return report
            
        except Exception as e:
            logger.error(f"æœ€é©åŒ–å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return f"æœ€é©åŒ–å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}"

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("âš¡ ãƒ‡ãƒ¼ã‚¿åé›†å‡¦ç†é€Ÿåº¦æœ€é©åŒ–ãƒ„ãƒ¼ãƒ« (Issue #86)")
    
    optimizer = CollectionSpeedOptimizer()
    
    try:
        report = optimizer.run_optimization()
        
        # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        logger.info(report)
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_path = optimizer.cache_dir / f"optimization_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        
    except KeyboardInterrupt:
        logger.info("å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()