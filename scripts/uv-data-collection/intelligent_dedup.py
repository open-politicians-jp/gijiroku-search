#!/usr/bin/env python3
"""
ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé‡è¤‡é™¤å»
åŒä¸€äººç‰©ã®è¤‡æ•°çœŒç™»éŒ²ã‚’é©åˆ‡ã«å‡¦ç†
"""

import json
import logging
from datetime import datetime
from pathlib import Path

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def intelligent_dedup():
    """ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé‡è¤‡é™¤å»"""
    logger.info("ğŸ§  ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé‡è¤‡é™¤å»é–‹å§‹...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    input_file = data_dir / "go2senkyo_complete_structured_20250702_133412.json"
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    original_count = len(data['data'])
    logger.info(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {original_count}å")
    
    # å€™è£œè€…ã‚’åå‰ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    name_groups = {}
    for candidate in data['data']:
        name = candidate.get('name', '')
        if name not in name_groups:
            name_groups[name] = []
        name_groups[name].append(candidate)
    
    # é‡è¤‡é™¤å»å‡¦ç†
    unique_candidates = []
    removed_duplicates = []
    
    for name, candidates in name_groups.items():
        if len(candidates) == 1:
            # é‡è¤‡ãªã— - ãã®ã¾ã¾è¿½åŠ 
            unique_candidates.append(candidates[0])
        else:
            # é‡è¤‡ã‚ã‚Š - æœ€é©ãªå€™è£œè€…ã‚’é¸æŠ
            logger.info(f"ğŸ” é‡è¤‡æ¤œå‡º: {name} ({len(candidates)}ä»¶)")
            
            # åŒä¸€äººç‰©ã®åˆ¤å®šï¼ˆURLãŒåŒã˜å ´åˆï¼‰
            urls = [c.get('profile_url', '') for c in candidates]
            unique_urls = set(url for url in urls if url)
            
            if len(unique_urls) <= 1:
                # åŒä¸€äººç‰©ã®è¤‡æ•°çœŒç™»éŒ² - æœ€åˆã«åé›†ã•ã‚ŒãŸéƒ½é“åºœçœŒã‚’æ¡ç”¨
                selected = candidates[0]
                for candidate in candidates[1:]:
                    removed_duplicates.append(candidate)
                    logger.info(f"  ğŸ—‘ï¸ é™¤å»: {candidate['prefecture']}")
                
                unique_candidates.append(selected)
                logger.info(f"  âœ… æ¡ç”¨: {selected['prefecture']}")
            else:
                # ç•°ãªã‚‹äººç‰©ï¼ˆåŒåç•°äººï¼‰ - å…¨ã¦ä¿æŒ
                unique_candidates.extend(candidates)
                logger.info(f"  ğŸ‘¥ åŒåç•°äººã¨ã—ã¦å…¨ã¦ä¿æŒ")
    
    # çµ±è¨ˆå†è¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    
    for candidate in unique_candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
    
    # æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    final_data = {
        "metadata": {
            "data_type": "go2senkyo_complete_structured_sangiin_2025_intelligent_dedup",
            "collection_method": "structured_html_extraction_all_47_prefectures_intelligent_dedup",
            "total_candidates": len(unique_candidates),
            "candidates_with_kana": len([c for c in unique_candidates if c.get('name_kana')]),
            "successful_prefectures": 47,
            "failed_prefectures": 0,
            "intelligent_duplicates_removed": len(removed_duplicates),
            "generated_at": datetime.now().isoformat(),
            "source_site": "sangiin.go2senkyo.com",
            "coverage": {
                "constituency_types": 1,
                "parties": len(party_stats),
                "prefectures": len(prefecture_stats)
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": prefecture_stats,
            "by_constituency_type": {"single_member": len(unique_candidates)}
        },
        "data": unique_candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = data_dir / f"go2senkyo_intelligent_dedup_{timestamp}.json"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {output_file}")
    logger.info(f"ğŸ“ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {latest_file}")
    
    # çµæœå ±å‘Š
    logger.info(f"\nâœ… ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé‡è¤‡é™¤å»å®Œäº†:")
    logger.info(f"  å…ƒãƒ‡ãƒ¼ã‚¿: {original_count}å")
    logger.info(f"  ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆé™¤å»æ•°: {len(removed_duplicates)}å")
    logger.info(f"  æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {len(unique_candidates)}å")
    logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
    logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_stats)}éƒ½é“åºœçœŒ")
    
    if removed_duplicates:
        logger.info(f"\nğŸ—‘ï¸ é™¤å»ã•ã‚ŒãŸé‡è¤‡ï¼ˆåŒä¸€äººç‰©ã®è¤‡æ•°çœŒç™»éŒ²ï¼‰:")
        for candidate in removed_duplicates:
            logger.info(f"  - {candidate['name']} ({candidate.get('prefecture', 'ä¸æ˜')})")
    
    return output_file

if __name__ == "__main__":
    intelligent_dedup()