#!/usr/bin/env python3
"""
å…¬å¼ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†
- ç·å‹™çœé¸æŒ™éƒ¨
- å„éƒ½é“åºœçœŒé¸æŒ™ç®¡ç†å§”å“¡ä¼š
- ä¸»è¦æ”¿å…šå…¬å¼ã‚µã‚¤ãƒˆ
"""

import json
import logging
import requests
import time
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import random

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OfficialSourcesCollector:
    def __init__(self):
        self.session = requests.Session()
        
        # æ—¥æœ¬ã®User-Agentãƒªã‚¹ãƒˆ
        self.japanese_user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]
        
        self.setup_session()
        
        # ä¸»è¦æ”¿å…šã®ã‚µã‚¤ãƒˆ
        self.party_sites = {
            "è‡ªç”±æ°‘ä¸»å…š": "https://www.jimin.jp/",
            "ç«‹æ†²æ°‘ä¸»å…š": "https://cdp-japan.jp/", 
            "æ—¥æœ¬ç¶­æ–°ã®ä¼š": "https://o-ishin.jp/",
            "å…¬æ˜å…š": "https://www.komei.or.jp/",
            "æ—¥æœ¬å…±ç”£å…š": "https://www.jcp.or.jp/",
            "å›½æ°‘æ°‘ä¸»å…š": "https://new-kokumin.jp/",
            "ã‚Œã„ã‚æ–°é¸çµ„": "https://reiwa-shinsengumi.com/",
            "å‚æ”¿å…š": "https://www.sanseito.jp/",
            "ç¤¾ä¼šæ°‘ä¸»å…š": "https://sdp.or.jp/",
        }

    def setup_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š"""
        user_agent = random.choice(self.japanese_user_agents)
        
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,ja-JP;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://www.google.co.jp/',
        })

    def collect_soumu_data(self):
        """ç·å‹™çœé¸æŒ™éƒ¨ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("ğŸ” ç·å‹™çœé¸æŒ™éƒ¨ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        
        # ç·å‹™çœé¸æŒ™é–¢é€£URL
        soumu_urls = [
            "https://www.soumu.go.jp/senkyo/",
            "https://www.soumu.go.jp/senkyo/sangiin2025/",
            "https://www.soumu.go.jp/senkyo/senkyo_s/",
        ]
        
        all_candidates = []
        
        for url in soumu_urls:
            try:
                logger.info(f"ğŸ“Š ç·å‹™çœã‚¢ã‚¯ã‚»ã‚¹: {url}")
                response = self.session.get(url, timeout=30)
                
                if response.status_code == 200:
                    logger.info(f"âœ… ç·å‹™çœãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(response.text):,} æ–‡å­—")
                    
                    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    debug_dir = Path(__file__).parent / "debug"
                    debug_dir.mkdir(exist_ok=True)
                    
                    filename = url.replace('https://', '').replace('/', '_') + '.html'
                    with open(debug_dir / filename, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    candidates = self.extract_soumu_candidates(soup, url)
                    all_candidates.extend(candidates)
                    
                    if candidates:
                        logger.info(f"ğŸ“‹ ç·å‹™çœå€™è£œè€…ç™ºè¦‹: {len(candidates)}å")
                else:
                    logger.warning(f"âš ï¸ ç·å‹™çœã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"âŒ ç·å‹™çœåé›†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                continue
        
        logger.info(f"ğŸ¯ ç·å‹™çœåé›†å®Œäº†: ç·è¨ˆ {len(all_candidates)}å")
        return all_candidates

    def extract_soumu_candidates(self, soup, source_url):
        """ç·å‹™çœã‚µã‚¤ãƒˆã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        candidates = []
        
        try:
            # ç·å‹™çœã®é¸æŒ™é–¢é€£ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
            election_links = self.find_election_links(soup, source_url)
            logger.info(f"ç·å‹™çœé¸æŒ™é–¢é€£ãƒªãƒ³ã‚¯: {len(election_links)}ä»¶")
            
            for link_text, link_url in election_links[:5]:  # æœ€åˆã®5ä»¶
                try:
                    page_candidates = self.collect_page_candidates(link_text, link_url, "soumu")
                    candidates.extend(page_candidates)
                    time.sleep(2)
                except Exception as e:
                    logger.debug(f"ç·å‹™çœãƒšãƒ¼ã‚¸åé›†ã‚¨ãƒ©ãƒ¼ ({link_text}): {e}")
                    continue
            
        except Exception as e:
            logger.error(f"ç·å‹™çœå€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def find_election_links(self, soup, base_url):
        """é¸æŒ™é–¢é€£ãƒªãƒ³ã‚¯ã®æ¢ç´¢"""
        links = []
        
        try:
            all_links = soup.find_all('a', href=True)
            
            election_keywords = [
                'å‚è­°é™¢', 'å€™è£œè€…', 'ç«‹å€™è£œ', 'é¸æŒ™åŒº', 'æ¯”ä¾‹ä»£è¡¨',
                '2025', 'ä»¤å’Œ7å¹´', 'æœŸæ—¥å‰æŠ•ç¥¨', 'æŠ•ç¥¨æ—¥'
            ]
            
            for link in all_links:
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # é¸æŒ™é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                if any(keyword in text for keyword in election_keywords):
                    full_url = self.resolve_url(href, base_url)
                    links.append((text, full_url))
            
            # é‡è¤‡é™¤å»
            unique_links = list(dict(links).items())
            return unique_links
            
        except Exception as e:
            logger.error(f"é¸æŒ™é–¢é€£ãƒªãƒ³ã‚¯æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def collect_party_candidates(self):
        """å„æ”¿å…šã‚µã‚¤ãƒˆã‹ã‚‰å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("ğŸ” æ”¿å…šã‚µã‚¤ãƒˆå€™è£œè€…åé›†é–‹å§‹...")
        
        all_candidates = []
        
        for party_name, party_url in self.party_sites.items():
            try:
                logger.info(f"ğŸ“Š {party_name} ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹: {party_url}")
                response = self.session.get(party_url, timeout=30)
                
                if response.status_code == 200:
                    logger.info(f"âœ… {party_name} ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(response.text):,} æ–‡å­—")
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    party_candidates = self.extract_party_candidates(soup, party_url, party_name)
                    
                    # æ”¿å…šæƒ…å ±ã‚’è¿½åŠ 
                    for candidate in party_candidates:
                        candidate["party"] = party_name
                        candidate["party_normalized"] = party_name
                    
                    all_candidates.extend(party_candidates)
                    
                    logger.info(f"ğŸ“‹ {party_name} å€™è£œè€…ç™ºè¦‹: {len(party_candidates)}å")
                else:
                    logger.warning(f"âš ï¸ {party_name} ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                
                time.sleep(3)  # æ”¿å…šã‚µã‚¤ãƒˆã¸ã®é…æ…®
                
            except Exception as e:
                logger.error(f"âŒ {party_name} åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        logger.info(f"ğŸ¯ æ”¿å…šã‚µã‚¤ãƒˆåé›†å®Œäº†: ç·è¨ˆ {len(all_candidates)}å")
        return all_candidates

    def extract_party_candidates(self, soup, source_url, party_name):
        """æ”¿å…šã‚µã‚¤ãƒˆã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        candidates = []
        
        try:
            # å€™è£œè€…é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢
            candidate_links = self.find_candidate_links(soup, source_url)
            logger.info(f"{party_name} å€™è£œè€…é–¢é€£ãƒªãƒ³ã‚¯: {len(candidate_links)}ä»¶")
            
            # å€™è£œè€…ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‚’æ¢ã™
            for link_text, link_url in candidate_links[:10]:  # æœ€åˆã®10ä»¶
                try:
                    if any(keyword in link_text.lower() for keyword in ['å€™è£œ', 'candidate', 'å‚è­°é™¢', 'è­°å“¡']):
                        page_candidates = self.collect_page_candidates(link_text, link_url, f"party_{party_name}")
                        candidates.extend(page_candidates)
                        time.sleep(1)
                except Exception as e:
                    logger.debug(f"{party_name} å€™è£œè€…ãƒšãƒ¼ã‚¸åé›†ã‚¨ãƒ©ãƒ¼ ({link_text}): {e}")
                    continue
            
            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚‚å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º
            main_candidates = self.extract_candidates_from_page(soup, source_url, f"party_{party_name}")
            candidates.extend(main_candidates)
            
        except Exception as e:
            logger.error(f"{party_name} å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def find_candidate_links(self, soup, base_url):
        """å€™è£œè€…é–¢é€£ãƒªãƒ³ã‚¯ã®æ¢ç´¢"""
        links = []
        
        try:
            all_links = soup.find_all('a', href=True)
            
            candidate_keywords = [
                'å€™è£œè€…', 'å€™è£œ', 'candidate', 'å‚è­°é™¢', 'è­°å“¡',
                'ç«‹å€™è£œ', 'æ”¿ç­–', 'profile', 'ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«',
                'ä¸€è¦§', 'list', 'åç°¿'
            ]
            
            for link in all_links:
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # å€™è£œè€…é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                if any(keyword in text.lower() for keyword in candidate_keywords):
                    full_url = self.resolve_url(href, base_url)
                    links.append((text, full_url))
            
            # é‡è¤‡é™¤å»
            unique_links = list(dict(links).items())
            return unique_links
            
        except Exception as e:
            logger.error(f"å€™è£œè€…é–¢é€£ãƒªãƒ³ã‚¯æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def collect_page_candidates(self, page_title, page_url, source_type):
        """å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‹ã‚‰å€™è£œè€…åé›†"""
        candidates = []
        
        try:
            logger.debug(f"ğŸ“ ãƒšãƒ¼ã‚¸åé›†: {page_title} ({source_type})")
            response = self.session.get(page_url, timeout=30)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                candidates = self.extract_candidates_from_page(soup, page_url, source_type)
                
                if candidates:
                    logger.debug(f"âœ… {page_title} åé›†å®Œäº†: {len(candidates)}å")
            
        except Exception as e:
            logger.debug(f"âŒ ãƒšãƒ¼ã‚¸åé›†ã‚¨ãƒ©ãƒ¼ ({page_title}): {e}")
        
        return candidates

    def extract_candidates_from_page(self, soup, source_url, source_type):
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º"""
        candidates = []
        
        try:
            # å€™è£œè€…æƒ…å ±ã‚‰ã—ãè¦ç´ ã‚’æ¢ç´¢
            candidate_selectors = [
                '[class*="candidate"]',
                '[class*="member"]',
                '[class*="person"]',
                '[class*="profile"]',
                'li[class*="item"]',
                'div[class*="card"]'
            ]
            
            for selector in candidate_selectors:
                elements = soup.select(selector)
                
                for element in elements:
                    try:
                        candidate = self.extract_candidate_info(element, source_url, source_type)
                        if candidate:
                            candidates.append(candidate)
                    except Exception as e:
                        logger.debug(f"å€™è£œè€…æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®å€™è£œè€…åæŠ½å‡ºã‚‚å®Ÿè¡Œ
            text_candidates = self.extract_candidates_from_text(soup.get_text(), source_url, source_type)
            candidates.extend(text_candidates)
            
        except Exception as e:
            logger.debug(f"ãƒšãƒ¼ã‚¸å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def extract_candidate_info(self, element, source_url, source_type):
        """HTMLè¦ç´ ã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": "",
                "name": "",
                "name_kana": "",
                "prefecture": "",
                "constituency": "",
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "age": "",
                "profile_url": "",
                "source_page": source_url,
                "source": source_type,
                "collected_at": datetime.now().isoformat()
            }
            
            # åå‰ã®æŠ½å‡º
            text = element.get_text()
            
            # æ—¥æœ¬äººåã‚‰ã—ããƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
            name_patterns = [
                r'([ä¸€-é¾¯ã²ã‚‰ãŒãª]{2,6})\s*([ä¸€-é¾¯ã²ã‚‰ãŒãª]{2,6})',  # æ¼¢å­—ãƒ»ã²ã‚‰ãŒãªåå‰
                r'([ä¸€-é¾¯ã²ã‚‰ãŒãª]{2,10})',  # å˜ä½“ã®æ¼¢å­—ãƒ»ã²ã‚‰ãŒãªåå‰
            ]
            
            for pattern in name_patterns:
                match = re.search(pattern, text)
                if match:
                    potential_name = match.group(1).strip()
                    
                    # åå‰ã¨ã—ã¦é©åˆ‡ã‹ãƒã‚§ãƒƒã‚¯
                    if self.is_valid_name(potential_name):
                        candidate["name"] = potential_name
                        break
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLã®æŠ½å‡º
            profile_link = element.find('a', href=True)
            if profile_link:
                candidate["profile_url"] = self.resolve_url(profile_link.get('href'), source_url)
            
            # å€™è£œè€…IDã®ç”Ÿæˆ
            if candidate["name"] and len(candidate["name"]) > 1:
                candidate["candidate_id"] = f"{source_type}_{hash(candidate['name']) % 1000000}"
                return candidate
            
        except Exception as e:
            logger.debug(f"å€™è£œè€…æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return None

    def extract_candidates_from_text(self, text, source_url, source_type):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å€™è£œè€…åã‚’æŠ½å‡º"""
        candidates = []
        
        try:
            # å€™è£œè€…åã‚‰ã—ããƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
            name_patterns = [
                r'å€™è£œè€…?[:ï¼š]\s*([ä¸€-é¾¯ã²ã‚‰ãŒãª\sã€ï¼Œ]+)',
                r'ç«‹å€™è£œè€…?[:ï¼š]\s*([ä¸€-é¾¯ã²ã‚‰ãŒãª\sã€ï¼Œ]+)',
                r'([ä¸€-é¾¯ã²ã‚‰ãŒãª]{2,6})\s*\(\s*([ã‚¡-ãƒ¶ãƒ¼\s]+)\s*\)',  # åå‰(èª­ã¿)å½¢å¼
                r'([ä¸€-é¾¯ã²ã‚‰ãŒãª]{2,6})\s*([ä¸€-é¾¯ã²ã‚‰ãŒãª]{2,6})\s*\d+æ­³',  # åå‰ å¹´é½¢å½¢å¼
            ]
            
            for pattern in name_patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    names_text = match.group(1)
                    
                    # è¤‡æ•°åå‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯åˆ†å‰²
                    potential_names = re.split(r'[ã€ï¼Œ\s]+', names_text)
                    
                    for name in potential_names:
                        name = name.strip()
                        if self.is_valid_name(name):
                            candidate = {
                                "candidate_id": f"{source_type}_text_{hash(name) % 1000000}",
                                "name": name,
                                "name_kana": match.group(2).strip() if len(match.groups()) > 1 else "",
                                "prefecture": "",
                                "constituency": "",
                                "constituency_type": "single_member",
                                "party": "æœªåˆ†é¡",
                                "party_normalized": "æœªåˆ†é¡",
                                "source_page": source_url,
                                "source": source_type,
                                "collected_at": datetime.now().isoformat()
                            }
                            candidates.append(candidate)
            
        except Exception as e:
            logger.debug(f"ãƒ†ã‚­ã‚¹ãƒˆå€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def is_valid_name(self, name):
        """åå‰ã¨ã—ã¦é©åˆ‡ã‹ãƒã‚§ãƒƒã‚¯"""
        if not name or len(name) < 2 or len(name) > 10:
            return False
        
        # é™¤å¤–ã™ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        invalid_keywords = [
            'æ”¿ç­–', 'å…¬ç´„', 'é¸æŒ™', 'æŠ•ç¥¨', 'è­°å“¡', 'å…š', 'ä¼š', 'å§”å“¡', 
            'å¤§è‡£', 'ç·ç†', 'é¦–ç›¸', 'ä»£è¡¨', 'å¹¹äº‹', 'äº‹å‹™', 'é€£çµ¡',
            'è©³ç´°', 'ã«ã¤ã„ã¦', 'ã“ã¡ã‚‰', 'ãƒšãƒ¼ã‚¸', 'ã‚µã‚¤ãƒˆ', 'ãƒ›ãƒ¼ãƒ '
        ]
        
        for keyword in invalid_keywords:
            if keyword in name:
                return False
        
        return True

    def resolve_url(self, href, base_url):
        """ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›"""
        if href.startswith('http'):
            return href
        elif href.startswith('/'):
            base_domain = '/'.join(base_url.split('/')[:3])
            return base_domain + href
        else:
            return base_url.rstrip('/') + '/' + href

    def collect_all_official_sources(self):
        """ã™ã¹ã¦ã®å…¬å¼ã‚½ãƒ¼ã‚¹ã‹ã‚‰å€™è£œè€…åé›†"""
        logger.info("ğŸš€ å…¬å¼ã‚½ãƒ¼ã‚¹å€™è£œè€…åé›†é–‹å§‹...")
        
        all_candidates = []
        
        # ç·å‹™çœãƒ‡ãƒ¼ã‚¿åé›†
        try:
            soumu_candidates = self.collect_soumu_data()
            all_candidates.extend(soumu_candidates)
        except Exception as e:
            logger.error(f"âŒ ç·å‹™çœåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ”¿å…šã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿åé›†
        try:
            party_candidates = self.collect_party_candidates()
            all_candidates.extend(party_candidates)
        except Exception as e:
            logger.error(f"âŒ æ”¿å…šã‚µã‚¤ãƒˆåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # é‡è¤‡é™¤å»
        unique_candidates = self.deduplicate_candidates(all_candidates)
        
        logger.info(f"ğŸ¯ å…¬å¼ã‚½ãƒ¼ã‚¹åé›†å®Œäº†: ç·è¨ˆ {len(unique_candidates)}å")
        return unique_candidates

    def deduplicate_candidates(self, candidates):
        """å€™è£œè€…é‡è¤‡é™¤å»"""
        seen = set()
        unique_candidates = []
        
        for candidate in candidates:
            # åå‰ã§é‡è¤‡åˆ¤å®š
            key = candidate['name']
            if key not in seen and len(key) > 1:
                seen.add(key)
                unique_candidates.append(candidate)
        
        logger.info(f"é‡è¤‡é™¤å»: {len(candidates)}å â†’ {len(unique_candidates)}å")
        return unique_candidates

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("ğŸš€ å…¬å¼ã‚½ãƒ¼ã‚¹å€™è£œè€…åé›†é–‹å§‹...")
    
    collector = OfficialSourcesCollector()
    candidates = collector.collect_all_official_sources()
    
    # çµæœä¿å­˜
    save_official_results(candidates)

def save_official_results(candidates):
    """å…¬å¼ã‚½ãƒ¼ã‚¹åé›†çµæœã®ä¿å­˜"""
    logger.info("ğŸ’¾ å…¬å¼ã‚½ãƒ¼ã‚¹åé›†çµæœã®ä¿å­˜...")
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    source_stats = {}
    
    for candidate in candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        source = candidate.get('source', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        source_stats[source] = source_stats.get(source, 0) + 1
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    data = {
        "metadata": {
            "data_type": "official_sources_sangiin_2025",
            "collection_method": "soumu_party_official_sources",
            "total_candidates": len(candidates),
            "generated_at": datetime.now().isoformat(),
            "sources": ["ç·å‹™çœé¸æŒ™éƒ¨", "å„æ”¿å…šå…¬å¼ã‚µã‚¤ãƒˆ"],
            "coverage": {
                "parties": len(party_stats),
                "sources": len(source_stats)
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_source": source_stats,
            "by_constituency_type": {"single_member": len(candidates)}
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    official_file = data_dir / f"official_sources_{timestamp}.json"
    
    with open(official_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ å…¬å¼ã‚½ãƒ¼ã‚¹çµæœä¿å­˜: {official_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    logger.info("\nğŸ“Š å…¬å¼ã‚½ãƒ¼ã‚¹åé›†çµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(candidates)}å")
    logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
    logger.info(f"  ã‚½ãƒ¼ã‚¹æ•°: {len(source_stats)}ã‚½ãƒ¼ã‚¹")
    
    for source, count in source_stats.items():
        logger.info(f"  {source}: {count}å")

if __name__ == "__main__":
    main()