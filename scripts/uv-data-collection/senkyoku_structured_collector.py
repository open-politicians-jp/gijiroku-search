#!/usr/bin/env python3
"""
é¸æŒ™åŒºåˆ¥æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿åé›†
Go2senkyo.comã®æ­£ç¢ºãªHTMLæ§‹é€ ã‚’ä½¿ç”¨ã—ãŸé…å»¶ãƒ­ãƒ¼ãƒ‰å¯¾å¿œç‰ˆ
"""

import json
import logging
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SenkyokuStructuredCollector:
    def __init__(self):
        self.session = requests.Session()
        ua = UserAgent()
        desktop_ua = ua.random
        
        self.session.headers.update({
            'User-Agent': desktop_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        # éƒ½é“åºœçœŒãƒãƒƒãƒ”ãƒ³ã‚°
        self.prefectures = {
            1: "åŒ—æµ·é“", 2: "é’æ£®çœŒ", 3: "å²©æ‰‹çœŒ", 4: "å®®åŸçœŒ", 5: "ç§‹ç”°çœŒ", 6: "å±±å½¢çœŒ", 7: "ç¦å³¶çœŒ",
            8: "èŒ¨åŸçœŒ", 9: "æ ƒæœ¨çœŒ", 10: "ç¾¤é¦¬çœŒ", 11: "åŸ¼ç‰çœŒ", 12: "åƒè‘‰çœŒ", 13: "æ±äº¬éƒ½", 14: "ç¥å¥ˆå·çœŒ",
            15: "æ–°æ½ŸçœŒ", 16: "å¯Œå±±çœŒ", 17: "çŸ³å·çœŒ", 18: "ç¦äº•çœŒ", 19: "å±±æ¢¨çœŒ", 20: "é•·é‡çœŒ", 21: "å²é˜œçœŒ",
            22: "é™å²¡çœŒ", 23: "æ„›çŸ¥çœŒ", 24: "ä¸‰é‡çœŒ", 25: "æ»‹è³€çœŒ", 26: "äº¬éƒ½åºœ", 27: "å¤§é˜ªåºœ", 28: "å…µåº«çœŒ",
            29: "å¥ˆè‰¯çœŒ", 30: "å’Œæ­Œå±±çœŒ", 31: "é³¥å–çœŒ", 32: "å³¶æ ¹çœŒ", 33: "å²¡å±±çœŒ", 34: "åºƒå³¶çœŒ", 35: "å±±å£çœŒ",
            36: "å¾³å³¶çœŒ", 37: "é¦™å·çœŒ", 38: "æ„›åª›çœŒ", 39: "é«˜çŸ¥çœŒ", 40: "ç¦å²¡çœŒ", 41: "ä½è³€çœŒ", 42: "é•·å´çœŒ",
            43: "ç†Šæœ¬çœŒ", 44: "å¤§åˆ†çœŒ", 45: "å®®å´çœŒ", 46: "é¹¿å…å³¶çœŒ", 47: "æ²–ç¸„çœŒ"
        }
        
        # æ”¿å…šãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
        self.parties = [
            "è‡ªç”±æ°‘ä¸»å…š", "ç«‹æ†²æ°‘ä¸»å…š", "æ—¥æœ¬ç¶­æ–°ã®ä¼š", "å…¬æ˜å…š", "æ—¥æœ¬å…±ç”£å…š",
            "å›½æ°‘æ°‘ä¸»å…š", "ã‚Œã„ã‚æ–°é¸çµ„", "å‚æ”¿å…š", "ç¤¾ä¼šæ°‘ä¸»å…š", "NHKå…š",
            "æ—¥æœ¬ä¿å®ˆå…š", "æ—¥æœ¬æ”¹é©å…š", "ç„¡æ‰€å±", "ç„¡æ‰€å±é€£åˆ", "æ—¥æœ¬èª çœŸä¼š",
            "æ—¥æœ¬ã®å®¶åº­ã‚’å®ˆã‚‹ä¼š", "å†ç”Ÿã®é“", "å·®åˆ¥æ’²æ»…å…š", "æ ¸èåˆå…š", "ãƒãƒ¼ãƒ ã¿ã‚‰ã„",
            "å¤šå¤«å¤šå¦»å…š", "å›½æ”¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ä¼š", "æ–°å…šã‚„ã¾ã¨", "æœªåˆ†é¡"
        ]

    def collect_prefecture_structured(self, pref_code: int):
        """æ§‹é€ åŒ–ã•ã‚ŒãŸéƒ½é“åºœçœŒãƒ‡ãƒ¼ã‚¿åé›†"""
        pref_name = self.prefectures.get(pref_code, f"æœªçŸ¥_{pref_code}")
        url = f"https://sangiin.go2senkyo.com/2025/prefecture/{pref_code}"
        
        logger.info(f"ğŸ” {pref_name} (ã‚³ãƒ¼ãƒ‰: {pref_code}) æ§‹é€ åŒ–åé›†é–‹å§‹")
        
        try:
            # åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                logger.error(f"âŒ {pref_name} ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            logger.info(f"ğŸ“Š {pref_name} åˆå›HTMLå–å¾—: {len(response.text):,} æ–‡å­—")
            
            # é…å»¶ãƒ­ãƒ¼ãƒ‰å¯¾å¿œ - æ®µéšçš„ãªå¾…æ©Ÿ
            logger.info(f"â³ {pref_name} é…å»¶ãƒ­ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¾…æ©Ÿ...")
            time.sleep(4)  # é•·ã‚ã®å¾…æ©Ÿæ™‚é–“
            
            # å†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒ•ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—
            response2 = self.session.get(url, timeout=30)
            logger.info(f"ğŸ“Š {pref_name} å†å–å¾—HTML: {len(response2.text):,} æ–‡å­—")
            
            soup = BeautifulSoup(response2.text, 'html.parser')
            
            # æ§‹é€ åŒ–æŠ½å‡ºã®å®Ÿè¡Œ
            candidates = self.extract_candidates_structured(soup, pref_name, url)
            
            logger.info(f"ğŸ¯ {pref_name} æ§‹é€ åŒ–æŠ½å‡ºå®Œäº†: {len(candidates)}å")
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def extract_candidates_structured(self, soup: BeautifulSoup, prefecture: str, page_url: str):
        """æ§‹é€ åŒ–ã•ã‚ŒãŸå€™è£œè€…æŠ½å‡º"""
        candidates = []
        
        try:
            # å…·ä½“çš„ãªHTMLæ§‹é€ ã«åŸºã¥ãæŠ½å‡º
            # <div class="p_senkyoku_list_block"> å†…ã®å€™è£œè€…ãƒ‡ãƒ¼ã‚¿
            candidate_blocks = soup.find_all('div', class_='p_senkyoku_list_block')
            logger.info(f"ğŸ“Š {prefecture} å€™è£œè€…ãƒ–ãƒ­ãƒƒã‚¯ç™ºè¦‹: {len(candidate_blocks)}å€‹")
            
            for i, block in enumerate(candidate_blocks):
                try:
                    candidate = self.extract_candidate_from_structured_block(block, prefecture, page_url, i)
                    if candidate:
                        candidates.append(candidate)
                        logger.info(f"  âœ… {candidate['name']} ({candidate['name_kana']}) - {candidate['party']}")
                    
                except Exception as e:
                    logger.debug(f"ãƒ–ãƒ­ãƒƒã‚¯ {i} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"{prefecture} æ§‹é€ åŒ–æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def extract_candidate_from_structured_block(self, block, prefecture: str, page_url: str, index: int):
        """æ§‹é€ åŒ–ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": "",
                "name": "",
                "name_kana": "",
                "prefecture": prefecture,
                "constituency": prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', ''),
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "profile_url": "",
                "source_page": page_url,
                "source": "structured_extraction",
                "collected_at": datetime.now().isoformat()
            }
            
            # 1. åå‰ã¨èª­ã¿ã®æŠ½å‡º
            name_info = self.extract_name_and_kana_structured(block)
            if name_info:
                candidate["name"] = name_info["name"]
                candidate["name_kana"] = name_info["kana"]
            
            # 2. æ”¿å…šã®æŠ½å‡º
            party = self.extract_party_from_block(block)
            if party:
                candidate["party"] = party
                candidate["party_normalized"] = party
            
            # 3. ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLã®æŠ½å‡º
            profile_url = self.extract_profile_url_from_block(block)
            if profile_url:
                candidate["profile_url"] = profile_url
                # candidate_idã‚’URLã‹ã‚‰æŠ½å‡º
                match = re.search(r'/seijika/(\d+)', profile_url)
                if match:
                    candidate["candidate_id"] = f"go2s_{match.group(1)}"
            
            # åå‰ãŒå–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not candidate["name"]:
                logger.debug(f"åå‰å–å¾—å¤±æ•—: ãƒ–ãƒ­ãƒƒã‚¯ {index}")
                return None
            
            return candidate
            
        except Exception as e:
            logger.debug(f"æ§‹é€ åŒ–ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def extract_name_and_kana_structured(self, block):
        """æ§‹é€ åŒ–ã•ã‚ŒãŸåå‰ãƒ»èª­ã¿æŠ½å‡º"""
        try:
            # Go2senkyo.comç‰¹æœ‰ã®æ§‹é€ :
            # <div class="p_senkyoku_list_block_text_name bold ">
            #   <p class="text"><span>ç‰§</span><span>å±±</span>...</p>
            #   <p class="kana "><span>ãƒ</span><span>ã‚­</span>...</p>
            
            name_container = block.find('div', class_='p_senkyoku_list_block_text_name')
            if not name_container:
                return None
            
            name = ""
            kana = ""
            
            # åå‰ã®æŠ½å‡º (class="text")
            text_elem = name_container.find('p', class_='text')
            if text_elem:
                spans = text_elem.find_all('span')
                name = ''.join(span.get_text() for span in spans).strip()
            
            # èª­ã¿ã®æŠ½å‡º (class="kana")
            kana_elem = name_container.find('p', class_='kana')
            if kana_elem:
                spans = kana_elem.find_all('span')
                kana = ''.join(span.get_text() for span in spans).strip()
            
            if name:
                logger.debug(f"åå‰ãƒ»èª­ã¿æŠ½å‡ºæˆåŠŸ: {name} ({kana})")
                return {"name": name, "kana": kana}
            
        except Exception as e:
            logger.debug(f"æ§‹é€ åŒ–åå‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return None

    def extract_party_from_block(self, block):
        """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰æ”¿å…šæŠ½å‡º"""
        try:
            # <div class="p_senkyoku_list_block_text_party">æ”¿å…šå</div>
            party_elem = block.find('div', class_='p_senkyoku_list_block_text_party')
            if party_elem:
                party_text = party_elem.get_text(strip=True)
                
                # æ”¿å…šãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆã¨ãƒãƒƒãƒãƒ³ã‚°
                for party in self.parties:
                    if party in party_text:
                        return party
                
                # ãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆã«ãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
                if party_text:
                    return party_text
            
        except Exception as e:
            logger.debug(f"æ”¿å…šæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return "æœªåˆ†é¡"

    def extract_profile_url_from_block(self, block):
        """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLæŠ½å‡º"""
        try:
            # <a href="https://go2senkyo.com/seijika/68099" target="_blank">
            # <p class="p_senkyoku_list_block_link_text">è©³ç´°ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«</p>
            
            links = block.find_all('a', href=True)
            for link in links:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                if '/seijika/' in href and 'è©³ç´°ãƒ»ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«' in link_text:
                    return href
            
        except Exception as e:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return ""

def collect_problem_prefectures():
    """å•é¡ŒçœŒã®æ§‹é€ åŒ–åé›†"""
    logger.info("ğŸš€ å•é¡ŒçœŒæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
    
    collector = SenkyokuStructuredCollector()
    
    # å•é¡ŒçœŒã¨ãƒ†ã‚¹ãƒˆçœŒ
    target_prefectures = [
        (24, "ä¸‰é‡çœŒ"),  # 1åâ†’6åæœŸå¾…
        (26, "äº¬éƒ½åºœ"),  # 6åâ†’9åæœŸå¾…
        (14, "ç¥å¥ˆå·çœŒ"), # ãƒ†ã‚¹ãƒˆï¼ˆ15åæœŸå¾…ï¼‰
        (47, "æ²–ç¸„çœŒ"),   # ãƒ†ã‚¹ãƒˆï¼ˆ5åæœŸå¾…ï¼‰
    ]
    
    all_results = []
    failed_prefectures = []
    
    for i, (pref_code, pref_name) in enumerate(target_prefectures):
        logger.info(f"\n=== [{i+1}/4] {pref_name} æ§‹é€ åŒ–åé›† ===")
        
        try:
            candidates = collector.collect_prefecture_structured(pref_code)
            all_results.extend(candidates)
            
            logger.info(f"âœ… {pref_name} å®Œäº†: {len(candidates)}å")
            
            # å…¨çœŒã§è©³ç´°è¡¨ç¤º
            logger.info(f"ğŸ“‹ {pref_name} å€™è£œè€…è©³ç´°:")
            for candidate in candidates:
                logger.info(f"  - {candidate['name']} ({candidate['name_kana']}) - {candidate['party']} - ID: {candidate['candidate_id']}")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆæœ€å¾Œã®çœŒä»¥å¤–ï¼‰
            if i < len(target_prefectures) - 1:
                time.sleep(1)
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            failed_prefectures.append((pref_code, pref_name))
            continue
    
    logger.info(f"\nğŸ¯ å•é¡ŒçœŒæ§‹é€ åŒ–åé›†å®Œäº†: ç·è¨ˆ {len(all_results)}å")
    
    # å¤±æ•—çœŒã®å ±å‘Š
    if failed_prefectures:
        logger.warning(f"âš ï¸ åé›†å¤±æ•—: {len(failed_prefectures)}çœŒ")
        for pref_code, pref_name in failed_prefectures:
            logger.warning(f"  - {pref_name} (ã‚³ãƒ¼ãƒ‰: {pref_code})")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    check_duplicates_structured(all_results)
    
    # çµæœä¿å­˜
    save_problem_prefecture_results(all_results)

def check_duplicates_structured(candidates):
    """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
    logger.info("\nğŸ” æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿é‡è¤‡ãƒã‚§ãƒƒã‚¯:")
    
    # candidate_idãƒ™ãƒ¼ã‚¹ã®é‡è¤‡
    id_counts = {}
    for candidate in candidates:
        candidate_id = candidate.get('candidate_id', '')
        if candidate_id:
            id_counts[candidate_id] = id_counts.get(candidate_id, 0) + 1
    
    id_duplicates = {k: v for k, v in id_counts.items() if v > 1}
    if id_duplicates:
        logger.warning("âš ï¸ IDé‡è¤‡:")
        for cid, count in id_duplicates.items():
            logger.warning(f"  {cid}: {count}å›")
    else:
        logger.info("âœ… IDé‡è¤‡ãªã—")
    
    # åå‰+éƒ½é“åºœçœŒãƒ™ãƒ¼ã‚¹ã®é‡è¤‡
    name_counts = {}
    for candidate in candidates:
        name = candidate.get('name', '')
        prefecture = candidate.get('prefecture', '')
        key = f"{name}_{prefecture}"
        name_counts[key] = name_counts.get(key, 0) + 1
    
    name_duplicates = {k: v for k, v in name_counts.items() if v > 1}
    if name_duplicates:
        logger.warning("âš ï¸ åå‰é‡è¤‡:")
        for name_pref, count in name_duplicates.items():
            logger.warning(f"  {name_pref}: {count}å›")
    else:
        logger.info("âœ… åå‰é‡è¤‡ãªã—")

def save_problem_prefecture_results(candidates):
    """å•é¡ŒçœŒæ§‹é€ åŒ–åé›†çµæœã®ä¿å­˜"""
    logger.info("ğŸ’¾ å•é¡ŒçœŒæ§‹é€ åŒ–åé›†çµæœã®ä¿å­˜...")
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    with_kana_count = 0
    
    for candidate in candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
        
        if candidate.get('name_kana'):
            with_kana_count += 1
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    data = {
        "metadata": {
            "data_type": "go2senkyo_problem_prefectures_structured_sangiin_2025",
            "collection_method": "structured_html_extraction_problem_prefectures",
            "total_candidates": len(candidates),
            "candidates_with_kana": with_kana_count,
            "generated_at": datetime.now().isoformat(),
            "source_site": "sangiin.go2senkyo.com",
            "coverage": {
                "constituency_types": 1,
                "parties": len(party_stats),
                "prefectures": len(prefecture_stats)
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": prefecture_stats,
            "by_constituency_type": {"single_member": len(candidates)}
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    problem_file = data_dir / f"go2senkyo_problem_prefectures_{timestamp}.json"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(problem_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ å•é¡ŒçœŒæ§‹é€ åŒ–åé›†çµæœä¿å­˜: {problem_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    logger.info("\nğŸ“Š å•é¡ŒçœŒæ§‹é€ åŒ–åé›†çµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(candidates)}å")
    logger.info(f"  èª­ã¿ä»˜ã: {with_kana_count}å ({with_kana_count/len(candidates)*100:.1f}%)")
    logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
    logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_stats)}éƒ½é“åºœçœŒ")
    
    # ä¸‰é‡çœŒãƒ»äº¬éƒ½åºœã®ç¢ºèª
    mie_count = prefecture_stats.get("ä¸‰é‡çœŒ", 0)
    kyoto_count = prefecture_stats.get("äº¬éƒ½åºœ", 0)
    kanagawa_count = prefecture_stats.get("ç¥å¥ˆå·çœŒ", 0)
    okinawa_count = prefecture_stats.get("æ²–ç¸„çœŒ", 0)
    
    logger.info(f"\nğŸ” å•é¡ŒçœŒç¢ºèª:")
    logger.info(f"  ä¸‰é‡çœŒ: {mie_count}å (æœŸå¾…: 6å) {'âœ…' if mie_count >= 6 else 'âŒ'}")
    logger.info(f"  äº¬éƒ½åºœ: {kyoto_count}å (æœŸå¾…: 9å) {'âœ…' if kyoto_count >= 9 else 'âŒ'}")
    logger.info(f"  ç¥å¥ˆå·çœŒ: {kanagawa_count}å (ãƒ†ã‚¹ãƒˆ: 15åæœŸå¾…) {'âœ…' if kanagawa_count == 15 else 'âŒ'}")
    logger.info(f"  æ²–ç¸„çœŒ: {okinawa_count}å (ãƒ†ã‚¹ãƒˆ: 5åæœŸå¾…) {'âœ…' if okinawa_count == 5 else 'âŒ'}")
    
    return problem_file

if __name__ == "__main__":
    collect_problem_prefectures()