#!/usr/bin/env python3
"""
HTMLæ§‹é€ ãƒ‡ãƒãƒƒã‚° - NHKãƒ»æœæ—¥æ–°èã®ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’åˆ†æ
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from pathlib import Path

def analyze_nhk_structure():
    """NHKãƒšãƒ¼ã‚¸ã®æ§‹é€ åˆ†æ"""
    print("ğŸ” NHKãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æé–‹å§‹...")
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,ja-JP;q=0.9,en;q=0.8',
        'Referer': 'https://www.google.co.jp/',
    })
    
    try:
        url = "https://www.nhk.or.jp/senkyo/database/sangiin/2025/expected-candidates/"
        response = session.get(url, timeout=30)
        
        print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
        print(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·: {len(response.text):,} æ–‡å­—")
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        debug_dir = Path(__file__).parent / "debug"
        debug_dir.mkdir(exist_ok=True)
        
        with open(debug_dir / "nhk_page.html", 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # éƒ½é“åºœçœŒé–¢é€£ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
        print("\nğŸ“‹ éƒ½é“åºœçœŒé–¢é€£è¦ç´ :")
        prefecture_keywords = ['éƒ½é“åºœçœŒ', 'é¸æŒ™åŒº', 'åœ°åŸŸ', 'å€™è£œè€…', 'åŒ—æµ·é“', 'æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ', 'äº¬éƒ½åºœ']
        
        for keyword in prefecture_keywords:
            elements = soup.find_all(string=re.compile(keyword))
            print(f"  {keyword}: {len(elements)}ä»¶")
            if elements:
                for elem in elements[:3]:  # æœ€åˆã®3ä»¶
                    parent = elem.parent if elem.parent else None
                    print(f"    - {elem.strip()[:50]}... (è¦ª: {parent.name if parent else 'None'})")
        
        # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’ç¢ºèª
        print("\nğŸ”— å…¨ãƒªãƒ³ã‚¯åˆ†æ:")
        links = soup.find_all('a', href=True)
        print(f"  ç·ãƒªãƒ³ã‚¯æ•°: {len(links)}")
        
        prefecture_links = []
        for link in links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # éƒ½é“åºœçœŒé–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            if any(keyword in text for keyword in ['çœŒ', 'éƒ½', 'åºœ', 'é“', 'é¸æŒ™åŒº']):
                prefecture_links.append((text, href))
        
        print(f"  éƒ½é“åºœçœŒé–¢é€£ãƒªãƒ³ã‚¯: {len(prefecture_links)}ä»¶")
        for text, href in prefecture_links[:10]:  # æœ€åˆã®10ä»¶
            print(f"    - {text} â†’ {href}")
        
        # JavaScriptã§å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        scripts = soup.find_all('script')
        print(f"\nğŸ“œ JavaScript: {len(scripts)}ä»¶")
        
        for script in scripts:
            script_content = script.string if script.string else ""
            if any(keyword in script_content for keyword in ['prefecture', 'éƒ½é“åºœçœŒ', 'candidate']):
                print(f"  é–¢é€£ã‚¹ã‚¯ãƒªãƒ—ãƒˆç™ºè¦‹: {script_content[:100]}...")
        
        return response.text
        
    except Exception as e:
        print(f"âŒ NHKåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return None

def analyze_asahi_structure():
    """æœæ—¥æ–°èãƒšãƒ¼ã‚¸ã®æ§‹é€ åˆ†æ"""
    print("\nğŸ” æœæ—¥æ–°èãƒšãƒ¼ã‚¸æ§‹é€ åˆ†æé–‹å§‹...")
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ja,ja-JP;q=0.9,en;q=0.8',
        'Referer': 'https://www.google.co.jp/',
    })
    
    try:
        url = "https://www.asahi.com/senkyo/saninsen/koho/"
        response = session.get(url, timeout=30)
        
        print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
        print(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·: {len(response.text):,} æ–‡å­—")
        
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        debug_dir = Path(__file__).parent / "debug"
        debug_dir.mkdir(exist_ok=True)
        
        with open(debug_dir / "asahi_page.html", 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # éƒ½é“åºœçœŒé–¢é€£ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è¦ç´ ã‚’æ¢ã™
        print("\nğŸ“‹ éƒ½é“åºœçœŒé–¢é€£è¦ç´ :")
        prefecture_keywords = ['éƒ½é“åºœçœŒ', 'é¸æŒ™åŒº', 'åœ°åŸŸ', 'å€™è£œè€…', 'åŒ—æµ·é“', 'æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ', 'äº¬éƒ½åºœ']
        
        for keyword in prefecture_keywords:
            elements = soup.find_all(string=re.compile(keyword))
            print(f"  {keyword}: {len(elements)}ä»¶")
            if elements:
                for elem in elements[:3]:  # æœ€åˆã®3ä»¶
                    parent = elem.parent if elem.parent else None
                    print(f"    - {elem.strip()[:50]}... (è¦ª: {parent.name if parent else 'None'})")
        
        # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’ç¢ºèª
        print("\nğŸ”— å…¨ãƒªãƒ³ã‚¯åˆ†æ:")
        links = soup.find_all('a', href=True)
        print(f"  ç·ãƒªãƒ³ã‚¯æ•°: {len(links)}")
        
        prefecture_links = []
        for link in links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # éƒ½é“åºœçœŒé–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            if any(keyword in text for keyword in ['çœŒ', 'éƒ½', 'åºœ', 'é“', 'é¸æŒ™åŒº']):
                prefecture_links.append((text, href))
        
        print(f"  éƒ½é“åºœçœŒé–¢é€£ãƒªãƒ³ã‚¯: {len(prefecture_links)}ä»¶")
        for text, href in prefecture_links[:10]:  # æœ€åˆã®10ä»¶
            print(f"    - {text} â†’ {href}")
        
        # selectè¦ç´ ã‚„optionè¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
        selects = soup.find_all('select')
        print(f"\nğŸ“‹ ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹: {len(selects)}ä»¶")
        
        for select in selects:
            options = select.find_all('option')
            print(f"  ã‚»ãƒ¬ã‚¯ãƒˆ: {len(options)}å€‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
            for option in options[:5]:  # æœ€åˆã®5ä»¶
                print(f"    - {option.get_text(strip=True)} (value: {option.get('value', '')})")
        
        return response.text
        
    except Exception as e:
        print(f"âŒ æœæ—¥æ–°èåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return None

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ” HTMLæ§‹é€ ãƒ‡ãƒãƒƒã‚°é–‹å§‹...")
    
    # NHKåˆ†æ
    nhk_content = analyze_nhk_structure()
    
    # æœæ—¥æ–°èåˆ†æ
    asahi_content = analyze_asahi_structure()
    
    print("\nâœ… HTMLæ§‹é€ åˆ†æå®Œäº†")
    print("ğŸ“ ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¯ debug/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main()