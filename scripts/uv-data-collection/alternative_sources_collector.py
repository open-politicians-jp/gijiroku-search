#!/usr/bin/env python3
"""
ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®å€™è£œè€…åé›†
- é¸æŒ™ç®¡ç†å§”å“¡ä¼šãƒ‡ãƒ¼ã‚¿
- Yahoo!é¸æŒ™
- Googleé¸æŒ™æƒ…å ±
- æ”¿æ²»é–¢é€£ã‚µã‚¤ãƒˆ
"""

import json
import logging
import requests
import time
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import random

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AlternativeSourcesCollector:
    def __init__(self):
        self.session = requests.Session()
        
        # æ—¥æœ¬ã®User-Agentãƒªã‚¹ãƒˆ
        self.japanese_user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0",
        ]
        
        self.setup_session()

    def setup_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š"""
        user_agent = random.choice(self.japanese_user_agents)
        
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,ja-JP;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://www.google.co.jp/',
        })

    def collect_yahoo_election_data(self):
        """Yahoo!é¸æŒ™ã‹ã‚‰å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("ğŸ” Yahoo!é¸æŒ™ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        
        # Yahoo!é¸æŒ™ã®å‚è­°é™¢é¸2025ãƒšãƒ¼ã‚¸
        base_urls = [
            "https://seiji.yahoo.co.jp/election/",
            "https://seiji.yahoo.co.jp/sangiin/2025/",
            "https://seiji.yahoo.co.jp/election/sangiin/2025/",
        ]
        
        all_candidates = []
        
        for url in base_urls:
            try:
                logger.info(f"ğŸ“Š Yahoo!é¸æŒ™ã‚¢ã‚¯ã‚»ã‚¹: {url}")
                response = self.session.get(url, timeout=30)
                
                if response.status_code == 200:
                    logger.info(f"âœ… Yahoo!ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(response.text):,} æ–‡å­—")
                    
                    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    debug_dir = Path(__file__).parent / "debug"
                    debug_dir.mkdir(exist_ok=True)
                    
                    filename = url.replace('https://', '').replace('/', '_') + '.html'
                    with open(debug_dir / filename, 'w', encoding='utf-8') as f:
                        f.write(response.text)
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    candidates = self.extract_yahoo_candidates(soup, url)
                    all_candidates.extend(candidates)
                    
                    if candidates:
                        logger.info(f"ğŸ“‹ Yahoo!å€™è£œè€…ç™ºè¦‹: {len(candidates)}å")
                        break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                else:
                    logger.warning(f"âš ï¸ Yahoo!ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"âŒ Yahoo!åé›†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                continue
        
        logger.info(f"ğŸ¯ Yahoo!åé›†å®Œäº†: ç·è¨ˆ {len(all_candidates)}å")
        return all_candidates

    def extract_yahoo_candidates(self, soup, source_url):
        """Yahoo!é¸æŒ™ã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        candidates = []
        
        try:
            # Yahoo!é¸æŒ™ã®å…¸å‹çš„ãªæ§‹é€ ã‚’æ¢ç´¢
            candidate_selectors = [
                '[class*="candidate"]',
                '[class*="koho"]',
                '[class*="person"]',
                '[class*="politician"]',
                'div[data-candidate]',
                'li[data-candidate]'
            ]
            
            for selector in candidate_selectors:
                elements = soup.select(selector)
                logger.info(f"Yahoo!ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}': {len(elements)}ä»¶")
                
                for element in elements:
                    try:
                        candidate = self.extract_candidate_from_element(element, source_url, "yahoo_election")
                        if candidate:
                            candidates.append(candidate)
                    except Exception as e:
                        logger.debug(f"Yahoo!å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            # éƒ½é“åºœçœŒåˆ¥ãƒªãƒ³ã‚¯ã‚‚æ¢ç´¢
            prefecture_links = self.find_prefecture_links(soup, source_url)
            logger.info(f"Yahoo!éƒ½é“åºœçœŒãƒªãƒ³ã‚¯: {len(prefecture_links)}ä»¶")
            
            # å„éƒ½é“åºœçœŒãƒšãƒ¼ã‚¸ã‚’åé›†ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
            for pref_name, pref_url in prefecture_links[:5]:  # æœ€åˆã®5çœŒã®ã¿
                try:
                    pref_candidates = self.collect_prefecture_page(pref_name, pref_url, "yahoo")
                    candidates.extend(pref_candidates)
                    time.sleep(2)
                except Exception as e:
                    logger.debug(f"Yahoo!éƒ½é“åºœçœŒåé›†ã‚¨ãƒ©ãƒ¼ ({pref_name}): {e}")
                    continue
            
        except Exception as e:
            logger.error(f"Yahoo!å€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def collect_google_election_data(self):
        """Googleé¸æŒ™æƒ…å ±ã‹ã‚‰å€™è£œè€…ãƒ‡ãƒ¼ã‚¿åé›†"""
        logger.info("ğŸ” Googleé¸æŒ™æƒ…å ±ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
        
        # Googleé¸æŒ™æƒ…å ±ã®URL
        base_urls = [
            "https://g.co/elections",
            "https://www.google.com/search?q=å‚è­°é™¢é¸æŒ™+2025+å€™è£œè€…",
            "https://www.google.co.jp/search?q=å‚è­°é™¢é¸æŒ™+2025+å€™è£œè€…ä¸€è¦§",
        ]
        
        all_candidates = []
        
        for url in base_urls:
            try:
                logger.info(f"ğŸ“Š Googleé¸æŒ™æƒ…å ±ã‚¢ã‚¯ã‚»ã‚¹: {url}")
                response = self.session.get(url, timeout=30)
                
                if response.status_code == 200:
                    logger.info(f"âœ… Googleãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {len(response.text):,} æ–‡å­—")
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    candidates = self.extract_google_candidates(soup, url)
                    all_candidates.extend(candidates)
                    
                    if candidates:
                        logger.info(f"ğŸ“‹ Googleå€™è£œè€…ç™ºè¦‹: {len(candidates)}å")
                
                time.sleep(3)
                
            except Exception as e:
                logger.error(f"âŒ Googleåé›†ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
                continue
        
        logger.info(f"ğŸ¯ Googleåé›†å®Œäº†: ç·è¨ˆ {len(all_candidates)}å")
        return all_candidates

    def extract_google_candidates(self, soup, source_url):
        """Googleé¸æŒ™æƒ…å ±ã‹ã‚‰å€™è£œè€…æƒ…å ±æŠ½å‡º"""
        candidates = []
        
        try:
            # Googleæ¤œç´¢çµæœã®æ§‹é€ ã‚’æ¢ç´¢
            result_selectors = [
                '[data-ved]',
                '.g',
                '.yuRUbf',
                '[class*="result"]'
            ]
            
            for selector in result_selectors:
                elements = soup.select(selector)
                logger.info(f"Googleã‚»ãƒ¬ã‚¯ã‚¿ '{selector}': {len(elements)}ä»¶")
                
                for element in elements:
                    try:
                        # Googleæ¤œç´¢çµæœã‹ã‚‰å€™è£œè€…é–¢é€£æƒ…å ±ã‚’æŠ½å‡º
                        text = element.get_text()
                        if any(keyword in text for keyword in ['å€™è£œè€…', 'å‚è­°é™¢', 'é¸æŒ™', 'ç«‹å€™è£œ']):
                            candidate_info = self.parse_google_result_text(text, source_url)
                            if candidate_info:
                                candidates.extend(candidate_info)
                    except Exception as e:
                        logger.debug(f"Googleçµæœè§£æã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
        except Exception as e:
            logger.error(f"Googleå€™è£œè€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def parse_google_result_text(self, text, source_url):
        """Googleæ¤œç´¢çµæœãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º"""
        candidates = []
        
        try:
            # å€™è£œè€…åã‚‰ã—ããƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
            # ä¾‹: "ç”°ä¸­å¤ªéƒï¼ˆãŸãªã‹ãŸã‚ã†ï¼‰è‡ªç”±æ°‘ä¸»å…š"
            name_patterns = [
                r'([ä¸€-é¾¯ã²ã‚‰ãŒãª\s]+)ï¼ˆ([ã‚¡-ãƒ¶ãƒ¼\s]+)ï¼‰([ä¸€-é¾¯ã²ã‚‰ãŒãª]+å…š|ç„¡æ‰€å±)',
                r'([ä¸€-é¾¯ã²ã‚‰ãŒãª\s]+)\s+([ä¸€-é¾¯ã²ã‚‰ãŒãª]+å…š|ç„¡æ‰€å±)',
                r'([ä¸€-é¾¯ã²ã‚‰ãŒãª]{2,6})\s*([ä¸€-é¾¯ã²ã‚‰ãŒãª]+å…š)'
            ]
            
            for pattern in name_patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    candidate = {
                        "candidate_id": f"google_{hash(match.group(1)) % 1000000}",
                        "name": match.group(1).strip(),
                        "name_kana": match.group(2).strip() if len(match.groups()) > 2 else "",
                        "party": match.groups()[-1].strip(),
                        "party_normalized": match.groups()[-1].strip(),
                        "prefecture": "",  # Googleæ¤œç´¢çµæœã§ã¯ç‰¹å®šå›°é›£
                        "constituency": "",
                        "constituency_type": "single_member",
                        "source_page": source_url,
                        "source": "google_search",
                        "collected_at": datetime.now().isoformat()
                    }
                    
                    if len(candidate["name"]) > 1:  # æœ€ä½é™ã®æ¤œè¨¼
                        candidates.append(candidate)
            
        except Exception as e:
            logger.debug(f"Googleãƒ†ã‚­ã‚¹ãƒˆè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def find_prefecture_links(self, soup, base_url):
        """éƒ½é“åºœçœŒåˆ¥ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ç´¢"""
        links = []
        
        try:
            # éƒ½é“åºœçœŒåã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            all_links = soup.find_all('a', href=True)
            
            prefecture_names = [
                'åŒ—æµ·é“', 'é’æ£®çœŒ', 'å²©æ‰‹çœŒ', 'å®®åŸçœŒ', 'ç§‹ç”°çœŒ', 'å±±å½¢çœŒ', 'ç¦å³¶çœŒ',
                'èŒ¨åŸçœŒ', 'æ ƒæœ¨çœŒ', 'ç¾¤é¦¬çœŒ', 'åŸ¼ç‰çœŒ', 'åƒè‘‰çœŒ', 'æ±äº¬éƒ½', 'ç¥å¥ˆå·çœŒ',
                'æ–°æ½ŸçœŒ', 'å¯Œå±±çœŒ', 'çŸ³å·çœŒ', 'ç¦äº•çœŒ', 'å±±æ¢¨çœŒ', 'é•·é‡çœŒ', 'å²é˜œçœŒ',
                'é™å²¡çœŒ', 'æ„›çŸ¥çœŒ', 'ä¸‰é‡çœŒ', 'æ»‹è³€çœŒ', 'äº¬éƒ½åºœ', 'å¤§é˜ªåºœ', 'å…µåº«çœŒ',
                'å¥ˆè‰¯çœŒ', 'å’Œæ­Œå±±çœŒ', 'é³¥å–çœŒ', 'å³¶æ ¹çœŒ', 'å²¡å±±çœŒ', 'åºƒå³¶çœŒ', 'å±±å£çœŒ',
                'å¾³å³¶çœŒ', 'é¦™å·çœŒ', 'æ„›åª›çœŒ', 'é«˜çŸ¥çœŒ', 'ç¦å²¡çœŒ', 'ä½è³€çœŒ', 'é•·å´çœŒ',
                'ç†Šæœ¬çœŒ', 'å¤§åˆ†çœŒ', 'å®®å´çœŒ', 'é¹¿å…å³¶çœŒ', 'æ²–ç¸„çœŒ'
            ]
            
            for link in all_links:
                href = link.get('href')
                text = link.get_text(strip=True)
                
                for pref_name in prefecture_names:
                    if pref_name in text:
                        full_url = self.resolve_url(href, base_url)
                        links.append((pref_name, full_url))
                        break
            
            # é‡è¤‡é™¤å»
            unique_links = list(dict(links).items())
            return unique_links
            
        except Exception as e:
            logger.error(f"éƒ½é“åºœçœŒãƒªãƒ³ã‚¯æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def collect_prefecture_page(self, pref_name, pref_url, source_type):
        """éƒ½é“åºœçœŒåˆ¥ãƒšãƒ¼ã‚¸ã‹ã‚‰å€™è£œè€…åé›†"""
        candidates = []
        
        try:
            logger.info(f"ğŸ“ {pref_name} {source_type}ãƒšãƒ¼ã‚¸åé›†...")
            response = self.session.get(pref_url, timeout=30)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # å„ã‚½ãƒ¼ã‚¹åˆ¥ã®æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
                if source_type == "yahoo":
                    candidates = self.extract_yahoo_candidates(soup, pref_url)
                elif source_type == "google":
                    candidates = self.extract_google_candidates(soup, pref_url)
                
                # éƒ½é“åºœçœŒæƒ…å ±ã‚’è¿½åŠ 
                for candidate in candidates:
                    candidate["prefecture"] = pref_name
                    candidate["constituency"] = pref_name.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', '')
                
                logger.info(f"âœ… {pref_name} {source_type}åé›†å®Œäº†: {len(candidates)}å")
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} {source_type}åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        return candidates

    def extract_candidate_from_element(self, element, source_url, source_type):
        """HTMLè¦ç´ ã‹ã‚‰å€™è£œè€…æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            candidate = {
                "candidate_id": "",
                "name": "",
                "name_kana": "",
                "prefecture": "",
                "constituency": "",
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "age": "",
                "profile_url": "",
                "source_page": source_url,
                "source": source_type,
                "collected_at": datetime.now().isoformat()
            }
            
            # åå‰ã®æŠ½å‡º
            name_selectors = [
                '.name', '.candidate-name', '.person-name',
                'h3', 'h4', '[class*="name"]'
            ]
            
            for selector in name_selectors:
                name_elem = element.select_one(selector)
                if name_elem:
                    candidate["name"] = name_elem.get_text(strip=True)
                    break
            
            # èª­ã¿ã®æŠ½å‡º
            kana_selectors = [
                '.kana', '.reading', '.furigana',
                '[class*="kana"]', '[class*="reading"]'
            ]
            
            for selector in kana_selectors:
                kana_elem = element.select_one(selector)
                if kana_elem:
                    candidate["name_kana"] = kana_elem.get_text(strip=True)
                    break
            
            # æ”¿å…šã®æŠ½å‡º
            party_selectors = [
                '.party', '.political-party', '.affiliation',
                '[class*="party"]', '[class*="affiliation"]'
            ]
            
            for selector in party_selectors:
                party_elem = element.select_one(selector)
                if party_elem:
                    candidate["party"] = party_elem.get_text(strip=True)
                    candidate["party_normalized"] = candidate["party"]
                    break
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«URLã®æŠ½å‡º
            profile_link = element.find('a', href=True)
            if profile_link:
                candidate["profile_url"] = self.resolve_url(profile_link.get('href'), source_url)
            
            # å€™è£œè€…IDã®ç”Ÿæˆ
            if candidate["name"]:
                candidate["candidate_id"] = f"{source_type}_{hash(candidate['name']) % 1000000}"
                return candidate
            
        except Exception as e:
            logger.debug(f"å€™è£œè€…è¦ç´ æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return None

    def resolve_url(self, href, base_url):
        """ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›"""
        if href.startswith('http'):
            return href
        elif href.startswith('/'):
            base_domain = '/'.join(base_url.split('/')[:3])
            return base_domain + href
        else:
            return base_url.rstrip('/') + '/' + href

    def collect_all_alternative_sources(self):
        """ã™ã¹ã¦ã®ä»£æ›¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰å€™è£œè€…åé›†"""
        logger.info("ğŸš€ ä»£æ›¿ã‚½ãƒ¼ã‚¹å€™è£œè€…åé›†é–‹å§‹...")
        
        all_candidates = []
        
        # Yahoo!é¸æŒ™ãƒ‡ãƒ¼ã‚¿åé›†
        try:
            yahoo_candidates = self.collect_yahoo_election_data()
            all_candidates.extend(yahoo_candidates)
        except Exception as e:
            logger.error(f"âŒ Yahoo!åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # Googleé¸æŒ™æƒ…å ±åé›†
        try:
            google_candidates = self.collect_google_election_data()
            all_candidates.extend(google_candidates)
        except Exception as e:
            logger.error(f"âŒ Googleåé›†ã‚¨ãƒ©ãƒ¼: {e}")
        
        # é‡è¤‡é™¤å»
        unique_candidates = self.deduplicate_candidates(all_candidates)
        
        logger.info(f"ğŸ¯ ä»£æ›¿ã‚½ãƒ¼ã‚¹åé›†å®Œäº†: ç·è¨ˆ {len(unique_candidates)}å")
        return unique_candidates

    def deduplicate_candidates(self, candidates):
        """å€™è£œè€…é‡è¤‡é™¤å»"""
        seen = set()
        unique_candidates = []
        
        for candidate in candidates:
            # åå‰+æ”¿å…šã§é‡è¤‡åˆ¤å®š
            key = f"{candidate['name']}_{candidate['party']}"
            if key not in seen and len(candidate['name']) > 1:
                seen.add(key)
                unique_candidates.append(candidate)
        
        logger.info(f"é‡è¤‡é™¤å»: {len(candidates)}å â†’ {len(unique_candidates)}å")
        return unique_candidates

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("ğŸš€ ä»£æ›¿ã‚½ãƒ¼ã‚¹å€™è£œè€…åé›†é–‹å§‹...")
    
    collector = AlternativeSourcesCollector()
    candidates = collector.collect_all_alternative_sources()
    
    # çµæœä¿å­˜
    save_alternative_results(candidates)

def save_alternative_results(candidates):
    """ä»£æ›¿ã‚½ãƒ¼ã‚¹åé›†çµæœã®ä¿å­˜"""
    logger.info("ğŸ’¾ ä»£æ›¿ã‚½ãƒ¼ã‚¹åé›†çµæœã®ä¿å­˜...")
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    source_stats = {}
    
    for candidate in candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        source = candidate.get('source', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        source_stats[source] = source_stats.get(source, 0) + 1
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    data = {
        "metadata": {
            "data_type": "alternative_sources_sangiin_2025",
            "collection_method": "yahoo_google_alternative_sources",
            "total_candidates": len(candidates),
            "generated_at": datetime.now().isoformat(),
            "sources": ["Yahoo!é¸æŒ™", "Googleé¸æŒ™æƒ…å ±"],
            "coverage": {
                "parties": len(party_stats),
                "sources": len(source_stats)
            }
        },
        "statistics": {
            "by_party": party_stats,
            "by_source": source_stats,
            "by_constituency_type": {"single_member": len(candidates)}
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    alt_file = data_dir / f"alternative_sources_{timestamp}.json"
    
    with open(alt_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ä»£æ›¿ã‚½ãƒ¼ã‚¹çµæœä¿å­˜: {alt_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    logger.info("\nğŸ“Š ä»£æ›¿ã‚½ãƒ¼ã‚¹åé›†çµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(candidates)}å")
    logger.info(f"  æ”¿å…šæ•°: {len(party_stats)}æ”¿å…š")
    logger.info(f"  ã‚½ãƒ¼ã‚¹æ•°: {len(source_stats)}ã‚½ãƒ¼ã‚¹")
    
    for source, count in source_stats.items():
        logger.info(f"  {source}: {count}å")

if __name__ == "__main__":
    main()