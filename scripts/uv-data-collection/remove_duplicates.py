#!/usr/bin/env python3
"""
é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from collections import defaultdict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def remove_duplicates():
    """é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
    logger.info("ğŸ” é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯é–‹å§‹...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    candidates = data.get('data', [])
    logger.info(f"ğŸ“Š å…ƒãƒ‡ãƒ¼ã‚¿: {len(candidates)}å")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicates = find_duplicates(candidates)
    
    if duplicates:
        logger.info(f"ğŸš¨ é‡è¤‡ç™ºè¦‹: {len(duplicates)}ãƒ‘ã‚¿ãƒ¼ãƒ³")
        for key, entries in duplicates.items():
            logger.info(f"  {key}: {len(entries)}å€‹")
            for i, entry in enumerate(entries):
                logger.info(f"    {i+1}: {entry['prefecture']} - {entry['constituency']} (ID: {entry['candidate_id']})")
    
    # é‡è¤‡å‰Šé™¤
    unique_candidates = deduplicate_candidates(candidates)
    
    removed_count = len(candidates) - len(unique_candidates)
    logger.info(f"ğŸ¯ é‡è¤‡å‰Šé™¤å®Œäº†:")
    logger.info(f"  å‰Šé™¤æ•°: {removed_count}å")
    logger.info(f"  æ®‹å­˜æ•°: {len(unique_candidates)}å")
    
    # çµ±è¨ˆã‚’å†è¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    
    for candidate in unique_candidates:
        party = candidate.get('party', 'ç„¡æ‰€å±')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
    
    # ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    data['data'] = unique_candidates
    data['metadata']['total_candidates'] = len(unique_candidates)
    data['metadata']['generated_at'] = datetime.now().isoformat()
    data['metadata']['data_type'] = "go2senkyo_deduplicated_sangiin_2025"
    data['metadata']['collection_method'] = "constituency_deduplicated"
    
    data['statistics'] = {
        "by_party": party_stats,
        "by_prefecture": prefecture_stats,
        "by_constituency_type": {"single_member": len(unique_candidates)}
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    dedup_file = data_dir / f"go2senkyo_deduplicated_{timestamp}.json"
    
    with open(dedup_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ä¿å­˜å®Œäº†: {dedup_file}")
    
    # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º
    show_final_statistics(unique_candidates)

def find_duplicates(candidates):
    """é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œç´¢"""
    
    # candidate_idãƒ™ãƒ¼ã‚¹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    id_groups = defaultdict(list)
    # åå‰ãƒ™ãƒ¼ã‚¹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    name_groups = defaultdict(list)
    
    for candidate in candidates:
        candidate_id = candidate.get('candidate_id', '')
        name = candidate.get('name', '')
        
        if candidate_id:
            id_groups[candidate_id].append(candidate)
        
        if name:
            name_groups[name].append(candidate)
    
    duplicates = {}
    
    # candidate_idã§é‡è¤‡ã—ã¦ã„ã‚‹ã‚‚ã®
    for candidate_id, entries in id_groups.items():
        if len(entries) > 1:
            duplicates[f"IDé‡è¤‡: {candidate_id}"] = entries
    
    # åå‰ã§é‡è¤‡ã—ã¦ã„ã‚‹ã‚‚ã®ï¼ˆcandidate_idãŒç•°ãªã‚‹å ´åˆï¼‰
    for name, entries in name_groups.items():
        if len(entries) > 1:
            # candidate_idãŒå…¨ã¦ç•°ãªã‚‹å ´åˆã®ã¿å ±å‘Š
            ids = [entry.get('candidate_id') for entry in entries]
            if len(set(ids)) == len(ids):  # å…¨ã¦ã®IDãŒç•°ãªã‚‹
                duplicates[f"åå‰é‡è¤‡: {name}"] = entries
    
    return duplicates

def deduplicate_candidates(candidates):
    """é‡è¤‡å€™è£œè€…ã‚’å‰Šé™¤"""
    
    seen_ids = set()
    seen_names = set()
    unique_candidates = []
    
    for candidate in candidates:
        candidate_id = candidate.get('candidate_id', '')
        name = candidate.get('name', '')
        prefecture = candidate.get('prefecture', '')
        
        # å„ªå…ˆåº¦åˆ¤å®šã®ãŸã‚ã®ã‚­ãƒ¼
        priority_key = (candidate_id, name)
        
        # candidate_idãƒ™ãƒ¼ã‚¹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if candidate_id and candidate_id in seen_ids:
            logger.debug(f"IDé‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {name} ({candidate_id}) - {prefecture}")
            continue
        
        # åå‰ã¨éƒ½é“åºœçœŒã®çµ„ã¿åˆã‚ã›ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
        name_pref_key = (name, prefecture)
        if name and name_pref_key in seen_names:
            logger.debug(f"åå‰+éƒ½é“åºœçœŒé‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {name} - {prefecture}")
            continue
        
        # é‡è¤‡ã—ã¦ã„ãªã„å ´åˆã¯è¿½åŠ 
        unique_candidates.append(candidate)
        
        if candidate_id:
            seen_ids.add(candidate_id)
        if name:
            seen_names.add(name_pref_key)
    
    return unique_candidates

def show_final_statistics(candidates):
    """æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º"""
    
    party_count = defaultdict(int)
    prefecture_count = defaultdict(int)
    
    for candidate in candidates:
        party = candidate.get('party', 'ç„¡æ‰€å±')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_count[party] += 1
        prefecture_count[prefecture] += 1
    
    logger.info("ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…æ•°: {len(candidates)}å")
    logger.info(f"  æ”¿å…šæ•°: {len(party_count)}æ”¿å…š")
    logger.info(f"  éƒ½é“åºœçœŒæ•°: {len(prefecture_count)}éƒ½é“åºœçœŒ")
    
    logger.info("ğŸ›ï¸ æ”¿å…šåˆ¥çµ±è¨ˆï¼ˆä¸Šä½10ï¼‰:")
    top_parties = sorted(party_count.items(), key=lambda x: x[1], reverse=True)[:10]
    for party, count in top_parties:
        logger.info(f"  {party}: {count}å")
    
    logger.info("ğŸ—¾ éƒ½é“åºœçœŒåˆ¥çµ±è¨ˆï¼ˆä¸Šä½10ï¼‰:")
    top_prefectures = sorted(prefecture_count.items(), key=lambda x: x[1], reverse=True)[:10]
    for prefecture, count in top_prefectures:
        logger.info(f"  {prefecture}: {count}å")

if __name__ == "__main__":
    remove_duplicates()