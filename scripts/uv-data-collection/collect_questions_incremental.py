#!/usr/bin/env python3
"""
è³ªå•ä¸»æ„æ›¸åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå¢—åˆ†åé›†ç‰ˆï¼‰

è¡†è­°é™¢è³ªå•ä¸»æ„æ›¸ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å®Ÿéš›ã®è³ªå•ä¸»æ„æ›¸æƒ…å ±ã‚’å¢—åˆ†åé›†
æ—¢çŸ¥ã®URLæ§‹é€  a217001.htm, a217002.htm, ... ã‚’åˆ©ç”¨ã—ã¦åŠ¹ç‡çš„ã«åé›†
"""

import json
import requests
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import logging
import random
from urllib.parse import urljoin

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class QuestionsIncrementalCollector:
    """è³ªå•ä¸»æ„æ›¸å¢—åˆ†åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, max_questions: int = 100):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.update_headers()
        
        # åé›†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.max_questions = max_questions
        
        # å¯¾è±¡ã¨ãªã‚‹å›½ä¼šå›æ¬¡ï¼ˆæœ€è¿‘ã®3ã¤ã®å›½ä¼šï¼‰
        self.target_sessions = [217, 216, 215]
        
        logger.info(f"æœ€å¤§åé›†ä»¶æ•°: {self.max_questions}ä»¶")
        logger.info(f"å¯¾è±¡å›½ä¼š: {', '.join(f'ç¬¬{s}å›' for s in self.target_sessions)}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.project_root = Path(__file__).parent.parent.parent
        self.questions_dir = self.project_root / "data" / "processed" / "questions"
        self.frontend_questions_dir = self.project_root / "frontend" / "public" / "data" / "questions"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.questions_dir.mkdir(parents=True, exist_ok=True)
        self.frontend_questions_dir.mkdir(parents=True, exist_ok=True)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        self.existing_questions = self.load_existing_questions()
        
        # åŸºæœ¬URLè¨­å®š
        self.base_url = "https://www.shugiin.go.jp"
        
    def update_headers(self):
        """User-Agentæ›´æ–°ã¨IPå½è£…"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
    
    def random_delay(self, min_seconds=1, max_seconds=3):
        """ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ"""
        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)
    
    def load_existing_questions(self) -> set:
        """æ—¢å­˜ã®è³ªå•ä¸»æ„æ›¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è³ªå•URLã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
        existing_urls = set()
        existing_numbers = set()
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        latest_file = self.frontend_questions_dir / "questions_latest.json"
        if latest_file.exists():
            try:
                with open(latest_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if 'data' in data:
                        for question in data['data']:
                            if 'question_url' in question:
                                existing_urls.add(question['question_url'])
                                # URLã‹ã‚‰è³ªå•ç•ªå·ã‚’æŠ½å‡º
                                number = self.extract_question_number_from_url(question['question_url'])
                                if number:
                                    existing_numbers.add(number)
                            elif 'url' in question:
                                existing_urls.add(question['url'])
                                number = self.extract_question_number_from_url(question['url'])
                                if number:
                                    existing_numbers.add(number)
                logger.info(f"æ—¢å­˜è³ªå•ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(existing_urls)}ä»¶")
            except Exception as e:
                logger.warning(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚‚èª­ã¿è¾¼ã¿
        for file_path in self.frontend_questions_dir.glob("questions_*.json"):
            if file_path.name == "questions_latest.json":
                continue
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if 'data' in data:
                        for question in data['data']:
                            url = question.get('question_url') or question.get('url')
                            if url:
                                existing_urls.add(url)
                                number = self.extract_question_number_from_url(url)
                                if number:
                                    existing_numbers.add(number)
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        logger.info(f"æ—¢å­˜è³ªå•ç·æ•°: {len(existing_urls)}ä»¶")
        logger.info(f"æ—¢å­˜è³ªå•ç•ªå·: {sorted(existing_numbers)[:10]}...ç­‰")
        return existing_urls
    
    def extract_question_number_from_url(self, url: str) -> Optional[str]:
        """URLã‹ã‚‰è³ªå•ç•ªå·ã‚’æŠ½å‡ºï¼ˆå›½ä¼š+ç•ªå·ã®çµ„ã¿åˆã‚ã›ï¼‰"""
        # a217001.htm -> 217001
        match = re.search(r'a(\d{6})\.htm', url)
        if match:
            return match.group(1)
        
        # ã‚ˆã‚ŠæŸ”è»Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³
        match = re.search(r'a(\d+)\.htm', url)
        if match:
            return match.group(1)
        
        return None
    
    def build_question_url(self, session: int, question_num: int) -> str:
        """è³ªå•URLã‚’æ§‹ç¯‰"""
        # 3æ¡ã®è³ªå•ç•ªå·ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        question_str = f"{question_num:03d}"
        filename = f"a{session}{question_str}.htm"
        return f"{self.base_url}/internet/itdb_shitsumon.nsf/html/shitsumon/{filename}"
    
    def check_question_exists(self, url: str) -> bool:
        """è³ªå•ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            response = self.session.head(url, timeout=10)
            return response.status_code == 200
        except Exception:
            return False
    
    def collect_questions_incrementally(self) -> List[Dict[str, Any]]:
        """å¢—åˆ†çš„ã«è³ªå•ä¸»æ„æ›¸ã‚’åé›†"""
        logger.info("è³ªå•ä¸»æ„æ›¸å¢—åˆ†åé›†é–‹å§‹...")
        all_questions = []
        
        try:
            for session in self.target_sessions:
                logger.info(f"ç¬¬{session}å›å›½ä¼šã®è³ªå•åé›†é–‹å§‹...")
                
                # å„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§1ã‹ã‚‰é †ç•ªã«ãƒã‚§ãƒƒã‚¯
                question_num = 1
                consecutive_not_found = 0
                max_consecutive_not_found = 10  # é€£ç¶šã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ä¸Šé™
                
                while len(all_questions) < self.max_questions and consecutive_not_found < max_consecutive_not_found:
                    question_url = self.build_question_url(session, question_num)
                    
                    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯
                    if question_url in self.existing_questions:
                        logger.debug(f"æ—¢å­˜è³ªå•ã‚’ã‚¹ã‚­ãƒƒãƒ—: {question_url}")
                        question_num += 1
                        consecutive_not_found = 0
                        continue
                    
                    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                    self.random_delay()
                    self.update_headers()
                    
                    # è³ªå•ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if not self.check_question_exists(question_url):
                        logger.debug(f"è³ªå•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {question_url}")
                        consecutive_not_found += 1
                        question_num += 1
                        continue
                    
                    # è³ªå•è©³ç´°ã‚’å–å¾—
                    try:
                        question_detail = self.extract_question_detail(question_url, session, question_num)
                        if question_detail:
                            all_questions.append(question_detail)
                            logger.info(f"è³ªå•å–å¾—æˆåŠŸ ({len(all_questions)}/{self.max_questions}): ç¬¬{session}å›ç¬¬{question_num}å· - {question_detail['title'][:50]}...")
                            consecutive_not_found = 0
                        else:
                            consecutive_not_found += 1
                    
                    except Exception as e:
                        logger.error(f"è³ªå•è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ {question_url}: {str(e)}")
                        consecutive_not_found += 1
                    
                    question_num += 1
                    
                    # æœ€å¤§ç•ªå·ãƒã‚§ãƒƒã‚¯ï¼ˆé€šå¸¸ã¯è³ªå•ç•ªå·ã¯100ã‚’è¶…ãˆã‚‹ã“ã¨ã¯ç¨€ï¼‰
                    if question_num > 200:
                        logger.info(f"ç¬¬{session}å›å›½ä¼šã®è³ªå•ç•ªå·ãŒ200ã‚’è¶…ãˆã¾ã—ãŸã€‚æ¬¡ã®å›½ä¼šã«ç§»è¡Œã—ã¾ã™ã€‚")
                        break
                
                logger.info(f"ç¬¬{session}å›å›½ä¼šã‹ã‚‰{sum(1 for q in all_questions if q.get('session') == session)}ä»¶ã®è³ªå•ã‚’åé›†")
                
                # æœ€å¤§åé›†ä»¶æ•°ã«åˆ°é”ã—ãŸå ´åˆã¯çµ‚äº†
                if len(all_questions) >= self.max_questions:
                    logger.info(f"æœ€å¤§åé›†ä»¶æ•°({self.max_questions})ã«åˆ°é”ã—ã¾ã—ãŸ")
                    break
            
            logger.info(f"è³ªå•ä¸»æ„æ›¸å¢—åˆ†åé›†å®Œäº†: {len(all_questions)}ä»¶")
            return all_questions
            
        except Exception as e:
            logger.error(f"è³ªå•ä¸»æ„æ›¸å¢—åˆ†åé›†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return all_questions
    
    def extract_question_detail(self, question_url: str, session: int, question_num: int) -> Optional[Dict[str, Any]]:
        """è³ªå•è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            response = self.session.get(question_url, timeout=30)
            response.raise_for_status()
            response.encoding = 'shift_jis'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # è³ªå•ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
            title = self.extract_title(soup)
            if not title:
                title = f"ç¬¬{session}å›å›½ä¼šè³ªå•ç¬¬{question_num}å·"
            
            # è³ªå•å†…å®¹ã‚’è§£æ
            question_content = self.extract_question_content(soup)
            questioner = self.extract_questioner(soup)
            submission_date = self.extract_date(soup, 'submission')
            
            # HTML/PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
            html_links = self.extract_document_links(soup, question_url, 'html')
            pdf_links = self.extract_document_links(soup, question_url, 'pdf')
            
            # å›ç­”URLã‚’æ§‹ç¯‰
            answer_url = self.build_answer_url(question_url)
            
            question_detail = {
                'title': title,
                'question_number': str(question_num),
                'session': session,
                'questioner': questioner,
                'house': 'è¡†è­°é™¢',
                'submission_date': submission_date,
                'answer_date': '',  # ç­”å¼æ—¥ã¯åˆ¥é€”å–å¾—ãŒå¿…è¦
                'question_content': question_content,
                'answer_content': '',  # ç­”å¼å†…å®¹ã¯åˆ¥é€”å–å¾—ãŒå¿…è¦
                'question_url': question_url,
                'answer_url': answer_url,
                'html_links': html_links,
                'pdf_links': pdf_links,
                'category': self.classify_question_category(title + " " + question_content),
                'collected_at': datetime.now().isoformat(),
                'year': datetime.now().year,
                'week': datetime.now().isocalendar()[1]
            }
            
            return question_detail
            
        except Exception as e:
            logger.error(f"è³ªå•è©³ç´°æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({question_url}): {str(e)}")
            return None
    
    def extract_title(self, soup: BeautifulSoup) -> str:
        """è³ªå•ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º"""
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
                if title and len(title) > 5:
                    return title
            
            # h1, h2ã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
            for tag_name in ['h1', 'h2', 'h3']:
                header_tag = soup.find(tag_name)
                if header_tag:
                    title = header_tag.get_text(strip=True)
                    if title and len(title) > 5:
                        return title
            
            # è³ªå•ä¸»æ„æ›¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
            page_text = soup.get_text()
            title_patterns = [
                r'(.+?ã«é–¢ã™ã‚‹è³ªå•ä¸»æ„æ›¸)',
                r'(.+?ã«ã¤ã„ã¦.*?è³ªå•ä¸»æ„æ›¸)',
                r'(.+?)è³ªå•ä¸»æ„æ›¸'
            ]
            
            for pattern in title_patterns:
                match = re.search(pattern, page_text)
                if match:
                    title = match.group(1).strip()
                    if len(title) > 5:
                        return title + "ã«é–¢ã™ã‚‹è³ªå•ä¸»æ„æ›¸"
            
            return ""
            
        except Exception as e:
            logger.error(f"ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_question_content(self, soup: BeautifulSoup) -> str:
        """è³ªå•å†…å®¹ã‚’æŠ½å‡º"""
        try:
            full_text = soup.get_text(separator='\n', strip=True)
            
            # ã¾ãšãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ–‡è¨€ã‚’é™¤å»
            cleaned_text = self.remove_navigation_text(full_text)
            
            # è³ªå•éƒ¨åˆ†ã‚’ç‰¹å®šã™ã‚‹æ§˜ã€…ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
            patterns = [
                r'è³ªå•ä¸»æ„æ›¸(.+?)å³è³ªå•ã™ã‚‹',
                r'æå‡ºè€…\s+.+?(.+?)å³è³ªå•ã™ã‚‹',
                r'(.{100,}?)å³è³ªå•ã™ã‚‹'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, cleaned_text, re.DOTALL)
                if match:
                    content = match.group(1).strip()
                    if len(content) > 50:
                        return self.clean_text(content)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã‚’è¿”ã™ï¼ˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³é™¤å»å¾Œï¼‰
            if len(cleaned_text) > 100:
                return self.clean_text(cleaned_text[:1000])
            
            return ""
            
        except Exception as e:
            logger.error(f"è³ªå•å†…å®¹æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_questioner(self, soup: BeautifulSoup) -> str:
        """è³ªå•è€…ã‚’æŠ½å‡º"""
        try:
            page_text = soup.get_text()
            
            patterns = [
                r'æå‡ºè€…\s+([^\s\n]+)',
                r'æå‡ºè€…[ï¼š:]\s*([^\s\n]+)',
                r'([^\s\n]+)å›æå‡º',
                r'è³ªå•ä¸»æ„æ›¸æå‡ºè€…\s+([^\s\n]+)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, page_text)
                if match:
                    questioner = match.group(1).strip()
                    if not questioner.endswith('å›'):
                        questioner += 'å›'
                    return questioner
            
            return ""
            
        except Exception as e:
            logger.error(f"è³ªå•è€…æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return ""
    
    def extract_date(self, soup: BeautifulSoup, date_type: str) -> str:
        """æ—¥ä»˜ã‚’æŠ½å‡º"""
        try:
            page_text = soup.get_text()
            
            if date_type == 'submission':
                patterns = [
                    r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥æå‡º',
                    r'æå‡º.*?(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
                    r'ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥æå‡º'
                ]
            else:  # answer
                patterns = [
                    r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥ç­”å¼',
                    r'ç­”å¼.*?(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥'
                ]
            
            for pattern in patterns:
                match = re.search(pattern, page_text)
                if match:
                    groups = match.groups()
                    if len(groups) == 3:
                        year, month, day = groups
                        # ä»¤å’Œå¹´å·ã®å‡¦ç†
                        if len(year) <= 2:
                            year = str(2018 + int(year))
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            return ""
            
        except Exception as e:
            logger.error(f"æ—¥ä»˜æŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({date_type}): {str(e)}")
            return ""
    
    def extract_document_links(self, soup: BeautifulSoup, base_url: str, doc_type: str) -> List[Dict[str, str]]:
        """è³ªå•ä¸»æ„æ›¸é–¢é€£ã®HTML/PDFãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç³»ãƒªãƒ³ã‚¯ã®ã¿é™¤å¤–ï¼‰"""
        links = []
        
        try:
            # æ˜ç¢ºã«é™¤å¤–ã™ã¹ããƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç³»ã®ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
            excluded_texts = [
                'ã‚µã‚¤ãƒˆãƒãƒƒãƒ—', 'ãƒ˜ãƒ«ãƒ—', 'ãƒ¡ã‚¤ãƒ³ã¸ã‚¹ã‚­ãƒƒãƒ—', 'ãƒ›ãƒ¼ãƒ ',
                'è¡†è­°é™¢ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸', 'ç«‹æ³•æƒ…å ±', 'è³ªå•ç­”å¼æƒ…å ±',
                'è³ªå•ã®ä¸€è¦§', 'ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸'
            ]
            
            # æ˜ç¢ºã«é™¤å¤–ã™ã¹ãURLãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿
            excluded_url_patterns = [
                'sitemap', 'help', 'index.nsf', 'honkai_top', 
                'rippo_top', 'giin_top', 'shiryo_top', 'tetsuzuki_top', 
                'index_e', 'menu_m'
            ]
            
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # æ˜ç¢ºã«ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ç³»ã¨ã‚ã‹ã‚‹ã‚‚ã®ã®ã¿é™¤å¤–
                is_excluded = False
                
                # é™¤å¤–ã™ã¹ããƒ†ã‚­ã‚¹ãƒˆã‹ãƒã‚§ãƒƒã‚¯
                for excluded_text in excluded_texts:
                    if excluded_text in text:
                        is_excluded = True
                        break
                
                if is_excluded:
                    continue
                
                # é™¤å¤–ã™ã¹ãURLãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ãƒã‚§ãƒƒã‚¯
                for pattern in excluded_url_patterns:
                    if pattern in href.lower():
                        is_excluded = True
                        break
                
                if is_excluded:
                    continue
                
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if doc_type == 'html':
                    # HTMLãƒªãƒ³ã‚¯: .htm/.htmlæ‹¡å¼µå­ ã¾ãŸã¯ è³ªå•é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    is_target = ('.htm' in href.lower() or 
                               '.html' in href.lower() or
                               'shitsumon' in href.lower() or
                               'çµŒé' in text or
                               'è³ªå•æœ¬æ–‡' in text or
                               'ç­”å¼æœ¬æ–‡' in text)
                else:  # pdf
                    # PDFãƒªãƒ³ã‚¯: .pdfæ‹¡å¼µå­ ã¾ãŸã¯ PDFé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                    is_target = ('.pdf' in href.lower() or
                               'PDF' in text)
                
                if is_target:
                    full_url = urljoin(base_url, href)
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if not any(existing['url'] == full_url for existing in links):
                        # ã€ŒHTMLæ–‡æ›¸ã€ãªã©ã®æ±ç”¨çš„ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã¯æ”¹å–„
                        display_title = text
                        if text == 'HTMLæ–‡æ›¸' and 'shitsumon' in href:
                            if '/a' in href:
                                display_title = 'è³ªå•æœ¬æ–‡'
                            elif '/b' in href:
                                display_title = 'ç­”å¼æœ¬æ–‡'
                        
                        links.append({
                            'url': full_url,
                            'title': display_title or f'{doc_type.upper()}æ–‡æ›¸',
                            'type': doc_type
                        })
            
        except Exception as e:
            logger.error(f"{doc_type}ãƒªãƒ³ã‚¯æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return links
    
    def build_answer_url(self, question_url: str) -> str:
        """è³ªå•URLã‹ã‚‰ç­”å¼URLã‚’æ§‹ç¯‰"""
        try:
            # a217001.htm -> b217001.htm
            answer_url = question_url.replace('/a', '/b')
            if answer_url != question_url:
                return answer_url
            return ""
        except Exception:
            return ""
    
    def classify_question_category(self, text: str) -> str:
        """è³ªå•ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é¡"""
        categories = {
            'å¤–äº¤ãƒ»å®‰å…¨ä¿éšœ': ['å¤–äº¤', 'å¤–å‹™', 'å›½éš›', 'å®‰ä¿', 'å®‰å…¨ä¿éšœ', 'æ¡ç´„', 'é˜²è¡›', 'è‡ªè¡›éšŠ'],
            'çµŒæ¸ˆãƒ»ç”£æ¥­': ['çµŒæ¸ˆ', 'è²¡æ”¿', 'äºˆç®—', 'ç¨åˆ¶', 'é‡‘è', 'ç”£æ¥­', 'ä¼æ¥­', 'æŠ•è³‡'],
            'ç¤¾ä¼šä¿éšœ': ['å¹´é‡‘', 'åŒ»ç™‚', 'ä»‹è­·', 'ç¦ç¥‰', 'ç¤¾ä¼šä¿éšœ', 'å¥åº·'],
            'æ•™è‚²ãƒ»æ–‡åŒ–': ['æ•™è‚²', 'å­¦æ ¡', 'å¤§å­¦', 'æ–‡åŒ–', 'æ–‡éƒ¨ç§‘å­¦'],
            'ç’°å¢ƒãƒ»ã‚¨ãƒãƒ«ã‚®ãƒ¼': ['ç’°å¢ƒ', 'æ°—å€™', 'ã‚¨ãƒãƒ«ã‚®ãƒ¼', 'åŸå­åŠ›', 'å†ç”Ÿå¯èƒ½'],
            'åŠ´åƒãƒ»é›‡ç”¨': ['åŠ´åƒ', 'é›‡ç”¨', 'åƒãæ–¹', 'è³ƒé‡‘', 'è·æ¥­'],
            'åœ°æ–¹ãƒ»éƒ½å¸‚': ['åœ°æ–¹', 'è‡ªæ²»ä½“', 'éƒ½é“åºœçœŒ', 'å¸‚ç”ºæ‘'],
            'å¸æ³•ãƒ»æ³•å‹™': ['å¸æ³•', 'è£åˆ¤', 'æ³•å‹™', 'åˆ‘äº‹', 'æ°‘äº‹'],
            'äº¤é€šãƒ»å›½åœŸ': ['äº¤é€š', 'é“è·¯', 'é‰„é“', 'èˆªç©º', 'å›½åœŸ', 'æ¸¯æ¹¾']
        }
        
        text_lower = text.lower()
        
        for category, keywords in categories.items():
            if any(keyword in text_lower for keyword in keywords):
                return category
        
        return 'ä¸€èˆ¬'
    
    def remove_navigation_text(self, text: str) -> str:
        """ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ–‡è¨€ã‚’é™¤å»"""
        if not text:
            return ""
        
        # é™¤å»ã™ã‚‹ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ–‡è¨€ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        navigation_patterns = [
            r'ãƒ¡ã‚¤ãƒ³ã¸ã‚¹ã‚­ãƒƒãƒ—',
            r'ã‚µã‚¤ãƒˆãƒãƒƒãƒ—',
            r'ãƒ˜ãƒ«ãƒ—',
            r'ãƒ–ãƒ©ã‚¦ã‚¶ã®JavaScriptãŒç„¡åŠ¹ã®ãŸã‚ã€ã‚µã‚¤ãƒˆå†…æ¤œç´¢ã¯ã”åˆ©ç”¨ã„ãŸã ã‘ã¾ã›ã‚“ã€‚',
            r'éŸ³å£°èª­ã¿ä¸Šã’',
            r'ã‚µã‚¤ãƒˆå†…æ¤œç´¢',
            r'è¡†è­°é™¢ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸\s*>',
            r'ç«‹æ³•æƒ…å ±\s*>',
            r'è³ªå•ç­”å¼æƒ…å ±\s*>',
            r'ç¬¬\d+å›å›½ä¼š\s+è³ªå•ã®ä¸€è¦§\s*>',
            r'è³ªå•æœ¬æ–‡æƒ…å ±',
            r'çµŒéã¸\s*\|',
            r'è³ªå•æœ¬æ–‡\(PDF\)ã¸\s*\|',
            r'ç­”å¼æœ¬æ–‡\(HTML\)ã¸\s*\|',
            r'ç­”å¼æœ¬æ–‡\(PDF\)ã¸',
            r'èª­ã¿ä¸Šã’æ©Ÿèƒ½ã‚’ã”åˆ©ç”¨ã®å ´åˆã¯.*?ã‚’ã”ä¸€èª­ãã ã•ã„ã€‚',
            r'ãªãŠã€èª­ã¿ä¸Šã’æ©Ÿèƒ½åˆ©ç”¨ã®æ³¨æ„äº‹é ….*?BR',
            r'<[^>]+>',  # HTMLã‚¿ã‚°
            r'&[a-zA-Z]+;',  # HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
        ]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦é™¤å»
        cleaned_text = text
        for pattern in navigation_patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE | re.MULTILINE)
        
        # é€£ç¶šã™ã‚‹æ”¹è¡Œã‚„ã‚¹ãƒšãƒ¼ã‚¹ã‚’æ•´ç†
        cleaned_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_text)
        cleaned_text = re.sub(r'^\s*[\|>\s]+', '', cleaned_text, flags=re.MULTILINE)
        
        return cleaned_text.strip()
    
    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢"""
        if not text:
            return ""
        
        # æ”¹è¡Œãƒ»ã‚¹ãƒšãƒ¼ã‚¹ã®æ­£è¦åŒ–
        text = re.sub(r'\n\s*\n', '\n\n', text)  # é€£ç¶šç©ºè¡Œã‚’2è¡Œã¾ã§
        text = re.sub(r'[ \t]+', ' ', text)  # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
        text = re.sub(r'[\u3000]+', ' ', text)  # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã«
        
        # è¿½åŠ ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        text = re.sub(r'^\s*[\|>\s]+', '', text, flags=re.MULTILINE)  # è¡Œé ­ã®è¨˜å·é™¤å»
        text = re.sub(r'\s*\|\s*', ' ', text)  # ãƒ‘ã‚¤ãƒ—è¨˜å·å‘¨è¾ºã®æ•´ç†
        
        return text.strip()
    
    def save_questions_data(self, questions: List[Dict[str, Any]]):
        """è³ªå•ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        if not questions:
            logger.warning("ä¿å­˜ã™ã‚‹è³ªå•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        current_date = datetime.now()
        data_period = current_date.strftime('%Y%m%d')
        timestamp = current_date.strftime('%H%M%S')
        
        data_structure = {
            "metadata": {
                "data_type": "shugiin_questions_incremental",
                "collection_method": "incremental_url_scanning",
                "total_questions": len(questions),
                "generated_at": current_date.isoformat(),
                "source_site": "www.shugiin.go.jp",
                "target_sessions": self.target_sessions,
                "data_quality": "incremental_collection",
                "version": "3.0"
            },
            "data": questions
        }
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜
        raw_filename = f"questions_{data_period}_{timestamp}.json"
        raw_filepath = self.questions_dir / raw_filename
        
        with open(raw_filepath, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        
        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        frontend_filename = f"questions_{data_period}_{timestamp}.json"
        frontend_filepath = self.frontend_questions_dir / frontend_filename
        
        with open(frontend_filepath, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°
        latest_file = self.frontend_questions_dir / "questions_latest.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(data_structure, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“ æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°: {latest_file}")
        
        logger.info(f"è³ªå•ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†:")
        logger.info(f"  - ç”Ÿãƒ‡ãƒ¼ã‚¿: {raw_filepath}")
        logger.info(f"  - ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰: {frontend_filepath}")
        logger.info(f"  - ä»¶æ•°: {len(questions)}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è³ªå•ä¸»æ„æ›¸åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå¢—åˆ†åé›†ç‰ˆï¼‰')
    parser.add_argument('--max-questions', type=int, default=100, help='æœ€å¤§åé›†ä»¶æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ä»¶)')
    
    args = parser.parse_args()
    
    collector = QuestionsIncrementalCollector(max_questions=args.max_questions)
    
    try:
        # è³ªå•ä¸»æ„æ›¸åé›†
        questions = collector.collect_questions_incrementally()
        
        if questions:
            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            collector.save_questions_data(questions)
            logger.info(f"æ–°è¦è³ªå•ä¸»æ„æ›¸åé›†å®Œäº†: {len(questions)}ä»¶")
        else:
            logger.info("æ–°è¦è³ªå•ä¸»æ„æ›¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
    except Exception as e:
        logger.error(f"ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

if __name__ == "__main__":
    main()