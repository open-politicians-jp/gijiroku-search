#!/usr/bin/env python3
"""
æ­£ç¢ºãªåå‰æŠ½å‡ºã¨ãƒ‡ãƒ¼ã‚¿é‡è¤‡ä¿®æ­£
"""

import json
import logging
import requests
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import re
from fake_useragent import UserAgent
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FixedExtractionProperNames:
    def __init__(self):
        self.session = requests.Session()
        ua = UserAgent()
        desktop_ua = ua.random
        
        self.session.headers.update({
            'User-Agent': desktop_ua,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # æ”¿å…šãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆ
        self.parties = [
            "è‡ªç”±æ°‘ä¸»å…š", "ç«‹æ†²æ°‘ä¸»å…š", "æ—¥æœ¬ç¶­æ–°ã®ä¼š", "å…¬æ˜å…š", "æ—¥æœ¬å…±ç”£å…š",
            "å›½æ°‘æ°‘ä¸»å…š", "ã‚Œã„ã‚æ–°é¸çµ„", "å‚æ”¿å…š", "ç¤¾ä¼šæ°‘ä¸»å…š", "NHKå…š",
            "æ—¥æœ¬ä¿å®ˆå…š", "æ—¥æœ¬æ”¹é©å…š", "ç„¡æ‰€å±", "ç„¡æ‰€å±é€£åˆ", "æ—¥æœ¬èª çœŸä¼š",
            "æ—¥æœ¬ã®å®¶åº­ã‚’å®ˆã‚‹ä¼š", "å†ç”Ÿã®é“", "å·®åˆ¥æ’²æ»…å…š", "æ ¸èåˆå…š", "ãƒãƒ¼ãƒ ã¿ã‚‰ã„",
            "å¤šå¤«å¤šå¦»å…š", "å›½æ”¿ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ä¼š", "æ–°å…šã‚„ã¾ã¨", "æœªåˆ†é¡"
        ]
    
    def extract_prefecture_fixed(self, pref_code: int, pref_name: str):
        """ä¿®æ­£ç‰ˆéƒ½é“åºœçœŒæŠ½å‡º"""
        url = f"https://sangiin.go2senkyo.com/2025/prefecture/{pref_code}"
        logger.info(f"ğŸ” {pref_name} (ã‚³ãƒ¼ãƒ‰: {pref_code}) ä¿®æ­£ç‰ˆæŠ½å‡ºé–‹å§‹")
        
        try:
            # ãƒšãƒ¼ã‚¸å–å¾—
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                logger.error(f"âŒ {pref_name} ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: HTTP {response.status_code}")
                return []
            
            logger.info(f"ğŸ“Š {pref_name} HTMLå–å¾—: {len(response.text):,} æ–‡å­—")
            
            # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            time.sleep(2)
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒªãƒ³ã‚¯ã‚’å–å¾—
            profile_links = soup.find_all('a', href=re.compile(r'/seijika/\d+'))
            unique_ids = set()
            
            for link in profile_links:
                match = re.search(r'/seijika/(\d+)', link.get('href', ''))
                if match:
                    unique_ids.add(match.group(1))
            
            logger.info(f"ğŸ“Š {pref_name} ç™ºè¦‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ID: {len(unique_ids)}å€‹")
            
            candidates = []
            
            # å„å€™è£œè€…IDã«å¯¾ã—ã¦æ­£ç¢ºãªæƒ…å ±ã‚’å–å¾—
            for candidate_id in unique_ids:
                try:
                    candidate = self.get_accurate_candidate_info(candidate_id, pref_name, url)
                    if candidate:
                        candidates.append(candidate)
                        logger.info(f"  âœ… {candidate['name']} ({candidate['party']}) - ID: {candidate_id}")
                    
                    # APIåˆ¶é™å¯¾ç­–
                    time.sleep(1)
                
                except Exception as e:
                    logger.debug(f"å€™è£œè€… {candidate_id} å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            logger.info(f"ğŸ¯ {pref_name} æŠ½å‡ºå®Œäº†: {len(candidates)}å")
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def get_accurate_candidate_info(self, candidate_id: str, prefecture: str, source_url: str):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰æ­£ç¢ºãªå€™è£œè€…æƒ…å ±ã‚’å–å¾—"""
        profile_url = f"https://go2senkyo.com/seijika/{candidate_id}"
        
        try:
            logger.debug(f"ğŸ“„ ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—: {profile_url}")
            
            response = self.session.get(profile_url, timeout=20)
            if response.status_code != 200:
                logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {candidate_id}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å€™è£œè€…åŸºæœ¬æƒ…å ±
            candidate = {
                "candidate_id": f"go2s_{candidate_id}",
                "name": "",
                "name_kana": "",
                "prefecture": prefecture,
                "constituency": prefecture.replace('çœŒ', '').replace('åºœ', '').replace('éƒ½', '').replace('é“', ''),
                "constituency_type": "single_member",
                "party": "æœªåˆ†é¡",
                "party_normalized": "æœªåˆ†é¡",
                "profile_url": profile_url,
                "source_page": source_url,
                "source": "profile_page_extraction",
                "collected_at": datetime.now().isoformat()
            }
            
            # åå‰ã¨èª­ã¿ã®æŠ½å‡º
            name_info = self.extract_name_and_reading_from_profile(soup)
            if name_info:
                candidate["name"] = name_info["name"]
                if name_info["reading"]:
                    candidate["name_kana"] = name_info["reading"]
            
            # æ”¿å…šæƒ…å ±ã®æŠ½å‡º
            party = self.extract_party_from_profile(soup)
            if party:
                candidate["party"] = party
                candidate["party_normalized"] = party
            
            # åå‰ãŒå–å¾—ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not candidate["name"]:
                logger.debug(f"åå‰å–å¾—å¤±æ•—: {candidate_id}")
                return None
            
            return candidate
            
        except Exception as e:
            logger.debug(f"ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ {candidate_id}: {e}")
            return None
    
    def extract_name_and_reading_from_profile(self, soup: BeautifulSoup):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰åå‰ã¨èª­ã¿ã‚’æŠ½å‡º"""
        try:
            # æ–¹æ³•1: Go2senkyoç‰¹æœ‰ã®æ§‹é€ ã‚’ä½¿ç”¨
            # åå‰: <h1 class="p_seijika_profle_ttl">ç‰§å±± ã²ã‚ãˆ</h1>
            name_elem = soup.find('h1', class_='p_seijika_profle_ttl')
            if name_elem:
                name_text = name_elem.get_text(strip=True)
                logger.debug(f"åå‰è¦ç´ ç™ºè¦‹: {name_text}")
                
                # èª­ã¿: <p class="p_seijika_profle_subttl">ãƒã‚­ãƒ¤ãƒ ãƒ’ãƒ­ã‚¨ï¼60æ­³ï¼å¥³</p>
                reading_elem = soup.find('p', class_='p_seijika_profle_subttl')
                reading_text = ""
                if reading_elem:
                    reading_full = reading_elem.get_text(strip=True)
                    # "ãƒã‚­ãƒ¤ãƒ ãƒ’ãƒ­ã‚¨ï¼60æ­³ï¼å¥³" ã‹ã‚‰èª­ã¿éƒ¨åˆ†ã®ã¿æŠ½å‡º
                    reading_match = re.search(r'^([ã‚¡-ãƒ¶ãƒ¼\s]+)', reading_full)
                    if reading_match:
                        reading_text = reading_match.group(1).strip()
                        logger.debug(f"èª­ã¿è¦ç´ ç™ºè¦‹: {reading_text}")
                
                return {"name": name_text, "reading": reading_text}
            
            # æ–¹æ³•2: titleã‚¿ã‚°ã‹ã‚‰æŠ½å‡ºï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
            title_elem = soup.find('title')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                logger.debug(f"titleã‚¿ã‚°: {title_text}")
                
                # "ç‰§å±±ã²ã‚ãˆï¼ˆãƒã‚­ãƒ¤ãƒãƒ’ãƒ­ã‚¨ï¼‰ï½œæ”¿æ²»å®¶æƒ…å ±ï½œé¸æŒ™ãƒ‰ãƒƒãƒˆã‚³ãƒ " å½¢å¼
                title_match = re.search(r'^([ä¸€-é¾¯ã²ã‚‰ãŒãª\s]+)ï¼ˆ([ã‚¡-ãƒ¶ãƒ¼\s]+)ï¼‰', title_text)
                if title_match:
                    name = title_match.group(1).strip()
                    reading = title_match.group(2).strip()
                    logger.debug(f"titleã‹ã‚‰æŠ½å‡º: åå‰={name}, èª­ã¿={reading}")
                    return {"name": name, "reading": reading}
                
                # "å€™è£œè€…å | ã‚µã‚¤ãƒˆå" å½¢å¼ï¼ˆèª­ã¿ãªã—ï¼‰
                if '|' in title_text:
                    name_part = title_text.split('|')[0].strip()
                    # ã‚«ãƒƒã‚³éƒ¨åˆ†ã‚’é™¤å»
                    name_clean = re.sub(r'ï¼ˆ.*?ï¼‰', '', name_part).strip()
                    name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãª\s]+)', name_clean)
                    if name_match:
                        name = name_match.group(1).strip()
                        return {"name": name, "reading": ""}
            
            # æ–¹æ³•3: ä»–ã®h1è¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
            h1_elements = soup.find_all('h1')
            for h1 in h1_elements:
                h1_text = h1.get_text(strip=True)
                if h1_text and len(h1_text) <= 20 and h1_text != "é¸æŒ™ãƒ‰ãƒƒãƒˆã‚³ãƒ ":
                    # æ—¥æœ¬äººåãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ
                    name_match = re.search(r'([ä¸€-é¾¯ã²ã‚‰ãŒãª\s]+)', h1_text)
                    if name_match:
                        name = name_match.group(1).strip()
                        logger.debug(f"h1ã‹ã‚‰æŠ½å‡º: {name}")
                        return {"name": name, "reading": ""}
            
            # æ–¹æ³•4: data-history_ttlå±æ€§ã‹ã‚‰æŠ½å‡º
            contents_div = soup.find('div', {'data-history_ttl': True})
            if contents_div:
                history_name = contents_div.get('data-history_ttl', '').strip()
                if history_name:
                    logger.debug(f"data-history_ttlã‹ã‚‰æŠ½å‡º: {history_name}")
                    return {"name": history_name, "reading": ""}
            
        except Exception as e:
            logger.debug(f"åå‰ãƒ»èª­ã¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return {"name": "", "reading": ""}
    
    def parse_name_and_reading(self, full_name: str):
        """åå‰ã¨èª­ã¿ã‚’åˆ†é›¢ï¼ˆå¤§ãã„æ–¹ãŒåå‰ã€å°ã•ã„æ–¹ãŒèª­ã¿ï¼‰"""
        try:
            # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ãƒ»åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²
            parts = re.split(r'[\sã€€]+', full_name.strip())
            
            if len(parts) == 2:
                part1, part2 = parts[0].strip(), parts[1].strip()
                
                # æ–‡å­—æ•°æ¯”è¼ƒï¼ˆå¤§ãã„æ–¹ãŒåå‰ã€å°ã•ã„æ–¹ãŒèª­ã¿ï¼‰
                if len(part1) >= len(part2):
                    name = part1
                    reading = part2
                else:
                    name = part2
                    reading = part1
                
                # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãŒå¤šã„æ–¹ã‚’èª­ã¿ã«
                part1_kana_ratio = len(re.findall(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]', part1)) / len(part1) if part1 else 0
                part2_kana_ratio = len(re.findall(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠ]', part2)) / len(part2) if part2 else 0
                
                if part1_kana_ratio > part2_kana_ratio:
                    name = part2
                    reading = part1
                elif part2_kana_ratio > part1_kana_ratio:
                    name = part1
                    reading = part2
                
                return {"name": name, "reading": reading}
            
            elif len(parts) == 1:
                # 1ã¤ã®è¦ç´ ã®å ´åˆã€ãã®ã¾ã¾åå‰ã¨ã—ã¦ä½¿ç”¨
                return {"name": parts[0], "reading": ""}
            
            else:
                # 3ã¤ä»¥ä¸Šã®è¦ç´ ãŒã‚ã‚‹å ´åˆã€æœ€åˆã®2ã¤ã‚’ä½¿ç”¨
                if len(parts) >= 2:
                    part1, part2 = parts[0].strip(), parts[1].strip()
                    
                    # åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨
                    if len(part1) >= len(part2):
                        return {"name": part1, "reading": part2}
                    else:
                        return {"name": part2, "reading": part1}
        
        except Exception as e:
            logger.debug(f"åå‰åˆ†é›¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return {"name": full_name, "reading": ""}
    
    def extract_party_from_profile(self, soup: BeautifulSoup):
        """ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰æ”¿å…šæƒ…å ±ã‚’æŠ½å‡º"""
        try:
            # ãƒšãƒ¼ã‚¸å…¨ä½“ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ”¿å…šã‚’æ¤œç´¢
            page_text = soup.get_text()
            
            for party in self.parties:
                if party in page_text:
                    return party
            
            # ã‚ˆã‚Šè©³ç´°ãªæ¤œç´¢
            text_lines = [line.strip() for line in page_text.split('\n') if line.strip()]
            for line in text_lines:
                if len(line) <= 50:  # çŸ­ã„è¡Œã«æ”¿å…šæƒ…å ±ãŒã‚ã‚‹å¯èƒ½æ€§
                    for party in self.parties:
                        if party in line:
                            return party
        
        except Exception as e:
            logger.debug(f"æ”¿å…šæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        
        return "æœªåˆ†é¡"

def test_fixed_extraction():
    """ä¿®æ­£ç‰ˆæŠ½å‡ºã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("ğŸš€ ä¿®æ­£ç‰ˆåå‰æŠ½å‡ºã®ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    extractor = FixedExtractionProperNames()
    
    # ãƒ†ã‚¹ãƒˆå¯¾è±¡éƒ½é“åºœçœŒï¼ˆå•é¡ŒãŒæ˜ç¢ºãªçœŒï¼‰
    test_prefectures = [
        (14, "ç¥å¥ˆå·çœŒ"),  # ç‰§å±±é‡è¤‡å•é¡Œ
        (1, "åŒ—æµ·é“"),    # ç”°ä¸­é‡è¤‡å•é¡Œ
    ]
    
    all_results = []
    
    for pref_code, pref_name in test_prefectures:
        logger.info(f"\n=== {pref_name} ä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆ ===")
        
        try:
            candidates = extractor.extract_prefecture_fixed(pref_code, pref_name)
            all_results.extend(candidates)
            
            logger.info(f"âœ… {pref_name} ãƒ†ã‚¹ãƒˆå®Œäº†: {len(candidates)}å")
            
            # è©³ç´°è¡¨ç¤º
            logger.info(f"ğŸ“‹ {pref_name} å€™è£œè€…è©³ç´°:")
            for i, candidate in enumerate(candidates):
                reading_info = f" (èª­ã¿: {candidate['name_kana']})" if candidate['name_kana'] else ""
                logger.info(f"  {i+1}. {candidate['name']}{reading_info} ({candidate['party']}) - ID: {candidate['candidate_id']}")
            
            # ãƒ†ã‚¹ãƒˆé–“ã®å¾…æ©Ÿ
            time.sleep(3)
            
        except Exception as e:
            logger.error(f"âŒ {pref_name} ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    logger.info(f"\nğŸ¯ ä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆå®Œäº†: ç·è¨ˆ {len(all_results)}å")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    check_duplicates(all_results)
    
    # çµæœä¿å­˜
    save_fixed_test_results(all_results)

def check_duplicates(candidates):
    """é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
    logger.info("\nğŸ” é‡è¤‡ãƒã‚§ãƒƒã‚¯:")
    
    # åå‰ãƒ™ãƒ¼ã‚¹ã®é‡è¤‡
    name_counts = {}
    for candidate in candidates:
        name = candidate['name']
        prefecture = candidate['prefecture']
        key = f"{name} ({prefecture})"
        name_counts[key] = name_counts.get(key, 0) + 1
    
    duplicates = {k: v for k, v in name_counts.items() if v > 1}
    if duplicates:
        logger.info("âš ï¸ åå‰ãƒ™ãƒ¼ã‚¹é‡è¤‡:")
        for name_pref, count in duplicates.items():
            logger.info(f"  {name_pref}: {count}å›")
    else:
        logger.info("âœ… åå‰ãƒ™ãƒ¼ã‚¹é‡è¤‡ãªã—")
    
    # IDãƒ™ãƒ¼ã‚¹ã®é‡è¤‡
    id_counts = {}
    for candidate in candidates:
        candidate_id = candidate['candidate_id']
        id_counts[candidate_id] = id_counts.get(candidate_id, 0) + 1
    
    id_duplicates = {k: v for k, v in id_counts.items() if v > 1}
    if id_duplicates:
        logger.info("âš ï¸ IDãƒ™ãƒ¼ã‚¹é‡è¤‡:")
        for cid, count in id_duplicates.items():
            logger.info(f"  {cid}: {count}å›")
    else:
        logger.info("âœ… IDãƒ™ãƒ¼ã‚¹é‡è¤‡ãªã—")

def save_fixed_test_results(candidates):
    """ä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆçµæœã®ä¿å­˜"""
    logger.info("ğŸ’¾ ä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆçµæœã®ä¿å­˜...")
    
    # çµ±è¨ˆè¨ˆç®—
    party_stats = {}
    prefecture_stats = {}
    
    for candidate in candidates:
        party = candidate.get('party', 'æœªåˆ†é¡')
        prefecture = candidate.get('prefecture', 'æœªåˆ†é¡')
        
        party_stats[party] = party_stats.get(party, 0) + 1
        prefecture_stats[prefecture] = prefecture_stats.get(prefecture, 0) + 1
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    data = {
        "metadata": {
            "data_type": "go2senkyo_fixed_names_sangiin_2025",
            "collection_method": "profile_page_accurate_extraction",
            "total_candidates": len(candidates),
            "generated_at": datetime.now().isoformat(),
            "source_site": "go2senkyo.com (profile pages)",
            "test_prefectures": ["ç¥å¥ˆå·çœŒ", "åŒ—æµ·é“"]
        },
        "statistics": {
            "by_party": party_stats,
            "by_prefecture": prefecture_stats,
            "by_constituency_type": {"single_member": len(candidates)}
        },
        "data": candidates
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    fixed_file = data_dir / f"go2senkyo_fixed_names_{timestamp}.json"
    
    with open(fixed_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆçµæœä¿å­˜: {fixed_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    logger.info("\nğŸ“Š ä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆçµæœçµ±è¨ˆ:")
    logger.info(f"  ç·å€™è£œè€…: {len(candidates)}å")
    
    for pref, count in prefecture_stats.items():
        logger.info(f"  {pref}: {count}å")

if __name__ == "__main__":
    test_fixed_extraction()