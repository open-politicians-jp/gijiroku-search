#!/usr/bin/env python3
"""
å…¨å€™è£œè€…ã®é–¢é€£ãƒªãƒ³ã‚¯åé›†
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from collect_candidate_links_fixed import get_candidate_links_fixed
from collect_go2senkyo_optimized import Go2senkyoOptimizedCollector

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def collect_all_candidate_links():
    """å…¨å€™è£œè€…ã®é–¢é€£ãƒªãƒ³ã‚¯ã‚’åé›†"""
    logger.info("ğŸ”— å…¨å€™è£œè€…é–¢é€£ãƒªãƒ³ã‚¯åé›†é–‹å§‹...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    data_dir = Path(__file__).parent.parent.parent / "frontend" / "public" / "data" / "sangiin_candidates"
    latest_file = data_dir / "go2senkyo_optimized_latest.json"
    
    if not latest_file.exists():
        logger.error("æœ€æ–°ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    candidates = data.get('data', [])
    logger.info(f"ğŸ“Š å¯¾è±¡å€™è£œè€…: {len(candidates)}å")
    
    collector = Go2senkyoOptimizedCollector()
    updated_candidates = []
    success_count = 0
    
    for i, candidate in enumerate(candidates):
        try:
            profile_url = candidate.get('profile_url', '')
            name = candidate.get('name', '')
            
            if (i + 1) % 10 == 0 or i < 5:
                logger.info(f"ğŸ“ {i+1}/{len(candidates)}: {name}")
            
            if not profile_url:
                updated_candidates.append(candidate)
                continue
            
            # é–¢é€£ãƒªãƒ³ã‚¯å–å¾—
            links_info = get_candidate_links_fixed(profile_url, collector)
            
            if links_info:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«é–¢é€£ãƒªãƒ³ã‚¯æƒ…å ±ã‚’è¿½åŠ 
                candidate.update(links_info)
                success_count += 1
                
                if (i + 1) % 10 == 0 or i < 5:
                    websites_count = len(links_info.get('websites', []))
                    logger.info(f"  âœ… ãƒªãƒ³ã‚¯å–å¾—æˆåŠŸ: {websites_count}å€‹")
            
            updated_candidates.append(candidate)
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 20 == 0:
                logger.info(f"ğŸ“ˆ é€²æ—: {i+1}/{len(candidates)} ({(i+1)/len(candidates)*100:.1f}%) - æˆåŠŸç‡: {success_count/(i+1)*100:.1f}%")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            collector.random_delay(0.5, 1.0)
            
        except Exception as e:
            logger.error(f"âŒ {name}ã‚¨ãƒ©ãƒ¼: {e}")
            updated_candidates.append(candidate)
            continue
    
    # ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    data['data'] = updated_candidates
    data['metadata']['collection_stats']['with_websites'] = success_count
    data['metadata']['quality_metrics']['website_coverage'] = f"{success_count/len(candidates)*100:.1f}%"
    data['metadata']['generated_at'] = datetime.now().isoformat()
    data['metadata']['data_type'] = "go2senkyo_enhanced_sangiin_2025"
    data['metadata']['collection_method'] = "constituency_with_enhanced_links"
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    enhanced_file = data_dir / f"go2senkyo_enhanced_{timestamp}.json"
    
    with open(enhanced_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ¯ é–¢é€£ãƒªãƒ³ã‚¯åé›†å®Œäº†:")
    logger.info(f"  æˆåŠŸ: {success_count}/{len(candidates)}å ({success_count/len(candidates)*100:.1f}%)")
    logger.info(f"ğŸ“ ä¿å­˜: {enhanced_file}")
    
    # çµ±è¨ˆè¡¨ç¤º
    show_links_statistics(updated_candidates)

def show_links_statistics(candidates):
    """é–¢é€£ãƒªãƒ³ã‚¯ã®çµ±è¨ˆã‚’è¡¨ç¤º"""
    
    site_types = {}
    age_available = 0
    
    for candidate in candidates:
        websites = candidate.get('websites', [])
        
        for site in websites:
            site_type = site['title']
            site_types[site_type] = site_types.get(site_type, 0) + 1
        
        if candidate.get('age_info'):
            age_available += 1
    
    logger.info("ğŸ“Š é–¢é€£ã‚µã‚¤ãƒˆçµ±è¨ˆ:")
    for site_type, count in sorted(site_types.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  {site_type}: {count}å")
    
    logger.info(f"ğŸ“… å¹´é½¢æƒ…å ±: {age_available}å ({age_available/len(candidates)*100:.1f}%)")

if __name__ == "__main__":
    collect_all_candidate_links()